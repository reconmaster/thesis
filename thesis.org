#+TITLE:
#+DATE:
#+AUTHOR:
#+EMAIL:
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t
#+OPTIONS: tags:nil tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+LATEX_CLASS:thesis
#+STARTUP: hideblocks
#+STARTUP: latexpreview

#+BEGIN_EXPORT latex
%% Use these commands to set biographic information for the title page:
\title{Enabling Novel IGRT Imaging Trajectories with Optimization-Based Reconstruction Algorithms}
\author{Andrew Davis}
\department{Committee on Medical Physics}
\division{Biological Sciences}
\degree{Ph. D.}
\date{August, 2017}

%% Use these commands to set a dedication and epigraph text
\dedication{Dedication Text}
\epigraph{Epigraph Text}

% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
\makededication
\makeepigraph

%% Make the various tables of contents
\tableofcontents
\listoffigures
\listoftables

\acknowledgments
% Enter Acknowledgements here

\abstract
% Enter Abstract here

\mainmatter
% Main body of text follows
#+END_EXPORT

* notes                                      :noexport:
  :PROPERTIES:
  :ID:       7f3d97de-795e-402a-82ac-591717f86bfd
  :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
** requirements
   :PROPERTIES:
   :ID:       931c9c50-bfaf-4c8e-b2cc-bcfdf62e327d
   :END:
- [[http://www.lib.uchicago.edu/e/phd/][uchicago]] dissertation guide
- [[https://github.com/zuwiki/ucetd-latex][uoc thesis]] template
* Introduction                               :intro:
  :PROPERTIES:
  :ID:       852796c3-9a3b-49da-bc08-1299e93e0768
  :END:
Tomography is the imaging technique of using a penetrating wave to
create an image of a slice in an object while either blurring or
obscuring details from other planes in the object. The ability to peer
inside an object and create a map of its contents is a powerful tool
that is routinely used in myriad applications. Today, tomographic
methods can be found being deployed in locations ranging from
border-control checkpoints to local medical clinics.

As the non-invasive nature of tomographic imaging had obvious benefits
for the field of medicine, a lot of significant advances in
tomographic technology were driven by clinical research. One such form
of tomographic imaging is computed tomography (CT) which uses
projection images acquired from different locations around the object
to compute the distribution of material densities inside the object.
Though the mathematical framework for solving this inverse problem had
been formulated by Johann Radon in 1917 and a patent for what is
essentially CT was filled in 1940 by Gabriel Frank, it was not until
1967 that the work of Allan M. Cormack and Godfrey N. Hounsfield led
to the first clinical CT scanner. For their work, Cormack and
Housfield shared the 1979 Nobel Prize in Physiology and Medicine
cite:hsieh_computed_2009.

*ADD DIAGRAM*

In a CT scanner system, an x-ray source and opposing detector
typically rotate relative to the object being imaged as x-ray
projection images are acquired at different angular positions. Through
the years of CT research, a fundamental questions has always been how
to move the source and detector the imaging system relative to the
object to obtain sufficient projection information to reconstruction a
useful tomographic image. Part of this answer must take into account
certain engineering limitations that go into building such a system.
However, this is fundamentally a question that must address the
requirements of the computational reconstruction algorithm used to
assemble the image from the x-ray projections.

** Organization
In this work, we will discuss the optimization-based,
image-reconstruction framework that enables the use of these new
scanning trajectories. In particular, we will focus on how this
approach was developed in addressing the two clinical bottlenecks of
limited axial FOV coverage and patient collisions with the linac
gantry. By using these two examples, we will hopefully not only show
the feasibility of using these non-circular trajectories, but also a
potential solution to existing clinical needs.

First, we will discuss the framework and considerations of using
optimization-based reconstruction with different scanning trajectories
in [[id:06ec01f2-e128-4baf-9ec7-4569a3aaa886][Optimization-based algorithms]]. Next, we will look at the need for
geometric calibration and discuss a method we developed to do this for
these trajectories in [[id:652970b8-4916-4190-b83b-2d6ae117c8b3][Geometric calibration]]. We will then look at
using new trajectories to address the limited axial FOV issue in [[id:eaae199f-f899-4862-af50-720895a31c36][Axial
field-of-view extension]] followed by using these trajectories to
alleviate the issue of patient collisions in [[id:99055e18-4b61-404e-9408-ebd5fd0a5d8d][Collision-avoiding
trajectories]]. Finally, we will summarize this work and discuss
possible clinical considerations with this methodology in [[id:1bade25b-80d6-4650-b8a3-baf370fa657c][Summary and
conclusions]].

* Optimization-based algorithms              :opt:
  :PROPERTIES:
  :ID:       06ec01f2-e128-4baf-9ec7-4569a3aaa886
  :END:
CT image reconstruction using a linearized imaging model of the x-ray
transform has existed since the first CT system built by Cormack and
Hounsfield.

** Background: Computed tomography
   :PROPERTIES:
   :ID:       898c8a79-a3b0-4cb2-b1be-2838c8b86426
   :END:
*** Analytic-Based Reconstruction
    :PROPERTIES:
    :ID:       20c14d08-7649-4644-b616-e86e0b7cc515
    :END:
In the 1980s, a lot of work was done to directly solve the inverse
problem for the cone-beam geometry. By modeling the projection
formation process as a Radon transform or an X-ray transform,
reconstruction algorithms were formulated by finding an analytic-based
inverse to the transform. However, for the inverse to be exact, it
needed to meet strict requirements such as Tuy's condition which
states that every plane through the object must intersect the source
trajectory cite:tuy_inversion_1983. While some exceptions to this
requirement were found, it demonstrates the strict requirements on the
types of scanning trajectories for which an exact inverse could be
found.

The circular scanning trajectory that is ubiquitous in the clinic for
CBCT is one trajectory that fails to meet Tuy's condition. The most
popular reconstruction algorithm for the circular CBCT trajectory is
the filtered-backprojection (FBP) algorithm proposed by Feldkamp,
Davis, and Kress (FDK) cite:feldkamp_practical_1984 which is still the
industry standard. FDK is only an exact inversion to the Radon
transform on the midplane containing the circular source
trajectory. For transaxial planes other than the midplane, a
quasi-redundancy in the scanning data is assumed. It is the violation
of this assumption which leads to cone-angle artifacts. These
artifacts become more severe at larger cone angles where this
assumption is less applicable.

The presence of cone-angle artifacts in FDK reconstructions from the
incomplete data acquired with circular scanning trajectories led to
research into inverse algorithms for cone-beam scans from
theoretically complete trajectories such as a circle plus a line
cite:zeng_cone-beam_1992. It became apparent in the reconstruction
results that implementing these direct reconstruction algorithms did
not produce the anticipated results cite:kudo_derivation_1994. Severe
artifacts and numerical errors were found in the reconstructions due
to factors such as truncation introducing high-frequency components
that are amplified in the filtration process.

*** Optimization-Based Reconstruction
    :PROPERTIES:
    :ID:       07e91084-61be-43d3-a905-65ef0ab997a4
    :END:
Analytic-based reconstruction algorithms are formulated by explicitly
finding an inverse to the X-ray transform
\begin{equation}
  \label{eq:xray}
  g(\mathbf{r}_0,\hat{\theta})=\int_0^{\infty}f(\mathbf{r}_0+t\hat{\theta})dt,
\end{equation}
where the data function $g$ is acquired by integrating along the ray
from the source at $\mathbf{r}_0$ in the direction $\hat{\theta}$ through
the object function $f$. A fundamental problem with these
reconstruction algorithms when practically reconstructing $f$ is the
assumption of a continuous-to-continuous (CC) model. These
analytic-based reconstruction algorithms impose dense sampling
requirements for both the detector and number of views to approximate
a continuous data function. Given that the data function from the
digital detector and the numerical array for storing the reconstructed
image are both discrete, a more natural approach to the inverse
problem would be a discrete-to-discrete (DD) imaging model
cite:barrett_foundations_2003.

Iterative reconstruction algorithms are more robust because they do
implement a more accurate DD model of the imaging system. The X-ray
transform of the object function can be represented as the linear
system
\begin{equation}
  \label{eq:ddsys}
  \mathbf{g}=\mathcal{H}\mathbf{f},
\end{equation}
where $\mathbf{g}$ is the discrete $M$ pixel sampled projection on the
detector, $\mathcal{H}$ is the $M\times N$ discrete form of the X-ray
transform, and $\mathbf{f}$ is the object function represented on a N
voxel basis. As direct inversion of $\mathcal{H}$ is impractical due
to both its size and inconsistencies from factors such as noise,
optimization techniques are used to solve this system for an estimate
of the object $\widetilde{\mathbf{f}}$.

The optimization problem for these iterative reconstruction algorithms
is formulated as an objective function based on the actual data
$\mathbf{g}$ and the image model $\mathcal{H}\mathbf{f}$. An
optimization algorithm is then used to iteratively update the estimate
of $\widetilde{\mathbf{f}}$ until a suitable convergence criterion has
been met. The parameters of the optimization problem, the optimization
algorithm, and the convergence criteria are all important factors in
determining the properties of the reconstructed image and subsequently
its utility.

In applying optimization-based reconstruction to reconstruct
non-circular trajectories, we focus primarily on the well-understood
maximum-likelihood expectation maximization (MLEM)
cite:shepp_maximum_1982,dempster_maximum_1977. Previous work has shown
that these iterative algorithms are able to reconstruct clinically
useful images under scanning conditions for which analytic-based FDK
fails
cite:han_optimization-based_2012,sidky_image_2007,sidky_accurate_2006.
The reconstruction work from sparse-view data
cite:bian_evaluation_2010 alone suggests that views could be
distributed at different axial positions to acquire additional scan
information without imparting more dose than the dense set of angular
views used in current clinical circular scans with analytic-based
reconstruction.

# constrained, total-variation (TV) minimization by adaptive steepest
# descent-projection onto convex sets (ASD-POCS) cite:sidky_image_2008.

** Background: Scanning trajectories
   :PROPERTIES:
   :ID:       c90cd638-44e6-49f3-9283-29f75d163005
   :END:
*** Standard Trajectories
    :PROPERTIES:
    :ID:       6293da29-e448-4614-84b6-065af1cc6be9
    :END:
In IGRT, linac-mounted CBCT imaging systems such as Variant's TrueBeam
JV-imaging system now routinely provide patient image information.
These images are used to check the patient alignment before delivering
the radiation treatment. The circular rotation of the linac gantry
defines the acquisition trajectory for the CBCT scan. While such a
scanning trajectory provides sufficient information for an
analytic-based reconstruction of the scan volume, there are a variety
of limitations that arise from this work flow.

Due to engineering and cost restrictions, the kV detector has a
limited size. This restricts the FOV that can be imaged in a
traditional circular scan. While the offset detector technique
cite:bian_optimization-based_2013,cho_cone-beam_1995 is commonly used
to increase the transaxial FOV, the axial coverage is still very
limited cite:pearson_non-circular_2010. The reason why the limited FOV
has not been addressed by increasing the detector size is partially
due to the industry reliance on the approximate FDK algorithm
cite:pan_why_2009. For increasingly large cone angles at the ends of
the axial FOV, the approximation in the algorithm becomes increasingly
worse resulting in cone-angle artifacts cite:feldkamp_practical_1984.

Another problem with the current circular imaging trajectory is
potential linac collisions with the patient
cite:hua_practical_2004,nioutsikou_patient-specific_2003. Cases arise
when the patient is positioned in the treatment position, a CBCT image
cannot be acquired due to part of the patient being in the path of the
linac's trajectory. As the current FDK algorithm requires a trajectory
with sufficient angular coverage, the patient must be moved to a
position where the gantry can make an uninterrupted rotation around
the patient.

In both of these examples, the default circular trajectory prescribed
by FDK is insufficient for obtaining the desired tomographic
information. Furthermore, the disruption to the clinical workflow
created by these limitation introduces bottlenecks into clinical
efficiency which affects both the clinical staff as well as the
patient's comfort in the procedure. In the case of a potential patient
collision, the inability to acquire the required trajectory can even
result in forgoing the CBCT image. For these particular examples, we
investigated ways in which new trajectories enabled by
optimization-based reconstruction could alleviate the complications
imposed by the standard circular scan required by FDK.

*** Non-circular trajectories
The increased flexibility in choosing different scanning trajectories
allowed by optimization-based reconstruction methods provided two
solutions to the issues of limited axial FOV coverage and potential
patient collisions. For these two problems, we found that the existing
limitations could be resolved by using a different scanning
configuration. In each case, we proposed a trajectory that would solve
the existing problem, and then we evaluated how well the
optimization-based reconstructions compared to the clinical images
currently being used.

For the problem of the limited axial coverage, the current clinical
method of extending the FOV is to acquire two different circular scans
at different axial positions and reconstruct each circle independently
using FDK before stitching the two volumes together. Unfortunately,
the increased distortion from cone-angle artifacts at large cone
angles limits the axial separation between these two circles. This
restricted separation distance is much less than what would be
expected based simply on the coverage expected from the geometry of
the kV detector.

The use of the two circles alone provides one interesting example of a
trajectory were optimization-based reconstruction provides an
interesting advantage to the stacked-FDK method currently used. Unlike
stitching two separate reconstructions together, it is possible to
reconstruct the entire volume at once provided the system matrix is
correctly calculated to reflect the acquisition of two circles in
planes located at different axial positions relative to the patient.
In addition to the reduced cone-angle artifacts already seen in
optimization-based methods *CITE*, reconstructing both volumes
together provides additional information about the overlapping region
between the circles that further helps to reduce the cone-angle
artifacts.

In addition to improving the use of the two circles, the
optimization-based framework then allows for trajectories that can
deviate beyond the circles that are still needed for FDK. Given that
there needs to be a relative axial translation between the kV-imaging
system and the patient, we investigated if there any advantages to
acquiring some of the projection views during the axial translation
to. With the optimization-based approach, any such trajectory where
some of the views were acquired during the translation stage could be
reconstructed provided the positions of these views are accurately
represented in the system matrix.

In the case of potential patient collisions with the linac gantry, a
simple change in the scanning trajectory when such a collision arose
would be sufficient to prevent a collision. Much like the extended
axial FOV case, optimization-based reconstruction is able to handle
variations in the acquisition trajectory provided it is accurately
reflected in the system matrix. As such, there are two different ways
we studied where the scanning trajectory could be modified to avoid a
collision.

If the patient collision were to occur with the kV detector (the
closest component of the CBCT system to the patient), one possible way
to avoid that collision would be to move the kV detector away from the
patient at the collision region. This effectively changes the
magnification for that region, but the reconstruction framework is
able to reconstruct from all the views at both magnifications provided
that it is accurately modeled in the reconstruction problem. The other
trajectory modification that could solve this problem would be to move
the patient.

As with the change in magnification, the change in the patient
position does not prevent reconstruction with the optimization-based
methods provided the patient motion is correctly modeled. Moving the
patient also provides a solution to patient collisions that occur with
the linac treatment head. The MV treatment head on Varian's TrueBeam
system is actually closer to the patient than the kV detector. Unlike
the kV detector, it is not possible to change the position of the
treatment head. In this case, moving the patient would be the only
viable trajectory to avoid a collision.

** Methods
   :PROPERTIES:
   :ID:       f9ebfd7f-108b-4dd3-a24c-dab617ab99dd
   :END:
*** Algorithms
**** MLEM
     :PROPERTIES:
     :ID:       e0a24b69-d136-4f9a-9e85-dc42e1d114a9
     :END:
*** Detector weighting
    :PROPERTIES:
    :ID:       cc6bcac6-a445-4dfb-8815-a95e31f517ed
    :END:
#+LABEL: fig:opt_weighting
#+BEGIN_SRC asymptote :file figures/opt/weighting.pdf :exports results :tangle no
settings.render = 0;
import geometry;
// size(8cm,0);
// unitsize(1cm)

// Affichage du repère par défaut (O,vec{i},vec_{j})
// show(defaultcoordsys);
// show(currentcoordsys);

// detector
real dlat=0, dlng=0, dvrt=50;
point det=(dvrt,dlat);

real ulen=40.0, vlen=30.0;

draw((dvrt,-ulen/2+dlat)--(dvrt,ulen/2+dlat),black);

// source
real slat=0, slng=0, svrt=-100;
point src=(svrt,slat);

draw(src--(dvrt, 0), dashed+red);
draw(src--(dvrt, -ulen/2+dlat), dashed+black);
draw(src--(dvrt, ulen/2+dlat), dashed+black);
dot("Source", src, N, red);

addMargins(0.5cm, 0.5cm);

// dot("Detector",det,N,5bp+.5blue);
// dot("Source",src,N,5bp+.5red);

// dot("Source", src)

// real a=5, b=4, theta=-70, poids=3;
// ellipse el = ellipse(origin, a, b);
// arc     ar = arc(el,(0,-b),(a,0),CCW);
// path p = (0,-b-1)--ar--(a+1,0)--(a+1,-b-1)--cycle;
// point pO = (0,0), pM=angpoint(ar,90+theta);
// abscissa abscM = nodabscissa(el,pM);
// real     timeM = abscM.x;
// vector utangM = -dir(el,timeM),
//        unormM = rotate(90)*utangM,
//        vpoids=(0,-poids),
//        vreactionN = -dot(vpoids,unormM)*unormM,
//        vfrottement = -dot(vpoids,utangM)*utangM;

// filldraw(p,lightgray,blue);
// draw(pO--pM,dashed);
// markangle("$\theta$",1.5cm,pM,origin,(1,0));

// coordsys R=cartesiansystem(pM,i=utangM,j=unormM);
// show("$M$", "$\vec{u_{\theta}}$", "$\vec{u_{r}}$", R, xpen=invisible);

// point RpM=changecoordsys(R, pM);
// show(Label("$\vec{f}$",EndPoint),RpM+vfrottement);
// show(Label("$\vec{R}$",EndPoint),RpM+vreactionN);
// show(Label("$\vec{P}$",EndPoint),RpM+vpoids);

// // size3(140,80,15);
// currentprojection=perspective(1,-1,1,up=Z);
// currentlight=White;

// // detector surface
// // path3 g=(1,0,0)..(0,1,0)..(-1,0,0)..(0,-1,0)..cycle;
// // draw(g);

// draw(O--X,red+dashed,Arrow3);
// draw(O--Y,red+dashed,Arrow3);
// draw(O--Z,red+dashed,Arrow3);

// // draw detector
// draw(((-1,-1,0)--(1,-1,0)--(1,1,0)--(-1,1,0)--cycle));

// real a=-0.4;
// real b=0.95;
// real y1=-5;
// real y2=-3y1/2;
// path A=(a,0){dir(10)}::{dir(89.5)}(0,y2);
// path B=(0,y1){dir(88.3)}::{dir(20)}(b,0);
// real c=0.5*a;
// pair z=(0,2.5);
// transform t=scale(1,15);
// transform T=inverse(scale(t.yy,t.xx));
// path[] g=shift(0,1.979)*scale(0.01)*t*
//   texpath(Label("{\it symptote}",z,0.25*E+0.169S,fontsize(24pt)));
// pair w=(0,1.7);
// pair u=intersectionpoint(A,w-1--w);

// real h=0.25*linewidth();
// real hy=(T*(h,h)).x;
// g.push(t*((a,hy)--(b,hy)..(b+hy,0)..(b,-hy)--(a,-hy)..(a-hy,0)..cycle));
// g.push(T*((h,y1)--(h,y2)..(0,y2+h)..(-h,y2)--(-h,y1)..(0,y1-h)..cycle));
// g.push(shift(0,w.y)*t*((u.x,hy)--(w.x,hy)..(w.x+hy,0)..(w.x,-hy)--(u.x,-hy)..(u.x-hy,0)..cycle));
// real f=0.75;
// g.push(point(A,0)--shift(-f*hy,f*h)*A--point(A,1)--shift(f*hy,-f*h)*reverse(A)--cycle);
// g.push(point(B,0)--shift(f*hy,-f*h)*B--point(B,1)--shift(-f*hy,f*h)*reverse(B)--cycle);

// triple H=-0.1Z;
// material m=material(lightgray,shininess=1.0);

// for(path p : g)
//   draw(extrude(p,H),m);

// surface s=surface(g);
// draw(s,red,nolight);
// draw(shift(H)*s,m);
#+END_SRC

#+CAPTION: Schematic representation of weighting factor
#+ATTR_LaTeX: :width \textwidth
#+RESULTS: fig:opt_weighting
[[file:figures/opt/weighting.pdf]]
*** Stopping criteria
    :PROPERTIES:
    :ID:       03857328-5d45-4133-b4a0-eff3fd941eaa
    :END:
** Conclusion
   :PROPERTIES:
   :ID:       0b2ab8ae-fd94-4d01-a7e0-8bef4db30078
   :END:
* Geometric calibration                      :geo:
  :PROPERTIES:
  :ID:       652970b8-4916-4190-b83b-2d6ae117c8b3
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       5c9cdd8b-721f-49b3-b136-c3282bf3659c
   :END:
** Introduction
   :PROPERTIES:
   :ID:       26feb0f0-f33e-4972-af9c-f73e0124f074
   :END:
Correctly modeling the geometric parameters of the image acquisition
is a critical tomographic image reconstruction. This is true
regardless of whether reconstruction is done with analytic-based or
optimization-based methods. Any inconsistency between the real
projection geometry and that used for image reconstruction will appear
as artifacts in the reconstructed image.

While investigating different non-standard scanning trajectories, we
found that correct geometric calibration must be performed to avoid
geometric imaging artifacts. As with the optimization-based image
reconstruction, we needed a calibration procedure that would provide a
robust calibration protocol for the different scanning configurations
we wanted to scan. This is especially true when working with
trajectories where the object is moving in addition to the source and
detector during the scan.

Previous work on geometric calibration for tomographic image
reconstruction has approached the calibration problem via analytic
cite:noo_analytic_2000,smekal_geometric_2004,cho_accurate_2005,yang_geometric_2006,daly_geometric_2008
and estimation
cite:gullberg_estimation_1990,rougee_geometrical_1993,mitschke_optimal_2000,silver_determination_2000,panetta_optimization-based_2008
frameworks. As with CT, the initial calibration efforts utilized
optimization-based methods to determine the geometric offsets from
projections of a known phantom geometry and nominal system setup. By
framing the calibration as an optimization problem, the acquisition
parameters were estimated in a way that minimized a cost function
associated with improper modeling of the acquisition geometry.

These calibration methods (analytic-based methods included) usually
rely on a known calibration phantom. This is typically a set of highly
attenuating fiducials arranged in a specific pattern. After scanning
the phantom with the system of interest, the detected fiducials are
then compared to the known geometry of the phantom. In the
analytic-based approach, the offsets are determined by solving for
parameters that would transform the projection of the phantom to match
the observed projection. In the optimization-based approach, geometric
parameters are varied to improve the match between the projection of
the modeled fiducials an the detected fiducials in the sinogram.

Both methods of performing geometric calibration have their own
strengths and weaknesses. The biggest advantage of utilizing
analytic-based calibration methods is that the sensitivity to
initialization and the sensitivity to the order of parameter variation
due to nonlinearity and coupling of parameters faced by estimation are
avoided cite:smekal_geometric_2004. However, as with
optimization-based reconstruction, optimization-based calibration
methods are more flexible in providing calibration offsets for the
novel trajectories that we studied.

Using previous work for optimization-based geometric calibration
cite:rougee_geometrical_1993,gullberg_estimation_1990,silver_determination_2000,
we developed a calibration method that utilizes a phantom with known
placement of highly attenuating fiducials. By scanning this phantom
and comparing the the projections to the modeled forward-projection of
a mathematical model of the phantom, we can more accurately determine
the system matrix $(\mathcal{H})$ in Equation ([[ref:eq:ddsys]]) for
reconstructing from a non-circular scanning trajectory with
optimization-based methods.

** Methods
   :PROPERTIES:
   :ID:       0b636fe5-fe45-4f10-a5fc-2de8a82bfbe4
   :END:
Where analytic-based methods, such as FDK, require a certain
acquisition trajectory such a as a fixed scanning radius of the source
and detector and the angular position of each projection, the
optimization-based system matrix makes no assumptions of the geometry
in other views. As such, we created a framework that incorporates the
best geometric estimate of the projection geometry of each view. The
flexibility to incorporate geometric corrections in this way is
another useful aspect in using optimization-based methods for image
reconstruction.

Before attempting to determine any geometric errors in our scanning
acquisition, we first modified the calculation of our system matrix to
incorporate the geometry information provided by the TrueBeam system.
In doing this, we took advantage of all the existing calibration
information that is provided with the current clinical system. Any
additional calibration information we could extract in addition to
this would then be the result of imaging with scanning configurations
that are not currently in clinical use.
*** Phantoms
    :PROPERTIES:
    :ID:       F5BECB45-8652-47A3-915C-1E96DA6110E7
    :END:
Our first calibration phantom for determining geometric offsets is
shown in Figure (\ref{fig:geo_geocal}). The phantom is a 15.2 cm outer
diameter acrylic tube with a spiral pattern of CT-spot fiducials
placed 2.5 cm apart every $45^{\circ}$. When scanned, the CT spots are
clearly visible in the projection images which is ideal for automating
the fiducial detection in the data domain.

However, we realized that using such a spiral calibration phantom
creates a degree of ambiguity in the geometry of the projected
fiducials. With both this phantom and additional calibration phantoms
we created, too much symmetry in the phantom design leads to a rather
challenging objective function. To avoid such complexity, a
calibration phantom with intentional asymmetry is desirable.

In addition to the necessary complexity created by this phantom,
another concern for a calibration phantom is the uncertainty in the
geometry of the phantom itself. Though the guide lines on the cylinder
were inscribed with the lathe and its rotational stage, we placed the
fiducials by hand. As we were trying to determine millimeter offsets
with our calibration, this fiducial placement was suboptimal.

#+CAPTION: Initial geometric calibration phantom with a spiral fiducial pattern.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:geo_geocal
[[../../research/trajectories/geometry/geocal/20140901_extended_cllc.jpg]]

The phantom we then decided to use for calibration was the Isocal
phantom created by Varian shown in Figure ([[ref:fig:geo_isocal]]). The Isocal
phantom directly addresses the two problems encountered with our first
phantom. First, the phantom is designed with intentional asymmetry.
Additionally, the phantom is manufactured by Varian to help align the
MV-treatment isocenter with the kV-imaging isocenter. As such, the
position of the beads on this phantom have a much tighter tolerance
than that of our original phantom.

#+CAPTION: Varian's Isocal phantom positioned at the isocenter.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:geo_isocal
[[../../research/phantoms/isocal/imgs/161012_isocal.jpg]]

*** Calibration method
    :PROPERTIES:
    :ID:       F53F4B5A-83EB-4B16-9B6D-F557D3E441C2
    :END:
We designed a calibration procedure specifically for the non-standard
scanning trajectories we implemented on the TrueBeam system with
Developer Mode. As such, the nominal trajectory we used to initialize
our calibration method was self-reported, view-by-view geometry
provided by the TrueBeam system for each projection. Starting with
this initial guess with which we calculated our reconstruction system
matrix $\mathcal{H}$, the additional calibration information we were
able to extract with our calibration improved our estimate of both the
system matrix and subsequently the estimated image from the
reconstruction.

Figure ([[ref:fig:geo_cal_schematic]]) provides a schematic illustration
of this with the Isocal phantom for a single view. Ideally, the
nominal geometry used to calculate a single projection would produce
the simulated projected fiducials in blue. However, as both our work
and that of others has found, this is not usually the case.
Discrepancies between the reported geometry and the actual scanning
geometry can arise from multiple sources in a given acquisition.

With a typical CBCT scan, deviations from the nominal geometry can
occur in both the phantom's setup (translation and rotation in all
three dimensions) as well as that of the source and detector positions
(due to translation and rotation deviations in the gantry, source, and
detector). The collective impact of these various discrepancies will
subsequently produce projection views for which the projected
fiducials in the data domain do not match the simulated projections
from the nominal geometry as shown by the red projected fiducials in
Figure ([[ref:fig:geo_cal_schematic]]).

#+NAME: fig:geo_cal_schematic
#+BEGIN_SRC asymptote :file figures/geo/cal_schematic.pdf :exports results
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import geometry;
import solids;
import three;

import graph3;
import geometry;
import solids;
import three;

// view configuration
size(10cm);
currentprojection=orthographic(-15,5,13,up=Y);
// currentprojection=perspective(-15,5,13,up=Y);
// currentlight=White;

// Draw axis
// draw(Label("$y$",1),(0,0,0)--(0,5,0),red,Arrow3);
// draw(Label("$x$",1),(0,0,0)--(5,0,0),red,Arrow3);
// draw(Label("$z$",1),(0,0,0)--(0,0,5),red,Arrow3);

// kV schematic
real dlat=0, dlng=0, dvrt=50;
triple det_cent=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det_cent-(0,ulen/2,vlen/2));

triple det0 = det_cent-(0,ulen/2,vlen/2);
real s=5;
triple u = (det0+s*(0,1,0));
triple v = (det0+s*(0,0,1));
triple w = (det0+s*(-1,0,0));

// detector coordinate system
draw(det0--u,blue,Arrow3,L=Label("$u$", position=EndPoint, align=W));
draw(det0--v,blue,Arrow3,L=Label("$v$", position=EndPoint, align=N));
draw(det0--w,blue,Arrow3,L=Label("$w$", position=EndPoint, align=S));

draw(detector, blue);

// path3 det180 = rot180*detector;
// path3 det270 = rot270*detector;

// uncal detector coordinate system
transform3 det_pitch=rotate(-5, det_cent, det_cent+(-1,0,0));
transform3 det_roll=rotate(-5, det_cent, det_cent+(0,0,1));
transform3 det_yaw=rotate(5, det_cent, det_cent+(0,-1,0));
transform3 det_shift=shift(5, -8, 2);

path3 detector_uncal = det_pitch*det_roll*det_yaw*det_shift*detector;
path3 det_cent_uncal = det_pitch*det_roll*det_yaw*det_shift*det_cent;
// path3 detector_uncal = det_shift*detector;
// path3 det_cent_uncal = det_shift*det_cent;
real op_uncal=0.35;
draw(detector_uncal, red+opacity(op_uncal));

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

// label("$\theta=0^{\circ}$",red,align=S,position=towardcamera((det_cent-(0, ulen/2, -vlen/2))));

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// uncal source
// triple src_uncal=shift(0,10,5)*(svrt,slat, slng);

// lines from source to detector edges
draw(src..det_cent-(0,-ulen/2,-vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,-ulen/2,vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,ulen/2,vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,ulen/2,-vlen/2),blue+opacity(0.15));

draw(Label("$X_{\theta_g=0^{\circ}}$", 1),src--det_cent-(110,0,0), blue, arrow=Arrow3);

// transformed frame vectors
triple det0_uncal = point(detector_uncal, 0);
triple u_p = point(detector_uncal, 1) - det0_uncal;
triple v_p = point(detector_uncal, 3) - det0_uncal;

// unit vectors
triple uhat_p = u_p / length(u_p);
triple vhat_p = v_p / length(v_p);
triple what_p = cross(uhat_p, -vhat_p);

// scale
triple u_p = s*uhat_p + det0_uncal;
triple v_p = s*vhat_p + det0_uncal;
triple w_p = s*what_p + det0_uncal;

// uncalibrated detector coordinate system
draw(det0_uncal--u_p,red,Arrow3,L=Label("$u'$", position=EndPoint, align=SE));
draw(det0_uncal--v_p,red,Arrow3,L=Label("$v'$", position=EndPoint, align=S));
draw(det0_uncal--w_p,red,Arrow3,L=Label("$w'$", position=EndPoint, align=S));

// draw(point(detector_uncal, 1)--src, red+opacity(0.15));
// real arrowlength = 5
// vector v_p=new path(real x){
//     return point(detector_uncal, 1)--arrowlength*(-1)*point(detector_uncal, 2));
// };

// draw(v_p)
// draw(point(detector_uncal, 1)--point(detector_uncal, 2),red, arrow=Arrow3);

// // and for real projection
// draw(src_uncal..point(detector_uncal, 0), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 1), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 2), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 3), red+opacity(0.15));

// draw(Label("$\mathcal{H}$", 1),src--det_cent_uncal, red, arrow=Arrow3);

// Draw cylinder
// cylinder(startpoint3d, radius, length, along_this_axis)
triple start = (0,0,-8);
real length = 16;
real radius = 11.3;
triple ax = (0,0,1);
revolution r = cylinder(start,radius,length,ax);
draw(r,black);

// isocal spots
triple[] isocal={(0,-11.3,-7.5),
                 (7.9903,-7.9903,-7.5),
                 (7.9903,7.9903,-7.5),
                 (-11.3,0.0,-7.5),
                 (-7.9903,7.9903,-5),
                 (11.3,0.0,-3),
                 (0,11.3,-2),
                 (-10.4398,4.3243,2),
                 (4.3243,10.4398,3),
                 (-10.4398,-4.3243,5),
                 (4.3243,-10.4398,5),
                 (10.4398,-4.3243,7.5),
                 (10.4398,4.3243,7.5),
                 (-4.3243,10.4398,7.5),
                 (-4.3243,-10.4398,7.5)
};

dot(isocal, black);

// project points
transform3 proj=planeproject(detector);
transform3 proj_uncal=planeproject(detector_uncal);
// transform3 proj090=planeproject(det090);
// transform3 proj180=planeproject(det180);
// transform3 proj270=planeproject(det270);

dot(proj*isocal,blue);
dot(proj_uncal*isocal,red+opacity(op_uncal));
// dot(proj090*isocal,red);
// dot(proj180*isocal,red);
// dot(proj270*isocal,red);
#+END_SRC

#+CAPTION: Schematic represenation of a single projection view for the isocal phantom with the TrueBeam kV-CBCT scanning geometry shown using an orthographic projection. The blue detector and projected isocal fiducials correspond to the self-reported geometry from the imaging system. The red detector and projected fiducials illustrates how translation and rotation offsets create variations in the projected fiducials in the sinogram space. The bottom left corner corresponds to the origin of the detector coordinate system. The detector's translation and rotation offsets are exaggerated here for illustrative purposes.
#+LABEL: fig:geo_cal_schematic
#+ATTR_LaTeX: :width 0.9\textwidth
#+RESULTS: fig:geo_cal_schematic
[[file:figures/geo/cal_schematic.pdf]]

Starting with the nominal scanning geometry reported by the projection
metadata, we first build an initial projection matrix $\boldsymbol{X}$
that transforms the simulated phantom fiducials in room coordinates to
projected spots in detector coordinates. The matrix $\boldsymbol{X}$
is calculated using the variables describing each view shown in Figure
([[ref:fig:geo_cal_proj]]). The source and detector (including the
detector's frame vectors $\left\{ \hat{u}, \hat{v}, \hat{w} \right\}$)
are rotated into the global image space by rotating these vectors by
the gantry angle $\left( \theta_{g} \right)$ at each view (the gantry rotation
axis is the $y$ axis in Figures ([[ref:fig:geo_cal_schematic]],
[[ref:fig:geo_cal_proj]]).

Next, the orthogonal component of the rays from the source to the
detector is calculated by first finding a ray connecting the source to
the detector, $\vec{r}_{\text{sd}}$. The component of this ray that is
orthogonal to the detector is then found using the dot product
\begin{equation}
  L=\vec{r}_{sd}}\cdot \hat{w},
  \label{eq:geo_along}
\end{equation}
where the frame vector $\hat{w}$ corresponds to the detector's normal
unit vector. This then provides the vector describing the piercing
point $\left( \vec{p} \right)$ at that view which is given by
\begin{equation}
  \vec{p}=\vec{r}_s+L \hat{w},
  \label{eq:geo_pierce}
\end{equation}
where $\vec{r}_s$ is the vector corresponding to the source position in
the image coordinates for that view.

#+NAME: fig:geo_cal_proj
#+BEGIN_SRC asymptote :file figures/geo/cal_proj.pdf :exports results
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;


import graph3;
import geometry;
import solids;
import three;

import graph3;
import geometry;
import solids;
import three;

// view configuration
size(10cm);
currentprojection=orthographic(-15,10,20,up=Y);
// currentprojection=perspective(-15,5,13,up=Y);
// currentlight=White;

// Draw axis
real ax_scale=15;
draw(Label("$z$",position=EndPoint,align=N),(0,0,0)--(0,ax_scale,0),black,Arrow3);
draw(Label("$x$",position=EndPoint,align=S),(0,0,0)--(ax_scale,0,0),black,Arrow3);
draw(Label("$y$",position=EndPoint,align=SW),(0,0,0)--(0,0,-ax_scale),black,Arrow3);

// show gantry angle
draw(Label("$\theta_{g}$", (2, -0.5, 0)), arc((0, 0, 0), (ax_scale/3, 0, 0), (0, -ax_scale/3, 0)), red, arrow=Arrow3);

// kV schematic
real dlat=-13, dlng=0, dvrt=50;
triple det_cent=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det_cent-(0,ulen/2,vlen/2));

triple det0 = det_cent-(0,ulen/2,vlen/2);
triple u = (det0+ax_scale/2*(0,1,0));
triple v = (det0+ax_scale/2*(0,0,1));
triple w = (det0+ax_scale/2*(-1,0,0));

// detector norm
triple dnorm = (det_cent+ax_scale*(-1,0,0));

// detector coordinate system
draw(det0--u,black,Arrow3,L=Label("$u$", position=EndPoint, align=W));
draw(det0--v,black,Arrow3,L=Label("$v$", position=EndPoint, align=N));
draw(det0--w,black,Arrow3,L=Label("$w$", position=EndPoint, align=N));

draw(detector, black);

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// lines from source to detector edges
draw(src..det_cent-(0,-ulen/2,-vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,-ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,-vlen/2),black+opacity(0.15));

// ray connecting the source to the detector
triple ray_sd = det_cent-src;
draw(L=Label("$\vec{r}_{sd}$", position=EndPoint, align=E), src--det_cent, blue, Arrow3);

// dot product of ray onto normal vecotr
real along = dot(ray_sd, dnorm);

// detector projection operator
transform3 proj=planeproject(detector);

// show pierecing point
triple pierce = proj*src;

draw(L=Label("$\vec{p}$", position=EndPoint, align=NW),src--pierce,blue+dashed,arrow=Arrow3);
draw(L=Label("$\vec{p}_{uv}$", position=EndPoint, align=SE),det0--pierce,red+dashed,arrow=Arrow3);
// draw(src--pierce,red+dotted, arrow=Arrow3);
#+END_SRC

#+CAPTION: Orthographic schematic of a single projection view and the associated variables used in building the projective transform matrix $\left( \boldsymbol{X} \right)$ for that view. The $\left\{x, y, z \right\}$ coordinate system corresponds to the standard IEC global coordinate system, and the $\left\{u, v, w \right\}$ coordinate system corresponds to the detector frame vectors for that view. The red arrow labeled by $\theta_g$ denotes the gantry rotation angle which is defined from the $x$ axis as shown here for the kV imaging system. The blue vector $\vec{r}_{sd}$ points from the source to the detector center, and the blue vector $\vec{p}$ shows the piercing point of the x-ray source on the detector. The red vector $\vec{p}_{uv}$ corresponds to the piercing point in the detector basis as calculated in Equation ([[ref:eq:geo_pierecuv]]).
#+LABEL: fig:geo_cal_proj
#+ATTR_LaTeX: :width 0.9\textwidth
#+RESULTS: fig:geo_cal_proj
[[file:figures/geo/cal_proj.pdf]]

With this new piercing point, it is possible to now construct a
transform that projects the fiducials as well as transforms them to
the detector basis. The transform to the detector basis is represented
by the homogeneous coordinate transform
\begin{equation}
  \boldsymbol{G} = \begin{bmatrix}
    u_i & u_j & u_k & -r_x \\
    v_i & v_j & v_k & -r_y \\
    w_i & w_j & w_k & -r_z \\
    0 & 0 & 0 & 1
  \end{bmatrix}.
  \label{eq:geo_gmat}
\end{equation}
Then using the orthogonal ray component found in Equation
([[ref:eq:geo_along]]), the homogeneous coordinate projection matrix is
\begin{equation}
  \boldsymbol{P} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & \frac{1}{L} & 0 \\
    0 & 0 & 0 & 0
  \end{bmatrix}.
  \label{eq:geo_pmat}
\end{equation}
Using these transforms so that they are pre-multiplied by the fiducial
position vectors, the combined transform is then
\begin{equation}
  \boldsymbol{M} = \boldsymbol{G}\boldsymbol{P}.
  \label{eq:geo_magicmat}
\end{equation}

Finally, this information can be combined to create a single transform
of the fiducials in the global image coordinate system to the
projected spots on the detector in discretized detector bin
coordinates. First, the coordinates of the piercing point must be
calculated in the detector basis as
\begin{equation}
  \vec{p}_{uv} = \(\vec{p}-\vec{r}_d\)\boldsymbol{G},
  \label{eq:geo_pierecuv}
\end{equation}
where $\vec{r}_d_{}$ is the detector coordinates rotated into the global
coordinate system. With all this, the projection transform used to
calculate the projected fiducials in discretized detector bin
coordinates is
\begin{equation}
  \boldsymbol{X} = \boldsymbol{M}\boldsymbol{T} (\vec{p}_{uv})
  \boldsymbol{S}\left( \left[\frac{1}{s_{\text{pix}}},
    \frac{1}{s_{\text{pix}}}, 1 \right]\right) \boldsymbol{T} \left(
  \left[\frac{u_{\text{len}}}{2}+0.5, \frac{v_{\text{len}}}{2}+0.5, 0
    \right] \right),
  \label{eq:geo_xproj}
\end{equation}
where $boldsymbol{S}$ is a scaling transformation along the $\left\{
u,v \right\}$ basis by the inverse of the pixel size $\left(
s_{{\text{pix}} \right)$, and \boldsymbol{T} is a translation
transformation to place the origin of the discretized detector basis
at the center of the corner pixel.

With the projection transform $\boldsymbol{X}$, each vector
corresponding to the fiducials on the Isocal phantom can be projected
onto the discretized detector basis as illustrated in Figure
([[ref:fig:geo_cal_schematic]]). These projected spots are then matched to
the detector spots in the real sinogram. The $L_2$ norm between the
real and simulated projected spots is then calculated as it serves as
the cost function for the optimization-based calibration. The idea
behind this being that when deviations from the nominal geometry are
identified, the simulated and real projected fiducials should
correspond to the same detector coordinates.

As with other optimization-based calibration procedures, we
iteratively varied the parameters corresponding to the geometric
degrees of freedom (DOF) of the scanning trajectory. The phantom pose
(position and orientation) is first allowed to vary in the room
coordinate system to account for potential setup errors between the
room coordinates and the modeled position of the phantom. Once the
pose of the Isocal phantom is identified, then the source, detector,
and patient couch translations and rotations are allowed to vary, and
the cost of the simulated fiducial projections are calculated at each
step. We used the Nelder-Mead simplex algorithm
cite:lagarias_convergence_1998 to minimize the $L_{2}\text{-norm}$ cost
function.

Given that there are there are different combinations of couch, source
and detector motions that can cause the same change of the object
relative to the source and detector within the image coordinate
system, there are some degrees of freedom that can couple with others.
For instance, shifting the patient in the positive longitudinal
direction if effectively the same as allowing the source and detector
to move the same distance in the negative longitudinal direction. This
requires that only a few parameters are allowed to vary at once as
allowing too many parameters on this non-convex surface will often
produce nonphysical geometric corrections. Once the cost has been
minimized, the geometric offsets are used as the calibration
information for calculating the system matrix $\mathcal{H}$ for the
image reconstruction.

For a new trajectory, this phantom is first scanned to identify any
geometric offsets that are incorrectly reported in the TrueBeam data
headers. Though we found the self-reported position accuracy from the
acquisition metadata to be very good, there were still some scanning
configurations for which we found the additional refinement from our
geometric calibration to be critical for obtaining a useful
reconstruction. This was particularly true for scanning trajectories
where the object and the kV imaging system were moving simultaneously.
*** Geometric-offset artifact catalog        :noexport:
    :PROPERTIES:
    :ID:       DED4A0A6-3775-41ED-AF64-BD6604B2B3AD
    :END:
Though the type of artifacts that are introduced by geometric offsets
for circular scanning trajectories are relatively well known, this
same sort of understanding is lacking for these new trajectories. To
study how geometric offsets affect images reconstructed from these new
trajectories, we will create a simulation catalog of artifacts
produced by different geometric errors. By introducing intentional
geometric inconsistencies in the reconstruction system matrix, we can
characterize the artifacts that appear in the reconstruction compared
to a numerically-exact inverse crime reconstruction.

As one of our primary objectives in using these novel trajectories is
to create an extended axial FOV image, we need to study how these
geometric errors degrade the image quality along the axial
direction. To ensure our simulation can adequately identify these
artifacts, we will create a simulated phantom such as an axially
extended version of the Catphan high resolution module. This will
provide resolution metrics not only in the axial dimension, but also
in the transverse planes as a function of axial position.

The simulation catalog of different artifacts that arise from
geometric offsets will provide a guide to visually identify potential
geometric errors based on the reconstructed image. This provide one
way in which we can verify the effectiveness of our geometric
calibration procedure. By incorporating the calibration information we
obtain with the calibration, known geometric error artifacts should be
reduced.
**** notebooks                               :noexport:
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"geometry/overview.ipynb")][geometry/overview.ipynb]]
- Overview of the simulated work/analysis
*** Image entropy                            :noexport:
    :PROPERTIES:
    :ID:       2410E321-8750-473F-B6B6-13DC1719B6AE
    :END:
To further verify the effectiveness of the calibration procedure, we
will also need to use additional metrics to quantitatively
characterize the impact of using the calibration on image quality. The
work of Wicklein et al. has suggested that the best metric for
measuring the impact of geometric error on image quality is entropy
$(E)$ of the image's gray-level histogram $(H)$. This is defined as
\begin{equation}
  \label{eq:entropy}
  E=-\sum_{q=0}^Q h(q)\cdot\text{log}(h(q))
\end{equation}
where $Q$ is the maximum intensity value and
\begin{equation}
  \label{eq:norm_hist}
  h(q)=\frac{H(q)}{N}
\end{equation}
is the normalized histogram cite:wicklein_image_2012. For this metric,
minimum entropy is obtained for an image with a single intensity value
while an image with uniform distribution over all intensity values
would have maximum entropy.

Geometric errors introduce blurring at sharp boundaries in the image
which increases the entropy. By reducing geometric errors with
calibration, this blurring effect and subsequently entropy should
reduced. For our non-circular trajectories, Wicklein's conclusion can
be verified readily with the images in our catalog of geometric
errors. The image entropy of the correct-geometry reconstruction will
be against the reconstructions with intentional geometric errors to
determine if improved geometric modeling reduces the image entropy in
Equation (\ref{eq:entropy}).

If the entropy calculations based on simulation agree with Wicklein's
findings, entropy would be reasonable metric to characterize the
benefits and limitations of using the geometric offsets from the
calibration phantom on different non-circular trajectory
reconstructions. We would then use entropy as the metric to compare
reconstructions with and without calibration. From this, we can not
only verify the effectiveness of our calibration method with different
non-circular trajectories, but also then characterize the impact
additional geometric corrections have on image quality.
*** Experimental validation
    :PROPERTIES:
    :ID:       150f19dd-e68d-4226-bdd4-01e31ea1176f
    :END:
To evaluate the efficacy of our calibration procedure, we investigated
its performance on calibrating both a standard, half-fan, circular
trajectory where the couch is stationary as well as a virtual
isocenter trajectory with the same object illumination where the couch
moves during the acquisition. For each of these trajectories, we used
the same Developer Mode script to scan both the Catphan phantom and
the Isocal phantom. We subsequently used the sinogram from the Isocal
scan to extract calibration offsets for that particular trajectory
using the calibration method described in Section ([[id:F53F4B5A-83EB-4B16-9B6D-F557D3E441C2][Calibration
method]]).

With this calibration extracted from the data domain using our
calibration routine, we reconstructed the Catphan scans from these two
trajectories with and without the calibrations offsets. We
reconstructed onto an isotropic image grid of 0.473 mm for each
reconstruction, and applied the half-fan weighting described in
Section ([[id:cc6bcac6-a445-4dfb-8815-a95e31f517ed][Detector weighting]]). For all of these reconstructions we used
200 iterations of MLEM as described in Section ([[id:e0a24b69-d136-4f9a-9e85-dc42e1d114a9][MLEM]]).

*Should we run the metric analysis on these two scans w/ and w/o calibration?*
*** figures                                  :noexport:
**** four detector schematic
# +LABEL: fig:geo_schematic
#+BEGIN_SRC asymptote :file figures/geo/schematic.pdf :exports results :tangle no
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import solids;
import three;

// view configuration
size(10cm);
// currentprojection=orthographic(-5,1,5,up=Y);
currentprojection=perspective(-5,1,5,up=Y);
// currentlight=White;

// Draw axis
// draw(Label("$y$",1),(0,0,0)--(0,5,0),red,Arrow3);
// draw(Label("$x$",1),(0,0,0)--(5,0,0),red,Arrow3);
// draw(Label("$z$",1),(0,0,0)--(0,0,5),red,Arrow3);

// kV schematic
real dlat=0, dlng=0, dvrt=50;
triple det=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det-(0,ulen/2,vlen/2));

transform3 rot090=rotate(90, Z);
transform3 rot180=rotate(180, Z);
transform3 rot270=rotate(270, Z);

path3 det090 = rot090*detector;
path3 det180 = rot180*detector;
path3 det270 = rot270*detector;

draw(detector, black);
draw(det090, black);
draw(det180, black);
draw(det270, black);

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

label("$\theta=0^{\circ}$",red,align=S,position=towardcamera((det-(0, ulen/2, -vlen/2))));
// label("$B$",align=S,position=towardcamera((B)));
// label("$C$",align=SE,position=towardcamera((C)));
// label("$D$",align=SE,position=towardcamera((D)));
// label("$E$",align=NE,position=towardcamera((E)));
// label("$F$",align=S,position=towardcamera((F)));

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// lines from source to detector edges
// draw(src..det-(0,-ulen/2,-vlen/2),black);
// draw(src..det-(0,-ulen/2,vlen/2),black);
// draw(src..det-(0,ulen/2,-vlen/2),black);
// draw(src..det-(0,ulen/2,vlen/2),black);

// Draw cylinder
// cylinder(startpoint3d, radius, length, along_this_axis)
triple start = (0,0,-8);
real length = 16;
real radius = 11.3;
triple ax = (0,0,1);
revolution r = cylinder(start,radius,length,ax);
draw(r,black);

// isocal spots
triple[] isocal={(0,-11.3,-7.5),
                 (7.9903,-7.9903,-7.5),
                 (7.9903,7.9903,-7.5),
                 (-11.3,0.0,-7.5),
                 (-7.9903,7.9903,-5),
                 (11.3,0.0,-3),
                 (0,11.3,-2),
                 (-10.4398,4.3243,2),
                 (4.3243,10.4398,3),
                 (-10.4398,-4.3243,5),
                 (4.3243,-10.4398,5),
                 (10.4398,-4.3243,7.5),
                 (10.4398,4.3243,7.5),
                 (-4.3243,10.4398,7.5),
                 (-4.3243,-10.4398,7.5)
};

dot(isocal, black);

// project points
transform3 proj=planeproject(detector);
transform3 proj090=planeproject(det090);
transform3 proj180=planeproject(det180);
transform3 proj270=planeproject(det270);

dot(proj*isocal,red);
dot(proj090*isocal,red);
dot(proj180*isocal,red);
dot(proj270*isocal,red);
#+END_SRC

#+CAPTION: Schematic representation of the scanning geometry
#+ATTR_LaTeX: :width 0.75\textwidth
# +RESULTS: fig:geo_schematic
*** geocal procedure                         :noexport:
    :PROPERTIES:
    :ID:       DD6F1968-D9A8-46AB-AC2C-BF79512B530A
    :END:
The current version of the geocal procedure uses a single step
projection matrix to transform positions in room coordinates to pixels
on the detector. do you want to incude the equation for that? this is
it basically, where “framevecs” are the 3 detector basis vectors
rotated by roll, pitch, yaw and then gantry angle - your usual frame
vectors. also note that my htransform_vectors function premultiplies
the matrix by the vector, i.e. X’ = X M, so the component
transformations are applied from left to right - if M=ABC, then it’s A
first followed by B followed by C.

#+BEGIN_SRC matlab :tangle no
grotvecs=framevecs;
dnormrot=grotvecs(3,:);

sourcedetray=rotdet-rotsrc;  % this can be any ray connecting the source
                             % with a point on the detector

along=dot(sourcedetray,dnormrot);  % this is the new L for projection.
                                   % the "along" component is the same for
                                   % every ray from source to detector

% find the new piercing point, of ray through source along det normal
% direction.
newpierce=rotsrc+along*dnormrot;
mygmat=eye(4,4);
mygmat(1:3,1:3)=grotvecs';
mygtmat=hmatrix_translate(-rotsrc)*mygmat;

projmat=eye(4,4);
projmat(4,4)=0;
projmat(3,4)=1.0/along;
% compress the above to a single step - transform to detector basis,
% project
magicmat=mygtmat*projmat;

% to compute pixel coordinates, first find the U and V coordinates of the
% piercing point.  then just add the fids_on_detector U and V values.
% note these will still be in cm, not in pixels.  last thing will be to
% scale by pixel size and add detector center position, e.g. (512,384).
pierceoffset=newpierce-rotdet;
uvpierce=htransform_vectors(mygmat,pierceoffset);

%uvfids1=(fids_on_det_4(:,1:2)+repmat(uvpierce(1:2),size(fids_on_det_4,1),1))/pixsize;
xprojmat=magicmat*hmatrix_translate(uvpierce)* ...
         hmatrix_scale([1.0/pixsize 1.0/pixsize 1])* ...
         hmatrix_translate([usize/2+0.5, vsize/2+0.5, 0]);

#+END_SRC

#+RESULTS:
*** ideas                                    :noexport:
Given that part of the robust nature of optimization-based algorithms
is the ability to handle the poorly-conditioned nature of the inverse
problem...
** Results
   :PROPERTIES:
   :ID:       bc50c80a-fbb7-41d3-a9d0-ebc552f59896
   :END:
*** Experimental validation
    :PROPERTIES:
    :ID:       2c3c25d5-477a-4013-bf2a-5a74716b9c20
    :END:
Figure ([[ref:fig:geo_cal_catphan_sens]]) shows the CTP 528 spatial
resolution module slice from the reconstructions of both the circular
scan in the left column and the virtual isocenter scan in the right
column. The top row shows the slice from the uncalibrated
reconstruction using just the nominal scanning information from scan's
metadata. It can be seen by comparing the circle and virtual isocenter
scans without calibration that the additional complexity of moving the
treatment couch during the scan introduces additional geometric error
over the standard circle scan which visually degrades spatial
resolution.

The bottom row of Figure ([[ref:fig:geo_cal_catphan_sens]]) shows the same
slice from the corresponding trajectory with the geometric offsets
from the calibration procedure incorporated into the system matrix
$\mathcal(H)$. For the circular scan, using the calibration
information does provide a bit of an improvement in spatial
resolution. However, the efficacy of the calibration method is
particularly striking for the virtual isocenter scan. By using the
calibration offsets in the reconstruction model, the spatial
resolution of the virtual isocenter reconstruction becomes comparable
to that of the circular scan.

Figure ([[ref:fig:geo_cal_cost]]) shows the $L_{2}_{}-\text{norm}$ from the data
domain between the simulated fiducial projections and the real
fiducial projections acquired from the circle and virtual isocenter
scans of the isocal phantom. As this served as the cost function which
we used as the minimization objective for the optimized offset search,
we can see that the calibration did effectively reduce this cost from
the nominal geometry (blue) to the calibrated geometry (green). This
cost also reflects the same trend we see in the spatial resolution of
the images shown in Figure ([[ref:fig:geo_cal_catphan_sens]]).

Comparing the the $L_{2}_{}-\text{norm}$ of the uncalibrated scans in Figure
([[ref:fig:geo_cal_cost]]), we see that there is far more disagreement
between the nominal trajectory and the real data for the virtual
isocenter scan than that of the circular scan. We see that this
subsequently produces far more artifacts and loss of spatial
resolution in the virtual isocenter reconstruction than in that of the
circular scan. We also see that after running the optimization-based
calibration procedure, the cost for both trajectories is reduced to
approximately the same order of magnitude. Again, this agrees with the
reconstructed images as the spatial resolution in both the circular
scan and the virtual isocetner scan is comparable after incorporating
the calibrated geometry into the system matrix $\mathcal{H}$.

*Describe TB1 virtual isocenter couch backlash?*
#+BEGIN_EXPORT latex
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/catphanCalComp}
    \caption{}
    \label{fig:geo_cal_catphan_sens}
  \end{subfigure}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad,
  % \hfill etc.
  % (or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/costComp1p5}
    \caption{}
    \label{fig:geo_cal_cost}
  \end{subfigure}
  \caption{(a) shows the 200$^{\text{th}}$ iteration of MLEM
    reconstructions of the CTP 528 spatial resolution module from the
    Catphan phantom for two different trajectories. The left column is
    from a 1.5X circular scan, and the right column is from a 1.5X
    virtual isocenter scan reconstructed onto a 0.473 mm isotropic
    image grid([-100, 2000] HU). The top row shows the reconstruction
    using the nominal geometry from self-reported metadata, and the
    bottom row corresponds to the calibrated reconstructions. (b)
    shows the $L_{2}$-norm used for the calibration cost function
    before (blue) and after (green) calibration for both the circle
    (left) and the virtual isocenter (right).}
  \label{fig:geo_cal_sens_cost}
\end{figure}
#+END_EXPORT
**** figures                                 :noexport:
- [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/calibration_images.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/calibration_images.ipynb]]
**** notes                                   :noexport:
- [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb")][170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb]]
** Conclusion
   :PROPERTIES:
   :ID:       fd41d566-a4b3-4dcd-9f8c-7417276ad25c
   :END:
In developing our optimization-based geometry calibration procedure,
we found that proper geometric calibration is a critical component of
improving tomographic image quality. This is particularly true for
more complicated trajectories where additional motion components such
as that of the treatment couch introduce additional degrees of freedom
in which geometric errors can arise. As shown in Figure
([[ref:fig:geo_cal_sens_cost]]), the additional motion of the couch with
the simultaneous motion of the source and detector introduces a larger
deviation from the nominal scanning geometry.

The optimization-based calibration we used in this study provides a
robust framework for calibrating arbitrary scanning trajectories. The
ability to acquire view-by-view calibration information with this
approach dovetails nicely with the optimization-based framework that
enables the reconstruction from the different trajectories we studied
in this research. Though many of the different analytic-based methods
described in the literature could be adopted to many of these
trajectories (and have been for some), the benefit of the
optimization-based framework for both reconstruction and geometric
calibration comes from freedom to easily model and reconstruct from
trajectories as well as geometric offsets that deviate from the
analytically prescribed model.

Though this does imply that calibration scans must be acquired for
each scan of interest, there are optimization-based calibration
methods similar to ours that attempt to extract calibration
information with no /a priori/ knowledge of the phantom
cite:panetta_optimization-based_2008. Such calibration methods or
built-in calibration markers in the table are potential ways in which
it would be possible to avoid acquiring calibration information for
every scan of interest. As we used the TrueBeam kV-CBCT system for our
data acquisition, Varian's Isocal phantom provided a convenient means
of calibrating the imaging system as the linac use case already
demands accurate calibration for treatment accuracy in addition to
image quality alone.

In the following chapters we where we investigate particular
applications of these different trajectories, we will use our
calibration method with the Isocal phantom to more accurately model
the system matrix $\mathcal(H)$. Though the more exotic scanning
trajectories introduce more degrees of freedom that create greater
geometric uncertainty, our calibration procedure performs rather well
in determining what these deviations are from the self-reported
geometry metadata. For these trajectories, we found that
incorporating geometric calibration consistently improves image
quality.
* Axial field-of-view extension              :fov:
  :PROPERTIES:
  :ID:       eaae199f-f899-4862-af50-720895a31c36
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       7c250434-fff6-41a3-aea3-e7bc9ff88dc6
   :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
*** publications
    :PROPERTIES:
    :ID:       48459222-20e7-43e5-9863-5022a5803a1b
    :END:
**** cite:davis_extended_2013
     :PROPERTIES:
     :ID:       5b4c7bca-d59b-4f33-8151-a6b359071249
     :END:
- simulation study of axial FOV extension
**** cite:davis_verifying_2013
     :PROPERTIES:
     :ID:       d4c20a7d-4982-4318-b591-9ff84ee809f5
     :END:
- Trilogy scans of RANDO and Defrise phantom for axial FOV extension
**** cite:pearson_investigation_2013
     :PROPERTIES:
     :ID:       6ae09b4c-d1d3-4705-b110-8a4a0e1f33dd
     :END:
- Similar results to [[id:d4c20a7d-4982-4318-b591-9ff84ee809f5][cite:davis_verifying_2013]] using RANDO and Defrise
  Trilogy scans
**** cite:davis_we-g-brf-07:_2014
     :PROPERTIES:
     :ID:       3f9687ce-f913-43a0-8e96-0ace96d7f67c
     :END:
- AAPM talk using CLLC scan from TrueBeam
**** cite:davis_su-e-i-02:_2015
     :PROPERTIES:
     :ID:       15f62bff-3fae-4083-b4b1-ad0594d25121
     :END:
- AAPM poster for disk phantom metrics
**** cite:davis_non-circular_2015
     :PROPERTIES:
     :ID:       cee07d24-100a-4c78-a42d-59cd707cda3b
     :END:
- Varian meeting showing non-circular scans
** Introduction
   :PROPERTIES:
   :ID:       b815fcd4-92c6-4f72-9905-10acc22b580e
   :END:

** Methods
   :PROPERTIES:
   :ID:       b42e5e65-dfda-4692-8ea6-f6d96bc1dd5b
   :END:
*** Simulation
    Clinical extended-axial-FOV images are obtained by stitching
    together two circular scans at different axial locations. We first
    wanted to find the maximum axial coverage that can be achieved with
    such a trajectory. That is, what would be the maximum axial
    spacing between the two planes of the source's trajectory for
    which a useful extended volume image could be reconstructed?

    Simulating forward projections from this trajectory, we compared
    the images obtained from stitching together the independent FDK
    images to those obtained by reconstructing the two circles as a
    single trajectory with MLEM. We also compared stacked FDK images
    to reconstructions from the simulated *CLC* and smooth
    trajectories.

    We simulated a Defrise-style phantom modeled with the 3D X-ray
    projection software TAKE cite:seger_matlab/c_2005. The phantom was
    composed of a 15.2 cm outer diameter acrylic cylinder with
    alternating density disks of Delrin and cork 0.5 mm thick. This
    particular phantom with alternating density disks is acknowledged
    by the authors of FDK as being particularly susceptible to
    cone-angle artifacts cite:feldkamp_practical_1984.

    We used the TAKE software to forward project the phantom as well
    as generate a digitized "truth" phantom for calculating comparison
    metrics. The projector generates a forward projection from a
    specified trajectory given a mathematical definition of the
    phantom as well as its material properties and the spectrum
    generated by the x-ray source.

    We created projection data for a set of dual-circle trajectories
    that had an increasing amount of axial separation between the two
    circles. With a 1.5x magnification factor and a 30 cm detector
    size along the axial direction, a single circular scan has a
    maximum axial coverage of 20 cm in the image space. Furthermore,
    the maximum spacing between the two circles is 20 cm as any
    separation larger than this means the independent image volumes
    from the two circles are no longer contiguous. We therefore
    created trajectories with 10, 12, 14, 16, 18, and 20 cm
    separations *only show the larger gaps that are of interest?*
    between the planes of the source's dual-circle trajectory.

    We uniformly distributed 600 views over the entire trajectory
    which is comparable to the total number of views used in a single
    clinical CBCT scan with the kV imaging system. For the other two
    trajectories with a component of projection views taken during the
    axial translation (CLC and smooth), 600 views were used with 20% of
    the views being distributed along the axial translation stage.

    *FIX*

    The reconstruction image space consisted of a $256\times256$ transverse
    grid of 1 mm isotropic voxels. As the spacing between the circles
    increased, the number of voxels in the axial direction also
    increased to accommodate the increasingly large FOV.

    For the extended-volume reconstruction using the stacked FDK, we
    independently reconstructed each circular scan with FDK using a
    standard Hann filter. To combine the two reconstructed volumes for an
    extended axial-coverage image at a given spacing, we used the midplane
    between the two planes of the source's circular trajectory to select
    how much of each reconstruction to put in the combined image.

    For the MLEM reconstructions, we used all of the projection data
    simultaneously to reconstruct the extended volume. After defining the
    extended image volume, we computed the system matrix for each of the
    different spacings and trajectories based on the trajectory of the
    source and detector. We used 100 iterations *justify choice* of the
    MLEM algorithm to find an estimate for the image.


    *FIX*

    Our initial evaluation of the images obtained from non-circular
    trajectories is simply a qualitative visual inspection which does
    provide an informative assessment of the variety of artifacts that
    occur for a given reconstruction. For a more rigorous evaluation of
    the images obtained from different trajectories, we will use mutual
    information (MI) cite:pluim_mutual-information-based_2003 and the
    universal quality index (UQI) cite:wang_universal_2002 to provide a
    quantitative assessment of the image similarity between the reference
    image and the images from different trajectories.

*** Old FOV Paper Simulation
   :PROPERTIES:
   :ID:       6888683a-4d00-4cb2-be7e-8ac554af7595
   :END:
Since extended axial images in the clinic are obtained with two
circular scans at different axial locations, we wanted to find the
maximum axial coverage allowed by this dual-circle trajectory. That
is, what would be the maximum axial spacing between the two planes of
the source's trajectory for which a useful extended volume image could
be reconstructed? Using simulated forward projections from this
trajectory, we compared the images obtained from stacking the
independent FDK images to those obtained by reconstructing the two
circles as a single trajectory with MLEM. We also compared stacked FDK
images to reconstructions from the simulated CLC and smooth
trajectories.

**** Phantom
    :PROPERTIES:
    :ID:       bd73b61b-002b-481a-8b3a-712f1f2f1743
    :END:
The simulated phantom was a Defrise-style phantom modeled with the 3D
X-ray projection software TAKE cite:seger_matlab/c_2005. The phantom
was composed of a 15.2 cm outer diameter acrylic cylinder with
alternating density disks of Delrin and cork 0.5 mm thick. This
particular phantom with alternating density disks is acknowledged by
the authors of FDK as being particularly susceptible to cone-angle
artifacts cite:feldkamp_practical_1984.

**** Forward projection
    :PROPERTIES:
    :ID:       17f947de-c9f5-4313-ad70-3b77ff7ca929
    :END:
- [ ] Verify that the new version with the tested polychromatic
  behavior produces sinograms that are comparable to the ones we used
  in the initial study.

The forward projections of the phantom were acquired from the TAKE
software. The projector generates a forward projection from a
specified trajectory given a mathematical definition of the phantom as
well as its material properties and the spectrum generated by the
x-ray source. A digitized truth of the phantom was rendered by the
software to match the image space of the reconstruction for
calculating comparison metrics.

**** Trajectories
    :PROPERTIES:
    :ID:       5d84bea3-21b6-4311-a7de-ee73a956a152
    :END:
Given the 1.5 magnification factor and the 30 cm detector size along
the axial direction, a single circular scan has a maximum axial
coverage of 20 cm in the image space. Furthermore, the maximum spacing
between the two circles is 20 cm as any separation larger than this
means the independent image volumes from the two circles are no longer
contiguous. We therefore created trajectories with 10, 12, 14, 16, 18,
and 20 cm separations *only show the larger gaps that are of
interest?* between the planes of the source's dual-circle trajectory.

We created projection data for a set of dual-circle trajectories that
had an increasing amount of axial separation between the two
circles. We uniformly distributed 600 views over the entire trajectory
which is comparable to the total number of views used in a single
clinical CBCT scan with the kV imaging system. For the other two
trajectories with a component of projection views taken during the
axial translation (CLC and smooth), 600 views were used with 20% of
the views being distributed along the axial translation stage.

***** TAKE check
:PROPERTIES:
:ID:       2bacd921-aa76-4225-9992-4b8f5b1f3198
:END:
- [ ] Verify new projection data matches old sinograms.
- [ ] Verify the number of views that compose the trajectory.
- [ ] If all of the projection information matches, use the
  reconstructions I presented in my proposal.

**** Reconstruction
    :PROPERTIES:
    :ID:       1c0fa74f-e955-492e-a1d4-bfb81cd21cbf
    :END:
The reconstruction image space consisted of a $256\times256$ transverse
grid of 1 mm isotropic voxels. As the spacing between the circles
increased, the number of voxels in the axial direction also increased
to accommodate the increasingly large FOV.

For the extended-volume reconstruction using the stacked FDK, we
independently reconstructed each circular scan with FDK using a
standard Hann filter. To combine the two reconstructed volumes for an
extended axial-coverage image at a given spacing, we used the midplane
between the two planes of the source's circular trajectory to select
how much of each reconstruction to put in the combined image.

For the MLEM reconstructions, we used all of the projection data
simultaneously to reconstruct the extended volume. After defining the
extended image volume, we computed the system matrix for each of the
different spacings and trajectories based on the trajectory of the
source and detector. We used 100 iterations *justify choice* of the
MLEM algorithm to find an estimate for the image.

**** Evaluation
:PROPERTIES:
:ID:       7b8e3df7-2509-4e65-8e5a-9b5a468322d8
:END:
Our initial evaluation of the images obtained from non-circular
trajectories is simply a qualitative visual inspection which does
provide an informative assessment of the variety of artifacts that
occur for a given reconstruction. For a more rigorous evaluation of
the images obtained from different trajectories, we will use mutual
information (MI) cite:pluim_mutual-information-based_2003 and the
universal quality index (UQI) cite:wang_universal_2002 to provide a
quantitative assessment of the image similarity between the reference
image and the images from different trajectories.

** Results
   :PROPERTIES:
   :ID:       b2a353e8-8531-4a0e-8337-9f702ecf02f8
   :END:
*** Simulation
:PROPERTIES:
:ID:       ff434d74-757c-47b8-bd98-9250d2751ff2
:END:
The initial simulation results demonstrated promising advantages to
using the optimization-based reconstruction to produce extended-axial
coverage images.

- [ ] insert comparison table of FDK
- [ ] show plot of RMSE in the overlap region

From the results shown in Figure (\ref{fig:})
*** Data
**** notebooks
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/fov/em_vs_fdk.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/fov/em_vs_fdk.ipynb]]
** Conclusion
   :PROPERTIES:
   :ID:       99a861bc-c072-4082-806f-9279fa7c3a3c
   :END:
* Collision-avoiding trajectories            :col:
  :PROPERTIES:
  :ID:       99055e18-4b61-404e-9408-ebd5fd0a5d8d
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       53a46fd0-a854-4b6a-a253-dde04d4f7a87
   :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
*** publications
    :PROPERTIES:
    :ID:       32703eae-6f65-4a6a-9f23-813e60747126
    :END:
- 2015 MIC virtual isocenter
- 2016 CT meeting dyanmic magnification
- 2016 MIC mixed magnification
- 2017 Varian dynamic magnification
** Introduction
   :PROPERTIES:
   :ID:       d5a22c7a-f72e-4a1c-b90f-69f7084d42e1
   :END:
The addition of a linac-mounted, kV-imaging, cone-beam computed
tomography (CBCT) system to the gantry-mounted clinical linear
accelerator (linac) helped this modality become the most popular form
of image-guided radiation therapy (IGRT)
cite:rahman_linac:_2015,jaffray_flat-panel_2002-1,letourneau_cone-beam-ct_2005.
The tomographic information provided in the kV energy range improves
soft-tissue contrast resolution over that provided by the MV
electronic portal imaging device (EPID) alone
cite:jaffray_radiographic_1999. The linac-mounted, kV-imaging, CBCT
system not only helps with patient setup and target verification, but
it also allows the monitoring of the tumor response during treatment
cite:dawson_advances_2007.

There is therefore a loss in clinical utility when it is not possible
to obtain tomographic information from the kV-imaging CBCT system. One
situation in which this can occur is when a collision between the
patient and the gantry arises cite:padilla_collision_2015. These may
be of particular concern in breast and lung cancer patients where the
arm positions leads to a possible collision as shown in Figure ([[ref:fig:barbie_collision][barbie
collision]]). Collisions also present a problem in treatment of
posterior and lateral lesions in stereotactic body radiosurgery
(SBRT). Similarly in prone breast treatments, where the target is near
the couch top and a lateral couch translation is needed to bring the
target to isocenter, collision with the contralateral side of the
patient may occur. When collisions do occur, the angular range
available for scanning is restricted and it is not possible to acquire
complete projection data.

#+CAPTION: Example of a typical patient-gantry collision. The patient's arms for this treatment position can often collide with the MV-treatment head.
#+ATTR_LaTeX: :width \columnwidth :placement [t]
#+LABEL: fig:col_barbie
[[./figures/col/barbie.jpg]]

The analytic-based FDK reconstruction algorithm, still the clinical
workhorse for CT reconstruction, imposes certain requirements on the
acquisition trajectory cite:feldkamp_practical_1984,pan_why_2009.
Specifically, the inverse problem is derived from a circular scanning
trajectory, and such a trajectory must be acquired to meet the data
sufficiency conditions. A potential patient-gantry collision
restricting the angular coverage could prevent sufficient projection
data from being acquired.

Though there has been previous work in developing analytic methods for
addressing the inverse problem from some novel trajectories
cite:katsevich_image_2004,katsevich_theoretically_2002,katsevich_image_2005,katsevich_formulation_2006,
it could be clinically useful to enable reconstruction from an
arbitrary, collision-avoiding trajectory. As the collision region (if
one arises) is contingent on the patient's treatment position, the
imaging trajectory would then vary on a per patient basis. As such,
deriving the analytic inverse for each patient's scanning trajectory
would be impractical in a clinical work flow.

Advances in optimization-based reconstruction algorithms provide a
potential means of enabling reconstruction from patient-specific
collision avoiding trajectories
cite:bian_optimization-based_2013,han_optimization-based_2012. The
imaging model is formulated as the linear transform
\begin{equation}
  \label{eq:linmodel}
  \mathbf{g}=\mathcal{H}\mathbf{f},
\end{equation}
where $\mathbf{g}$ is the discrete $M$ pixel sampled projection on the
detector, $\mathcal{H}$ is the $M\times N$ discrete form of the X-ray
transform, and $\mathbf{f}$ is the object function represented on a N
voxel basis. As the direct inversion of Equation ([[ref:eq:linmodel][linear model]]) is
impractical due to both its size and inconsistencies from factors such
as noise, optimization techniques are used to solve this system for an
estimate of the object $\widetilde{\mathbf{f}}$.

These optimization-based methods impose no restrictions on the
trajectory of the acquired views. Provided the geometry of each view
is correctly incorporated into the system matrix $\mathcal{H}$, these
robust algorithms can provide clinically useful reconstructions from
acquisitions that fail to meet the stipulated data conditions assumed
in the formulation of an analytic inverse.

In this study, we investigate examples of potential scanning
trajectories that would allow the acquisition of sufficient projection
information for a clinically useful image while avoiding a potential
patient collision with the gantry. As the gantry rotates, there are
two components of the linac that are potential sources of patient
collisions. These are the MV-treatment head, shown in Figure ([[ref:fig:barbie_collision][barbie
collision]]), and the kV-CBCT detector.

One trajectory that would avoid a patient collision with the
MV-treatment head is a virtual isocenter trajectory. This trajectory
resolves such a collision by increasing the effective source-to-axis
distance (SAD). By using this increased SAD for an imaging trajectory,
the clearance between the patient and the MV-treatment head as the
gantry rotates is increased and the collision is avoided.

The virtual isocenter trajectory utilizes synchronized gantry rotation
and couch translation to maintain a fixed distance (``virtual SAD'')
between the MV source and a chosen center of rotation (``virtual
isocenter'') in the patient. At the beginning of the scan, the patient
is moved away from the linac head along the MV beam direction. As the
gantry rotates, the couch continuously moves away from the linac head
to maintain the specified separation as shown in Figure
([[ref:fig:col_virtiso]]). The virtual SAD can be chosen large enough so
that collisions as shown in Figure ([[ref:fig:col_barbie]]) are
avoided; at this point in the trajectory, the couch would have moved
far enough to the left to avoid the collision.

#+BEGIN_EXPORT latex
\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/col/gantry_6angles_a.pdf}
  \caption{
    Patient, kV and MV beams and kV detector at several angles during a virtual isocenter rotation.  Room coordinate system (dotted axes) has its origin at mechanical isocenter, also the intersection of the MV (red) and kV (green) beam axes.  As the gantry rotates, the patient (filled contour) is continually shifted to maintain a specified distance along the MV beam direction between the mechanical isocenter and the chosen virtual isocenter (circle symbol within the patient).  The path of the virtual isocenter is a circle about the mechanical isocenter, with radius equal to the chosen shift (12 cm from the isocenter in this example).  Detector may or may not be shifted as shown, depending on virtual isocenter position and patient geometry.
    \label{fig:col_virtiso}}
\end{figure*}
#+END_EXPORT

Another trajectory that could avoid a patient collision with the kV
detector would be one during which either the patient of the detector
is moved during the scan in the angular range of a collision. Either
solution leads to dynamic magnification scan where the kV-CBCT imaging
magnification changes during the acquisition. Again,
optimization-based reconstruction methods can easily handle such a
change in magnification provided the projection information is
correctly incorporated into the system matrix.

Finally, we study a trajectory that combines both the virtual
isocenter and the dynamic magnification trajectories to create a
hybrid scanning acquisition that would alleviate collisions with both
the MV-treatment head and the kV-CBCT detector (note that it is only
the distance to the linac head that is increased in the plain virtual
SAD technique; the distance from the kV source and detector to the
patient and to each other are unchanged). We use such a trajectory as
an example of a patient-specific scanning trajectory designed to avoid
a particular collision that arises with a particular treatment
position.

** Methods
   :PROPERTIES:
   :ID:       9d2ba9ad-7739-46ab-9983-754fa6adac28
   :END:
*** Virtual isocenter trajectory
The virtual isocenter trajectory utilizes synchronized gantry rotation
and couch translation to maintain a fixed distance (``virtual SAD'')
between the MV source and a chosen center of rotation (``virtual
isocenter'') in the patient. At the beginning of the scan, the patient
is moved away from the linac head along the MV beam direction. As the
gantry rotates, the couch continuously moves away from the linac head
to maintain the specified separation as shown in Figure
([[ref:fig:col_virtiso]]).

*** Dynamic magnification trajectory

*** Moving toward generalized trajecotries
** Results
   :PROPERTIES:
   :ID:       a59ef4e0-4966-4c76-b283-5aea6b92360e
   :END:
*** Virtual isocenter trajectory
*** Dynamic magnification trajectory
*** Moving toward generalized trajecotries

** Conclusion
   :PROPERTIES:
   :ID:       8784656f-c169-410a-9a76-0454c6ab5dde
   :END:

* Summary and conclusions                    :conc:
  :PROPERTIES:
  :ID:       1bade25b-80d6-4650-b8a3-baf370fa657c
  :END:

\makebibliography
