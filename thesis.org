#+TITLE:
#+DATE:
#+AUTHOR:
#+EMAIL:
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:nil d:(not "LOGBOOK") date:nil e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t
#+OPTIONS: tags:nil tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+LATEX_CLASS:thesis
#+STARTUP: hideblocks
# +STARTUP: latexpreview

#+BEGIN_EXPORT latex
%% Use these commands to set biographic information for the title page:
\title{Enabling Novel IGRT Imaging Trajectories with Optimization-Based Reconstruction Algorithms}
\author{Andrew Davis}
\department{Committee on Medical Physics}
\division{Biological Sciences}
\degree{Ph. D.}
\date{October, 2017}

%% Use these commands to set a dedication and epigraph text
\dedication{Dedication Text}
\epigraph{Epigraph Text}

% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
\makededication
\makeepigraph

%% Make the various tables of contents
\tableofcontents
\listoffigures
\listoftables

\acknowledgments

*EXPAND* Funding was provided in part by Varian Medical Systems, the Lawrence
H. Lanzl Fellowship (to A. D.), and NIH Grants R01 CA182264, R01
EB018102, S10 RR021039 and P30 CA14599. We are grateful to Pascal
Paysan and Dieter Seghers (also Varian) for providing and assisting
with the iTools Reconstruction software. The contents of this work are
solely the responsibility of the authors and do not necessarily
represent the official view of any of the supporting organizations.
The authors have no relevant conflicts of interest to disclose.

\abstract
% Enter Abstract here

\mainmatter
% Main body of text follows
#+END_EXPORT

* notes                                      :noexport:
  :PROPERTIES:
  :ID:       7f3d97de-795e-402a-82ac-591717f86bfd
  :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
** requirements
   :PROPERTIES:
   :ID:       931c9c50-bfaf-4c8e-b2cc-bcfdf62e327d
   :END:
- [[http://www.lib.uchicago.edu/e/phd/][uchicago]] dissertation guide
- [[https://github.com/zuwiki/ucetd-latex][uoc thesis]] template
* Introduction                               :intro:
  :PROPERTIES:
  :ID:       852796c3-9a3b-49da-bc08-1299e93e0768
  :END:
Tomography is the imaging technique of using a penetrating wave to
create an image of a slice in an object while either blurring or
obscuring details from other planes in the object. The ability to peer
inside an object and create a map of its contents is a powerful tool
that is routinely used in myriad applications. Today, tomographic
methods can be found being deployed in locations ranging from
border-control checkpoints to local medical clinics.

As the non-invasive nature of tomographic imaging had obvious benefits
for the field of medicine, a lot of significant advances in
tomographic technology were driven by clinical research. One such form
of tomographic imaging is x-ray computed tomography (CT) which uses
projection images acquired from different locations around the object
to compute the distribution of material densities inside the object.
With the growth of tomographic imaging, additional technologies were
developed to acquire images from a variety of waves from ultrasound to
injecting radioactive tracers which emit these waves from inside a
patient as in single-photon computed tomography (SPECT) and positron
emission tomography (PET).

** CT development history
   :PROPERTIES:
   :ID:       1b7b31c8-4bd5-4402-b930-d81984ee5901
   :END:
In a CT scanner system, an x-ray source and opposing detector
typically rotate in a circle relative to the object being imaged as
x-ray projection images are acquired at different angular positions.
By modeling the attenuation of the incident x-rays by the object being
imaged at different projection angles and finding an approximate
inversion of this model, an estimate of the objects interior could be
produced. The development of modern CT imaging systems today was
driven not only by innovation in the hardware design, but also by
advances in the algorithms used to invert the poorly conditioned
forward model encountered in CT imaging.

Figure ([[ref:fig:intro_hist_1st_gen]]) shows a schematic of the first
generation of CT scanner, which was built by EMI (/Electric and
Musical Industries Ltd./) in 1967 using the work of Allan M. Cormack
and Godfrey N. Hounsfield cite:buzug_thorston_m._milestones_2008. In
this design, a source and detector acquire a series of pencil-beam
projections by translating together along a line in a plane orthogonal
to the rotation axis. The source and detector then rotate before
acquiring another series of projections along another line. This
process is repeated a successive rotation angles until the system has
rotated an entire $180^{{\circ}}$ and the source and detector have moved to
each others starting position. For their work, Cormack and Housfield
shared the 1979 Nobel Prize in Physiology or Medicine
cite:hsieh_computed_2009.

#+CAPTION: First-generation CT scanner. At each angle, the source and detector move together in a line to acquire a series of pencil beam projections at that angle. The source and detector then rotate one degree around the object before repeating this process.
#+ATTR_LaTeX: :width 0.5\textwidth
#+LABEL: fig:intro_hist_1st_gen
[[file:figures/intro/1st_gen_ct.png]]

The algorithms for reconstructing the tomographic image from these
x-ray projections at different angle evolved concurrently with the
different CT-scanner hardware. The projection data acquired from the
first-generation scanner was necessarily digitized as the
reconstructed image was computed by solving a linear system of
equations which was what is now known as algebraic tomographic
reconstruction (ART) cite:hsieh_computed_2009. Though Radon's theories
that provide the basis for some of the analytic-based reconstruction
algorithms were known at the time, the necessary computer hardware
needed for a practical implementation had yet to be developed when
Cormack and Hounsfield reconstructed their first CT image
cite:buzug_thorston_m._milestones_2008.

Though the first-generation CT system was a groundbreaking
achievement, the rasterized scanning of the pencil beam at multiple
angles required approximately 4.5 minutes to acquire the projection
information to reconstruct a single two-dimensional (2D) slice of the
scanned object cite:hsieh_computed_2009. The second generation of CT
scanners significantly reduced this acquisition time to about 30
seconds by replacing the pencil beam of x-rays with a fan beam and a
detector array with around 30 detector elements. This new design,
shown in Figure ([[ref:fig:intro_hist_2nd_gen]]), still required the
source and detector to acquire projections using a linear translation
before rotating to a new angular position and repeating the process
cite:buzug_thorston_m._milestones_2008.

#+CAPTION: Second-generation CT scanner. Though similar to the first-generation scanner in Figure ([[ref:fig:intro_hist_1st_gen]]), a significant reduction in acquisition time was achieved by replacing the pencil beam and single detector with a fan beam and an array of detector elements. The acquistion method remained the same in that the source and detector first acquired multiple projections via a linear translation before rotating to a new angular position and repeating the translation.
#+ATTR_LaTeX: :width 0.5\textwidth
#+LABEL: fig:intro_hist_2nd_gen
[[file:figures/intro/2nd_gen_ct.png]]

One of the most important factors driving the hardware development in
CT-scanner technology was the issue of acquisition speed. As the
reconstruction framework used a forward model that assumes the
projections are acquired from a stationary object, the first two
generations were only successful in imaging parts of the patient that
were relatively stationary. As such, these early scanners were
initially only used with the cranium as it is relatively motionless
relative to the required acquisition time. Unlike the cranium, other
parts of patient anatomy such as the thorax and abdomen are greatly
affected by cardiac and respiratory. The next generation of scanners
were designed to reduce the acquisition time to under 20 seconds in
order to image a patient's abdomen in a single breath hold
cite:buzug_thorston_m._milestones_2008.

#+CAPTION: Third-generation CT scanner. This design kept the fan-beam x-ray source seen in the second generation of scanners, but the linear detector array was curved. This new design no longer required the source and detector to acquire a linear translation before rotating to a new scanning angle. Instead, this generation of scanners simply acquire a single projection at each angle which significantly reduces the acquisition time.
#+ATTR_LaTeX: :width 0.7\textwidth
#+LABEL: fig:intro_hist_3rd_gen
[[file:figures/intro/3rd_gen_ct.png]]

The third generation of CT scanners, shown in Figure
([[ref:fig:intro_hist_3rd_gen]]), would soon become the most popular
generation of CT scanners. This remains true today as most modern
diagnostic CT imaging systems, such as the one shown in Figure
([[ref:fig:intro_ct_scan]]), are still based on the third generation
design. Unlike the earlier generations of scanners, this design
removed the need for the linear translation of the source and detector
by replacing the flat array of detector elements with a curved array
of detector elements. By doing this, it was only necessary to acquire
a single projection at each rotation angle which significantly reduced
the acquisition time cite:hsieh_computed_2009.

The evolution from the first generation of scanners to the second and
third generations of scanners was enabled by advances in both computer
hardware and reconstruction algorithms for solving the inverse
problem. As we will further discuss in following chapter,
analytic-based algorithms for solving the CT inverse problem gradually
replaced the initial algebraic solution to the linearized forward
model used by Cormack and Hounsfield. The most popular form of this
implementation is known as filtered-backprojection (FBP) cite:buzug_thorston_m._two-dimensional_2008,hsieh_computed_2009.

The FBP approach to CT reconstruction was first implemented as the
parallel-beam backprojection algorithm
cite:buzug_thorston_m._two-dimensional_2008. This provided an analytic
inverse to the acquisition method of the first generation of scanners
where at a given angle, all of the projections are acquired as
parallel incident x-ray beams. However, with the second and third
generation of CT scanners, this imaging model changed to account for
the divergent x-ray beam of a point-like x-ray source on an array of
x-ray detectors. The new fan-beam FBP algorithm enabled the scanning
geometry of the third generation of scanners which are still the
backbone of clinical CT today cite:pan_why_2009.

#+CAPTION: Modern diagnostic CT imaging machine.
#+ATTR_LaTeX: :width 0.5\textwidth
#+LABEL: fig:intro_ct_scan
[[file:figures/intro/US_Navy_cat_scan.jpg]]

The fourth generation of CT scanners was developed to eliminate ring
artifacts that can appear the third-generation CT scanners. These ring
artifacts can occur when there is a mismatch in projection data of
opposing rays along the same line in the patient which can occur from
misalignment of the moving detector. With a stationary ring of
detectors, these ring artifacts are eliminated. However, with the
advent of multi-slice detector technology which will be discussed in
[[id:f84fb81e-a07d-4945-9653-fd1544703733][Cone-beam CT and new scanning trajectories]], the engineering and cost
requirements will likely result in these scanners being phased out
cite:hsieh_computed_2009.

#+CAPTION: Fourth-generation CT scanner. In this design, only the x-ray source rotates inside a ring of fixed detector elements.
#+ATTR_LaTeX: :width 0.5\textwidth
#+LABEL: fig:intro_hist_4th_gen
[[file:figures/intro/4th_gen_ct.png]]

The fifth generation of CT scanners, also known as electron-beam,
computed-tomography (EBCT) scanners, was developed in the early 1980s
for cardiac imaging. In order to acquire the projection data fast
enough (20-50 ms), it would be impossible to design a mechanical
system that could rotate that fast and withstand the centripetal force
incurred with such high rotational velocity. Instead, this generation
was designed to drive the electron beam onto the x-ray anode that was
curved around the patient -- effectively placing the patient inside
the x-ray tube. The design is similar to the fourth generation in that
the EBCT scanners have a fixed, partial-ring detector around the
patient cite:buzug_thorston_m._milestones_2008,hsieh_computed_2009.

Though the fourth and fifth generation scanners are interesting
manifestations of CT scanning technology, they are only included here
for completeness and will not be discussed further. In the following
section, we will look at a major development in reconstruction
algorithm technology that enabled the the third-generation CT
scanner's popularity. Furthermore, the changes in the third-generation
CT scanner design this enabled provided the development of a new CT
geometry which is the focus of this work.

** Cone-beam CT and new scanning trajectories
   :PROPERTIES:
   :ID:       f84fb81e-a07d-4945-9653-fd1544703733
   :END:
For all the CT scanners discussed in the previous section [[id:1b7b31c8-4bd5-4402-b930-d81984ee5901][CT
development history]], the only scanning trajectory utilized for CT was
the circular rotation of the source and detector around the patient.
This limitation was due to both the hardware geometry and the
reconstruction algorithms that were initially focused on acquiring and
reconstructing 2D-planar slices of the object being imaged.
Unfortunately, this slice-by-slice acquisition and reconstruction
framework was somewhat limiting in acquiring volumetric CT images. 

The use of new scanning trajectories to increase the clinical utility
of CT began with the development of the spiral or helical CT
reconstruction algorithm
cite:kalender_spiral_1990,kudo_helical-scan_1991,katsevich_theoretically_2002,katsevich_exact_2004.
By adding longitudinal translation of the patient couch through a
third-generation scanner, it was possible to acquire a helical
trajectory of the source and detector around the patient. This made it
possible to rapidly acquire multi-slice (or volumetric) CT of a
patient using the existing diagnostic imaging hardware of the
third-generation scanners.

Another approach acquire volumetric tomographic images was to extend
the CT detector array in the longitudinal direction by adding
additional rows of detector arrays. These multi-array detectors helped
to improve the interpolation procedure used for reconstructing the
data acquired from a helical scan, and continue to be used in modern
third generation CT scanners. As these multi-array detectors began to
cover larger extents of the axial FOV, they eventually led to large
flat-panel detector rays being used to acquire projection information.
These new flat-panel detector systems are now known as cone-beam CT
(CBCT) systems to reflect the cone of x-ray illumination on these
detectors as opposed to the fan-beam geometry of the earlier
slice-by-slice scanners.

With the advent of CBCT scanners, efforts were made to modify the FBP
algorithm so that it could be extended in three dimension (3D)
cite:grangeat_mathematical_1991,kudo_derivation_1994,kudo_fast_1998,buzug_thorston_m._three-dimensional_2008.
Though all of these methods attempted to find an exact analytic
inverse imaging model, this required obtaining exact Radon data which
is not the case for the circular trajectory routinely employed by
third generation scanners. It was the development of a modified FBP
algorithm by Feldkamp, Davis and Krees or FDK
cite:feldkamp_practical_1984 (which we will discuss further in
[[id:04DD4E55-A20B-4A27-BBDD-BB493DD82674][Analytic-based reconstruction]]) that made it possible to obtain a
useful reconstruction from a circular scanning trajectory on a CBCT
system.

** Image-guided radiation therapy
X-ray technology is unique in how rapidly it was applied to the field
of medicine following the discovery of x-rays by Wilhelm
R\text{\"o}ntgen in 1895. The next year in Chicago, Emil Grubbe built
his own x-ray device which he began to use for therapeutic purposes
cite:mukherjee_emperor_2010. Both diagnostic and therapeutic uses of
radiation developed in concert throughout the 20^th century culminating
in radiation treatment devices that combine low-energy CT imaging with
high-energy treatment beams in image-guide radiation therapy. These
image-guided radiation therapy (IGRT) linear accelerators (linacs)
allow for the delivery of powerful megavoltage (MV) treatment beams to
diseased tissue while using kilovoltage (kV) CT techniques to both
localize the tissue and evaluate the tissues response to therapy
cite:oldham_cone-beam-ct_2005,xing_overview_2006,saw_performance_2007.

#+CAPTION: Annotated image of a TrueBeam linac. On the treatment couch is the CIRS torso phantom aligned at the mechanical isocenter using the laser guidance system. Above the torso phantom to the left is the round MV treatment head with the metallic accessory mount and beam exit window. Below the table to the left is the kV source which provides the kV x-rays for the kV-CBCT imaging system. Above the phantom to the right is the kV detector panel which acquires the projections through the phantom for the kV-CBCT imaging system. Both the kV source and kV detector are mounted on robotic position arms. Below the phantom to the right is the MV EPID, which is retracted in this image, for acquiring MV projections from the MV treatment beam. Finally all of these components are mounted on a rotating gantry which can rotate $360^{\circ}$ around the mechanical isocenter for a single rotation. A subsequent rotation must occur in the opposite direction as the gantry lacks the ability to make multiple rotations in the same direction like a diagntostic-CT system due to the complexity of the MV linac design.
#+ATTR_LaTeX: :width \textwidth
#+LABEL: fig:intro_linac
[[file:figures/intro/annotated_linac.png]]

The addition of a linac-mounted, kV-imaging, cone-beam computed
tomography (CBCT) system to the gantry-mounted clinical linear
accelerator (linac)
cite:jaffray_flat-panel_2002,letourneau_cone-beam-ct_2005,rahman_linac:_2015
helped this modality become the most popular form of image-guided
radiation therapy (IGRT)
cite:xing_overview_2006,bissonnette_quality_2012,dawson_advances_2007.
Figure ([[ref:fig:intro_linac]]) shows an annotated image of a TrueBeam
linac. The tomographic information provided in the kV energy range
improves soft-tissue contrast resolution over that provided by the MV
electronic portal imaging device (EPID) alone
cite:jaffray_radiographic_1999. The linac-mounted, kV-imaging, CBCT
system not only helps with patient setup and target verification, but
it also allows the monitoring of the tumor response during treatment
cite:oldham_cone-beam-ct_2005,xing_overview_2006,dawson_advances_2007.

While there are many advantages in using linac-mounted CBCT imaging
systems in IGRT, there are still limitations in the current clinical
workflow that could be alleviated by utilizing non-circular scanning
trajectories with optimization-based reconstruction. One issue is the
limited axial FOV coverage provided by the current detectors and
circular scanning trajectory. Another issue are possible patient
collisions with the rotating treatment gantry. In this work, we will
focus exclusively on utilizing a generalized optimization-based
reconstruction framework from arbitrary CBCT trajectories to address
these clinical issues for IGRT. However, the framework itself is not
necessarily restricted to IGRT and could be of potential use for a
variety of other CBCT applications.

** Organization
   :PROPERTIES:
   :ID:       252a18dd-1210-4360-b082-fce5510334ab
   :END:
In this work, we will discuss the optimization-based,
image-reconstruction framework that enables the use of these new
scanning trajectories. In particular, we will focus on how this
approach was developed in addressing the two clinical bottlenecks of
limited axial FOV coverage and patient collisions with the linac
gantry. By using these two examples, we will hopefully not only show
the feasibility of using these non-circular trajectories, but also a
potential solution to existing clinical needs.

First, we will discuss the framework and considerations of using
optimization-based reconstruction with different scanning trajectories
in [[id:06ec01f2-e128-4baf-9ec7-4569a3aaa886][Optimization-based algorithms]]. Next, we will look at the need for
geometric calibration and discuss a method we developed to do this for
these trajectories in [[id:652970b8-4916-4190-b83b-2d6ae117c8b3][Geometric calibration]]. We will then look at
using new trajectories to address the limited axial FOV issue in [[id:eaae199f-f899-4862-af50-720895a31c36][Axial
field-of-view extension]] followed by using these trajectories to
alleviate the issue of patient collisions in [[id:99055e18-4b61-404e-9408-ebd5fd0a5d8d][Collision-avoiding
trajectories]]. Finally, we will summarize this work and discuss
possible clinical considerations with this methodology in [[id:1bade25b-80d6-4650-b8a3-baf370fa657c][Summary and
conclusions]].

* General CBCT trajectory reconstruction framework  with optimization-based algorithms :opt:
  :PROPERTIES:
  :ID:       06ec01f2-e128-4baf-9ec7-4569a3aaa886
  :END:
Through the years of CT research, a fundamental questions has always
been how to move the source and detector of the imaging system
relative to the object to obtain sufficient projection information to
reconstruct a useful image. Part of this answer must take into account
certain engineering limitations that go into building such a system.
However, this is fundamentally a question that must address the
requirements of the computational reconstruction algorithm used to
assemble the image from the x-ray projections.

Though the mathematical framework for solving this inverse problem had
been formulated by Johann Radon in 1917 and a patent for what is
essentially CT was filled in 1940 by Gabriel Frank, it was not until
1967 that the work of Allan M. Cormack and Godfrey N. Hounsfield led
to the first clinical CT scanner. For their work, Cormack and
Housfield shared the 1979 Nobel Prize in Physiology and Medicine
cite:hsieh_computed_2009.

Optimization-based image reconstruction provides robust framework for
reconstructing from projections acquired with the task-specific
imaging trajectories we studied . Unlike the analytic-based methods,
such as FDK algorithm cite:feldkamp_practical_1984, optimization-based
methods require no assumptions of the initial scanning trajectory. As
such, there is greater flexibility in choosing a scanning trajectory
for a given task for which an analytic inverse has yet to be derived.
In this work, we investigate some of the task-specific scanning
trajectories enabled by optimization-based methods.

The use of optimization-based methods for tomographic image
reconstruction is a natural extension of linearizing the x-ray
transform imaging model of tomographic scan. Approaching the image
reconstruction problem as a linearized imaging model has existed since
the first CT system built by Cormack and Hounsfield. For their initial
image reconstructions, they utilized the algebraic reconstruction
technique (ART) to solve a system of equations created by the
summation of the rays through the image pixel grid at each projection
angle cite:herman_art:_1973.

Though the initial optimization-based image reconstruction with ART
was successful in providing a solution to the inverse problem, the
limited computation power available at the time proved to be an
intractable limitation. This computational complexity was further
increased when moving from two-dimension (2D), single-slice images to
the three-dimensional (3D), volumetric images reconstruction which
introduce a greater number of unknowns. The computationally-intensive
nature optimization-based reconstructions has been one of the biggest
barriers preventing the widespread use of these reconstruction methods
today cite:pan_why_2009. However, a recent renaissance of utilizing
graphics processing units (GPUs) -- technology once solely in the
purview of video games -- for scientific computation has made these
optimization-based methods temporally competitive with analytic-based
methods cite:xu_accelerating_2005,sharp_gpu-based_2007.

** Background: Cone-beam computed tomography
   :PROPERTIES:
   :ID:       d136ffd1-6def-4c22-85ed-6049f04b8486
   :END:
*** Analytic-based reconstruction
    :PROPERTIES:
    :ID:       04DD4E55-A20B-4A27-BBDD-BB493DD82674
    :END:
Analytic-based reconstruction algorithms are formulated by explicitly
finding an inverse to the X-ray transform
\begin{equation}
  \label{eq:xray}
  g(\mathbf{r}_0,\hat{\theta})=\int_0^{\infty}f(\mathbf{r}_0+t\hat{\theta})dt,
\end{equation}
where the data function $g$ is acquired by integrating along the ray
from the source at $\mathbf{r}_0$ in the direction $\hat{\theta}$ through
the object function $f$. A fundamental problem with these
reconstruction algorithms when practically reconstructing $f$ is the
assumption of a continuous-to-continuous (CC) model. These
analytic-based reconstruction algorithms impose dense sampling
requirements for both the detector and number of views to approximate
a continuous data function. Given that the data function from the
digital detector and the numerical array for storing the reconstructed
image are both discrete, a more natural approach to the inverse
problem would be a discrete-to-discrete (DD) imaging model
cite:barrett_foundations_2003.

#+LABEL: fig:opt_analytic
#+BEGIN_SRC asymptote :file figures/opt/analytic.pdf :exports results :tangle no
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import geometry;
import solids;
import three;

// view configuration
size(10cm);
currentprojection=orthographic(-15,10,20,up=Y);
// currentprojection=perspective(-15,5,13,up=Y);
// currentlight=White;

// Draw axis
real ax_scale=15;
draw(Label("$z$",position=EndPoint,align=N),(0,0,0)--(0,ax_scale,0),black,Arrow3);
draw(Label("$x$",position=EndPoint,align=S),(0,0,0)--(ax_scale,0,0),black,Arrow3);
draw(Label("$y$",position=EndPoint,align=SW),(0,0,0)--(0,0,-ax_scale),black,Arrow3);

// show gantry angle
draw(Label("$\theta_{g}$", (2, -0.5, 0)), arc((0, 0, 0), (ax_scale/3, 0, 0), (0, -ax_scale/3, 0)), red, arrow=Arrow3);

// kV schematic
real dlat=0, dlng=0, dvrt=50;
triple det_cent=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det_cent-(0,ulen/2,vlen/2));

triple det0 = det_cent-(0,ulen/2,vlen/2);
triple u = (det0+ax_scale/2*(0,1,0));
triple v = (det0+ax_scale/2*(0,0,1));
triple w = (det0+ax_scale/2*(-1,0,0));

// detector norm
triple dnorm = (det_cent+ax_scale*(-1,0,0));

// detector coordinate system
draw(det0--u,black,Arrow3,L=Label("$u$", position=EndPoint, align=W));
draw(det0--v,black,Arrow3,L=Label("$v$", position=EndPoint, align=N));
draw(det0--w,black,Arrow3,L=Label("$w$", position=EndPoint, align=N));

draw(detector, black);

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// lines from source to detector edges
draw(src..det_cent-(0,-ulen/2,-vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,-ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,-vlen/2),black+opacity(0.15));

// ray connecting the source to the detector
triple ray_sd = det_cent-src;
draw(L=Label("$\vec{r}_{sd}$", position=EndPoint, align=E), src--det_cent, blue, Arrow3);

// dot product of ray onto normal vecotr
real along = dot(ray_sd, dnorm);

// detector projection operator
transform3 proj=planeproject(detector);

// show pierecing point
triple pierce = proj*src;

draw(L=Label("$\vec{p}$", position=EndPoint, align=NW),src--pierce,blue+dashed,arrow=Arrow3);
draw(L=Label("$\vec{p}_{uv}$", position=EndPoint, align=SE),det0--pierce,red+dashed,arrow=Arrow3);
// draw(src--pierce,red+dotted, arrow=Arrow3);

///////////////////////////////////

settings.render = 0;
import geometry;
// size(8cm,0);
// unitsize(1cm)

// Affichage du repère par défaut (O,vec{i},vec_{j})
// show(defaultcoordsys);
// show(currentcoordsys);

// detector
real dlat=0, dlng=0, dvrt=50;
point det=(dvrt,dlat);

real ulen=40.0, vlen=30.0;

draw((dvrt,-ulen/2+dlat)--(dvrt,ulen/2+dlat),black);

// source
real slat=0, slng=0, svrt=-100;
point src=(svrt,slat);

draw(src--(dvrt, 0), dashed+red);
draw(src--(dvrt, -ulen/2+dlat), dashed+black);
draw(src--(dvrt, ulen/2+dlat), dashed+black);
dot("Source", src, N, red);

addMargins(0.5cm, 0.5cm);

// dot("Detector",det,N,5bp+.5blue);
// dot("Source",src,N,5bp+.5red);

// dot("Source", src)

// real a=5, b=4, theta=-70, poids=3;
// ellipse el = ellipse(origin, a, b);
// arc     ar = arc(el,(0,-b),(a,0),CCW);
// path p = (0,-b-1)--ar--(a+1,0)--(a+1,-b-1)--cycle;
// point pO = (0,0), pM=angpoint(ar,90+theta);
// abscissa abscM = nodabscissa(el,pM);
// real     timeM = abscM.x;
// vector utangM = -dir(el,timeM),
//        unormM = rotate(90)*utangM,
//        vpoids=(0,-poids),
//        vreactionN = -dot(vpoids,unormM)*unormM,
//        vfrottement = -dot(vpoids,utangM)*utangM;

// filldraw(p,lightgray,blue);
// draw(pO--pM,dashed);
// markangle("$\theta$",1.5cm,pM,origin,(1,0));

// coordsys R=cartesiansystem(pM,i=utangM,j=unormM);
// show("$M$", "$\vec{u_{\theta}}$", "$\vec{u_{r}}$", R, xpen=invisible);

// point RpM=changecoordsys(R, pM);
// show(Label("$\vec{f}$",EndPoint),RpM+vfrottement);
// show(Label("$\vec{R}$",EndPoint),RpM+vreactionN);
// show(Label("$\vec{P}$",EndPoint),RpM+vpoids);

// // size3(140,80,15);
// currentprojection=perspective(1,-1,1,up=Z);
// currentlight=White;

// // detector surface
// // path3 g=(1,0,0)..(0,1,0)..(-1,0,0)..(0,-1,0)..cycle;
// // draw(g);

// draw(O--X,red+dashed,Arrow3);
// draw(O--Y,red+dashed,Arrow3);
// draw(O--Z,red+dashed,Arrow3);

// // draw detector
// draw(((-1,-1,0)--(1,-1,0)--(1,1,0)--(-1,1,0)--cycle));

// real a=-0.4;
// real b=0.95;
// real y1=-5;
// real y2=-3y1/2;
// path A=(a,0){dir(10)}::{dir(89.5)}(0,y2);
// path B=(0,y1){dir(88.3)}::{dir(20)}(b,0);
// real c=0.5*a;
// pair z=(0,2.5);
// transform t=scale(1,15);
// transform T=inverse(scale(t.yy,t.xx));
// path[] g=shift(0,1.979)*scale(0.01)*t*
//   texpath(Label("{\it symptote}",z,0.25*E+0.169S,fontsize(24pt)));
// pair w=(0,1.7);
// pair u=intersectionpoint(A,w-1--w);

// real h=0.25*linewidth();
// real hy=(T*(h,h)).x;
// g.push(t*((a,hy)--(b,hy)..(b+hy,0)..(b,-hy)--(a,-hy)..(a-hy,0)..cycle));
// g.push(T*((h,y1)--(h,y2)..(0,y2+h)..(-h,y2)--(-h,y1)..(0,y1-h)..cycle));
// g.push(shift(0,w.y)*t*((u.x,hy)--(w.x,hy)..(w.x+hy,0)..(w.x,-hy)--(u.x,-hy)..(u.x-hy,0)..cycle));
// real f=0.75;
// g.push(point(A,0)--shift(-f*hy,f*h)*A--point(A,1)--shift(f*hy,-f*h)*reverse(A)--cycle);
// g.push(point(B,0)--shift(f*hy,-f*h)*B--point(B,1)--shift(-f*hy,f*h)*reverse(B)--cycle);

// triple H=-0.1Z;
// material m=material(lightgray,shininess=1.0);

// for(path p : g)
//   draw(extrude(p,H),m);

// surface s=surface(g);
// draw(s,red,nolight);
// draw(shift(H)*s,m);
#+END_SRC

#+CAPTION: Schematic representation of weighting factor
#+ATTR_LaTeX: :width \textwidth
#+RESULTS: fig:opt_analytic
[[file:figures/opt/analytic.pdf]]

In the 1980s, a lot of work was done to directly solve the inverse
problem for the cone-beam geometry. By modeling the projection
formation process as a Radon transform or an X-ray transform,
reconstruction algorithms were formulated by finding an analytic-based
inverse to the transform. However, for the inverse to be exact, it
needed to meet strict requirements such as Tuy's condition which
states that every plane through the object must intersect the source
trajectory cite:tuy_inversion_1983. While some exceptions to this
requirement were found, it demonstrates the strict requirements on the
types of scanning trajectories for which an exact inverse could be
found.

The circular scanning trajectory that is ubiquitous in the clinic for
CBCT is one trajectory that fails to meet Tuy's condition. The most
popular reconstruction algorithm for the circular CBCT trajectory is
the filtered-backprojection (FBP) algorithm proposed by Feldkamp,
Davis, and Kress (FDK) cite:feldkamp_practical_1984 which is still the
industry standard. FDK is only an exact inversion to the Radon
transform on the midplane containing the circular source
trajectory. For transaxial planes other than the midplane, a
quasi-redundancy in the scanning data is assumed. It is the violation
of this assumption which leads to cone-angle artifacts. These
artifacts become more severe at larger cone angles where this
assumption is less applicable.

The presence of cone-angle artifacts in FDK reconstructions from the
incomplete data acquired with circular scanning trajectories led to
research into inverse algorithms for cone-beam scans from
theoretically complete trajectories such as a circle plus a line
cite:zeng_cone-beam_1992. It became apparent in the reconstruction
results that implementing these direct reconstruction algorithms did
not produce the anticipated results cite:kudo_derivation_1994. Severe
artifacts and numerical errors were found in the reconstructions due
to factors such as truncation introducing high-frequency components
that are amplified in the filtration process.

*** Optimization-based reconstruction
    :PROPERTIES:
    :ID:       07e91084-61be-43d3-a905-65ef0ab997a4
    :END:
Analytic-based reconstruction algorithms are problematic in that they
require a fixed trajectory in formulating the inverse. When
approximations are made for the inverse as in FDK, deviations from
where these approximations are valid lead to inconsistencies in the
model and subsequently artifacts in the reconstruction. For FDK, at
larger cone angles (edges of the FOV at the axial extremes), the
assumption of quasi redundancy for planes other than that of the
source's circular trajectory becomes increasingly invalid at larger
cone angles leading to cone-angle artifacts.  In contrast to these
analytic models, optimization-based reconstruction algorithms have
demonstrated more robust model of the image formation process reducing
the artifacts that arise from these analytic-based methods
cite:shepp_maximum_1982,han_optimization-based_2012,sidky_image_2008,sidky_accurate_2006,bian_evaluation_2010.

Optimization-based reconstruction algorithms are more robust as they
more accurately model this DD imaging system. The X-ray transform of
the object function can be represented as the linear system
\begin{equation}
  \label{eq:ddsys}
  \mathbf{g}=\mathcal{H}\mathbf{f},
\end{equation}
where $\mathbf{g}$ is the discrete $M$ pixel sampled projection on the
detector, $\mathcal{H}$ is the $M\times N$ discrete form of the X-ray
transform, and $\mathbf{f}$ is the object function represented on a N
voxel basis. As direct inversion of $\mathcal{H}$ is impractical due
to both its size and inconsistencies from factors such as noise,
optimization techniques are used to solve this system for an estimate
of the object $\widetilde{\mathbf{f}}$.

The optimization problem for these iterative reconstruction algorithms
is formulated as an objective function based on the actual data
$\mathbf{g}$ and the image model $\mathcal{H}\mathbf{f}$. An
optimization algorithm is then used to iteratively update the estimate
of $\widetilde{\mathbf{f}}$ until a suitable convergence criterion has
been met. The parameters of the optimization problem, the optimization
algorithm, and the convergence criteria are all important factors in
determining the properties of the reconstructed image and subsequently
its utility.

In applying optimization-based reconstruction to reconstruct
non-circular trajectories, we focus primarily on the well-understood
maximum-likelihood expectation maximization (MLEM)
cite:shepp_maximum_1982,dempster_maximum_1977. Previous work has shown
that these iterative algorithms are able to reconstruct clinically
useful images under scanning conditions for which analytic-based FDK
fails
cite:han_optimization-based_2012,sidky_image_2007,sidky_accurate_2006.
The reconstruction work from sparse-view data
cite:bian_evaluation_2010 alone suggests that views could be
distributed at different axial positions to acquire additional scan
information without imparting more dose than the dense set of angular
views used in current clinical circular scans with analytic-based
reconstruction.

# constrained, total-variation (TV) minimization by adaptive steepest
# descent-projection onto convex sets (ASD-POCS)
# cite:sidky_image_2008.

** Background: Scanning trajectories
   :PROPERTIES:
   :ID:       c90cd638-44e6-49f3-9283-29f75d163005
   :END:
*** Standard Trajectories
    :PROPERTIES:
    :ID:       6293da29-e448-4614-84b6-065af1cc6be9
    :END:
In IGRT, linac-mounted CBCT imaging systems such as Variant's TrueBeam
kV-imaging system now routinely provide patient image information.
These images are used to check the patient alignment before delivering
the radiation treatment. The circular rotation of the linac gantry
defines the acquisition trajectory for the CBCT scan. While such a
scanning trajectory provides sufficient information for an
analytic-based reconstruction of the scan volume, there are a variety
of limitations that arise from this work flow.

Due to engineering and cost restrictions, the kV detector has a
limited size. This restricts the FOV that can be imaged in a
traditional circular scan. While the offset detector technique
cite:bian_optimization-based_2013,cho_cone-beam_1995 is commonly used
to increase the transaxial FOV, the axial coverage is still very
limited cite:pearson_non-circular_2010. The reason why the limited FOV
has not been addressed by increasing the detector size is partially
due to the industry reliance on the approximate FDK algorithm
cite:pan_why_2009. For increasingly large cone angles at the ends of
the axial FOV, the approximation in the algorithm becomes increasingly
worse resulting in cone-angle artifacts cite:feldkamp_practical_1984.

Another problem with the current circular imaging trajectory is
potential linac collisions with the patient
cite:hua_practical_2004,nioutsikou_patient-specific_2003. Cases arise
when the patient is positioned in the treatment position, a CBCT image
cannot be acquired due to part of the patient being in the path of the
linac's trajectory. As the current FDK algorithm requires a trajectory
with sufficient angular coverage, the patient must be moved to a
position where the gantry can make an uninterrupted rotation around
the patient.

In both of these examples, the default circular trajectory prescribed
by FDK is insufficient for obtaining the desired tomographic
information. Furthermore, the disruption to the clinical workflow
created by these limitation introduces bottlenecks into clinical
efficiency which affects both the clinical staff as well as the
patient's comfort in the procedure. In the case of a potential patient
collision, the inability to acquire the required trajectory can even
result in forgoing the CBCT image. For these particular examples, we
investigated ways in which new trajectories enabled by
optimization-based reconstruction could alleviate the complications
imposed by the standard circular scan required by FDK.

*** General trajectories
Though there has been previous work in developing analytic methods for
addressing the reconstruction from some novel trajectories
cite:katsevich_theoretically_2002,katsevich_image_2004,katsevich_image_2005,katsevich_formulation_2006,
it could be clinically useful to enable reconstruction from an
arbitrary, collision-avoiding trajectory. As the collision region (if
one arises) is contingent on the patient's treatment position, the
imaging trajectory would then vary on a per patient basis. As such,
deriving the analytic inverse for each patient's scanning trajectory
would be impractical in a clinical work flow.

Optimization-based reconstruction algorithms provide a straightforward
means of enabling reconstruction from patient-specific collision
avoiding trajectories
cite:han_optimization-based_2012,bian_optimization-based_2013. The
imaging model is formulated as the linear transform
\begin{equation}
  \label{eq:linmodel}
  \mathbf{g}=\mathcal{H}\mathbf{f},
\end{equation}
where $\mathbf{g}$ is the discrete $M$ pixel sampled projection on the
detector, $\mathcal{H}$ is the $M\times N$ discrete form of the X-ray
transform, and $\mathbf{f}$ is the object function represented on a N
voxel basis. As the direct inversion of Equation ([[ref:eq:linmodel][linear model]]) is
impractical due to both its size and inconsistencies from factors such
as noise, optimization techniques are used to solve this system for an
estimate of the object $\mathbf{f^{*}}$.

This approach allows for a generalized reconstruction framework that
allows for greater flexibility in reconstructing from projections
acquired with non-circular trajectories. Provided the geometry of each
view is correctly incorporated into the system matrix $\mathcal{H}$,
clinically useful reconstructions can be obtained from acquisitions
for which an analytic inverse may not be available. As the collision
zone for a given patient would be patient specific, this robust
approach enables tomographic imaging from collision-avoiding
trajectories that would accommodate the patient's specific needs.

The increased flexibility in choosing different scanning trajectories
allowed by optimization-based reconstruction methods provided two
solutions to the issues of limited axial FOV coverage and potential
patient collisions. For these two problems, we found that the existing
limitations could be resolved by using a different scanning
configuration. In each case, we proposed a trajectory that would solve
the existing problem, and then we evaluated how well the
optimization-based reconstructions compared to the clinical images
currently being used.

For the problem of the limited axial coverage, the current clinical
method of extending the FOV is to acquire two different circular scans
at different axial positions and reconstruct each circle independently
using FDK before stitching the two volumes together. Unfortunately,
the increased distortion from cone-angle artifacts at large cone
angles limits the axial separation between these two circles. This
restricted separation distance is much less than what would be
expected based simply on the coverage expected from the geometry of
the kV detector.

The use of the two circles alone provides one interesting example of a
trajectory were optimization-based reconstruction provides an
interesting advantage to the stacked-FDK method currently used. Unlike
stitching two separate reconstructions together, it is possible to
reconstruct the entire volume at once provided the system matrix is
correctly calculated to reflect the acquisition of two circles in
planes located at different axial positions relative to the patient.
In addition to the reduced cone-angle artifacts already seen in
optimization-based methods *CITE*, reconstructing both volumes
together provides additional information about the overlapping region
between the circles that further helps to reduce the cone-angle
artifacts.

In addition to improving the use of the two circles, the
optimization-based framework then allows for trajectories that can
deviate beyond the circles that are still needed for FDK. Given that
there needs to be a relative axial translation between the kV-imaging
system and the patient, we investigated if there any advantages to
acquiring some of the projection views during the axial translation
to. With the optimization-based approach, any such trajectory where
some of the views were acquired during the translation stage could be
reconstructed provided the positions of these views are accurately
represented in the system matrix.

In the case of potential patient collisions with the linac gantry, a
simple change in the scanning trajectory when such a collision arose
would be sufficient to prevent a collision. Much like the extended
axial FOV case, optimization-based reconstruction is able to handle
variations in the acquisition trajectory provided it is accurately
reflected in the system matrix. As such, there are two different ways
we studied where the scanning trajectory could be modified to avoid a
collision.

If the patient collision were to occur with the kV detector (the
closest component of the CBCT system to the patient), one possible way
to avoid that collision would be to move the kV detector away from the
patient at the collision region. This effectively changes the
magnification for that region, but the reconstruction framework is
able to reconstruct from all the views at both magnifications provided
that it is accurately modeled in the reconstruction problem. The other
trajectory modification that could solve this problem would be to move
the patient.

As with the change in magnification, the change in the patient
position does not prevent reconstruction with the optimization-based
methods provided the patient motion is correctly modeled. Moving the
patient also provides a solution to patient collisions that occur with
the linac treatment head. The MV treatment head on Varian's TrueBeam
system is actually closer to the patient than the kV detector. Unlike
the kV detector, it is not possible to change the position of the
treatment head. In this case, moving the patient would be the only
viable trajectory to avoid a collision.

** Algorithms                                :noexport:
*** MLEM
    :PROPERTIES:
    :ID:       e0a24b69-d136-4f9a-9e85-dc42e1d114a9
    :END:
To utilize optimization-based methods for image reconstruction, the
image formation process is modeled as the linear system
#+BEGIN_LaTeX
\begin{equation}
  \label{eq:ddsys}
  \mathbf{g}=\mathcal{H}\mathbf{f},
\end{equation}
#+END_LaTeX
where $\mathbf{g}$ is the discrete $M$-pixel sampled projection on the
detector, $\mathcal{H}$ is the $M\times N$ discrete form of the X-ray
transform, and $\mathbf{f}$ is the object function represented on a
$N$-voxel basis. As direct inversion of $\mathcal{H}$ is impractical
due to both its size and inconsistencies from factors such as noise,
optimization techniques are used to solve this system for an estimate
of the object $\widetilde{\mathbf{f}}$.

The optimization problem for this approach is then framed as an
objective function based on the actual data $\mathbf{g}$ and the image
model $\mathcal{H}\mathbf{f}$. An optimization algorithm is then used
to iteratively update the estimate of $\widetilde{\mathbf{f}}$ until a
suitable convergence criterion has been met.

Where, we used the maximum-likelihood expectation maximization (MLEM)
algorithm cite:dempster_maximum_1977. As we are interested in using
optimization-based methods to enable reconstruction from novel
trajectories, we selected an algorithm that is well understood and has
relatively few parameters. However, the freedom in calculating the
system matrix by modeling the image formation process as a linear
system does not limit the algorithm choice to MLEM alone.

The reconstruction program for solving Equation (\ref{eq:ddsys}) using
MLEM can be formulated as
#+BEGIN_LaTeX
\begin{equation}
  D_{KL}(\mathbf{f})\leq \epsilon_{KL}
  \label{eq:kl}
\end{equation}
#+END_LaTeX
where $D_{KL}(\mathbf{f})$ is the Kullback-Leibler (KL) divergence
between $\mathbf{g}$ and $\mathcal{H}\mathbf{f}$
cite:barrett_foundations_2003 and $\epsilon_{KL}>0$ is the upper bound for the
KL divergence. The KL divergence can be minimized with the MLEM
algorithm
#+BEGIN_LaTeX
\begin{equation}
  f_j^{(n+1)}=\frac{f_j^{(n)}}{\sum\nolimits_{i=1}^{M}\mathcal{W}_{ii}\mathcal{H}_{ij}}\sum\limits_{i=1}^{M}\mathcal{W}_{ii}\mathcal{H}_{ij}\frac{g_i}{\sum\nolimits_{j=1}^{N}\mathcal{H}_{ij}f_j^{(n)}}}
    \label{eq:mlem}
\end{equation}
#+END_LaTeX
where $f_{j}^{n}$ is $j$-th voxel value at iteration $n$ and
$\mathcal{H}_{ij}$ is the element of the system matrix at the $i$-th
row and $j$-th column for $i=1,2,...,M$ and $j=1,2,...,N$. The initial
image estimate for the reconstructions was $\mathbf{f}^{(0)}=1$. The
$M\times M$ diagonal matrix $\mathcal{W}$ weights the corresponding
data entries cite:bian_optimization-based_2013.

** Additional reconstruction parameters
*** Iterations and stopping criteria
    :PROPERTIES:
    :ID:       03857328-5d45-4133-b4a0-eff3fd941eaa
    :END:
*** Resolution
*** Detector weighting
    :PROPERTIES:
    :ID:       cc6bcac6-a445-4dfb-8815-a95e31f517ed
    :END:
 #+LABEL: fig:opt_weighting
 #+BEGIN_SRC asymptote :file figures/opt/weighting.pdf :exports results :tangle no
settings.render = 0;
import geometry;
// size(8cm,0);
// unitsize(1cm)

// Affichage du repère par défaut (O,vec{i},vec_{j})
// show(defaultcoordsys);
// show(currentcoordsys);

// detector
real dlat=0, dlng=0, dvrt=50;
point det=(dvrt,dlat);

real ulen=40.0, vlen=30.0;

draw((dvrt,-ulen/2+dlat)--(dvrt,ulen/2+dlat),black);

// source
real slat=0, slng=0, svrt=-100;
point src=(svrt,slat);

draw(src--(dvrt, 0), dashed+red);
draw(src--(dvrt, -ulen/2+dlat), dashed+black);
draw(src--(dvrt, ulen/2+dlat), dashed+black);
dot("Source", src, N, red);

addMargins(0.5cm, 0.5cm);

// dot("Detector",det,N,5bp+.5blue);
// dot("Source",src,N,5bp+.5red);

// dot("Source", src)

// real a=5, b=4, theta=-70, poids=3;
// ellipse el = ellipse(origin, a, b);
// arc     ar = arc(el,(0,-b),(a,0),CCW);
// path p = (0,-b-1)--ar--(a+1,0)--(a+1,-b-1)--cycle;
// point pO = (0,0), pM=angpoint(ar,90+theta);
// abscissa abscM = nodabscissa(el,pM);
// real     timeM = abscM.x;
// vector utangM = -dir(el,timeM),
//        unormM = rotate(90)*utangM,
//        vpoids=(0,-poids),
//        vreactionN = -dot(vpoids,unormM)*unormM,
//        vfrottement = -dot(vpoids,utangM)*utangM;

// filldraw(p,lightgray,blue);
// draw(pO--pM,dashed);
// markangle("$\theta$",1.5cm,pM,origin,(1,0));

// coordsys R=cartesiansystem(pM,i=utangM,j=unormM);
// show("$M$", "$\vec{u_{\theta}}$", "$\vec{u_{r}}$", R, xpen=invisible);

// point RpM=changecoordsys(R, pM);
// show(Label("$\vec{f}$",EndPoint),RpM+vfrottement);
// show(Label("$\vec{R}$",EndPoint),RpM+vreactionN);
// show(Label("$\vec{P}$",EndPoint),RpM+vpoids);

// // size3(140,80,15);
// currentprojection=perspective(1,-1,1,up=Z);
// currentlight=White;

// // detector surface
// // path3 g=(1,0,0)..(0,1,0)..(-1,0,0)..(0,-1,0)..cycle;
// // draw(g);

// draw(O--X,red+dashed,Arrow3);
// draw(O--Y,red+dashed,Arrow3);
// draw(O--Z,red+dashed,Arrow3);

// // draw detector
// draw(((-1,-1,0)--(1,-1,0)--(1,1,0)--(-1,1,0)--cycle));

// real a=-0.4;
// real b=0.95;
// real y1=-5;
// real y2=-3y1/2;
// path A=(a,0){dir(10)}::{dir(89.5)}(0,y2);
// path B=(0,y1){dir(88.3)}::{dir(20)}(b,0);
// real c=0.5*a;
// pair z=(0,2.5);
// transform t=scale(1,15);
// transform T=inverse(scale(t.yy,t.xx));
// path[] g=shift(0,1.979)*scale(0.01)*t*
//   texpath(Label("{\it symptote}",z,0.25*E+0.169S,fontsize(24pt)));
// pair w=(0,1.7);
// pair u=intersectionpoint(A,w-1--w);

// real h=0.25*linewidth();
// real hy=(T*(h,h)).x;
// g.push(t*((a,hy)--(b,hy)..(b+hy,0)..(b,-hy)--(a,-hy)..(a-hy,0)..cycle));
// g.push(T*((h,y1)--(h,y2)..(0,y2+h)..(-h,y2)--(-h,y1)..(0,y1-h)..cycle));
// g.push(shift(0,w.y)*t*((u.x,hy)--(w.x,hy)..(w.x+hy,0)..(w.x,-hy)--(u.x,-hy)..(u.x-hy,0)..cycle));
// real f=0.75;
// g.push(point(A,0)--shift(-f*hy,f*h)*A--point(A,1)--shift(f*hy,-f*h)*reverse(A)--cycle);
// g.push(point(B,0)--shift(f*hy,-f*h)*B--point(B,1)--shift(-f*hy,f*h)*reverse(B)--cycle);

// triple H=-0.1Z;
// material m=material(lightgray,shininess=1.0);

// for(path p : g)
//   draw(extrude(p,H),m);

// surface s=surface(g);
// draw(s,red,nolight);
// draw(shift(H)*s,m);
 #+END_SRC

 #+CAPTION: Schematic representation of weighting factor
 #+ATTR_LaTeX: :width \textwidth
 #+RESULTS: fig:opt_weighting
 [[file:figures/opt/weighting.pdf]]
** Generalized trajectory framework
To find an estimate of the object $\mathbf{f^{*}}$ as an approximate
solution to Equation ([[ref:eq:linmodel]]), we choose a reconstruction
program that can be solved with the well-understood maximum-likelihood
expectation maximization (MLEM) algorithm cite:dempster_maximum_1977.
Here, our reconstruction program is formulated as
\begin{equation}
  \mathbf{f^{*}} = \text{argmin}D_{KL} \left(\mathbf{f}\right)
  \label{eq:opt_kl}
\end{equation}
where $D_{KL}(\mathbf{f})$ is the KL divergence between $\mathbf{g}$ and
$\mathcal{H}\mathbf{f}$ cite:barrett_foundations_2003. The KL
divergence is minimized with the MLEM algorithm
\begin{equation}
f_j^{(n+1)}=\frac{f_j^{(n)}}{\sum\nolimits_{i=1}^{M}\mathcal{W}_{ii}\mathcal{H}_{ij}}\sum\limits_{i=1}^{M}\mathcal{W}_{ii}\mathcal{H}_{ij}\frac{g_i}{\sum\nolimits_{j=1}^{N}\mathcal{H}_{ij}f_j^{(n)}}
  \label{eq:opt_mlem}
\end{equation}
where $f_{j}^{n}$ is $j-\text{th}$ voxel value at iteration $n$ and
$\mathcal{H}_{ij}$ is the element of the system matrix at the
$i-\text{th}$ row and $j-\text{th}$ column for $i=1,2,...,M$ and
$j=1,2,...,N$. The initial image estimate for the reconstructions was
$\mathbf{f}^{(0)}=1$. The $M\times M$ diagonal matrix $\mathcal{W}$ weights
the corresponding data entries as is typically done for a standard
half-fan detector configuration for a circular scan
cite:bian_optimization-based_2013.

In a traditional scanning configuration where the patient is
stationary, the system matrix $\mathcal{H}_{ij}$ projects the object
$\mathbf{f}$ from image space to the data space of $\mathbf{g}$. When
this is the case, it is sufficient that the coordinate basis of the
image space coincides with the room coordinate system, or is at least
stationary relative to it. From each projection view acquired from the
TrueBeam system, we extracted the position information of the CBCT
imaging arms and subsequently built $\mathcal{H}_{ij}$ as the projective
transform from image space in the IEC coordinate system to the data
space of the kV detector.

For a virtual isocenter-scanning trajectory, the image space
$(\mathbf{f}_{\text{patient}})$ is moving relative to the room coordinate
system for each projection view. As such, a change of basis for the
columns space of $\mathcal{H}$ is necessary so that the new system
matrix represents a transform from the image space of the patient to
the data space of the detector represented as $\mathcal{H'}$. As the
acquisition system also reports the couch position, we used this to
build the required transformation matrix
$\mathcal{T}_{\text{IEC,patient}$ for each projection view. The
imaging model in Equation ([[ref:eq:linmodel]]) then becomes
\begin{equation}
  \label{eq:opt_linmodel_patient}
  \mathbf{g}=\mathcal{H'}\mathbf{f_{\text{patient}}},
\end{equation}
where
\begin{equation}
  \label{eq:sys_patient}
  \mathcal{H'}=\mathcal{H}\mathcal{T_{\text{IEC,patient}}}.
\end{equation}
Once the change of basis is accounted for, the reconstruction program
in Equation ([[ref:eq:kl]]) can be reformulated with $\mathbf{f_{\text{patient}}}$
instead of $\mathbf{f}$ and solved with the MLEM algorithm in Equation
([[ref:eq:mlem]]) using $\mathcal{H'}$ instead of $\mathcal{H}$.

** Framework implementation with Varian TrueBeam kV-CBCT system
*** TrueBeam linac with Developer Mode
     :PROPERTIES:
     :ID:       3b90dfa6-e2de-4bdf-886a-31238cfa1cec
     :END:
To study these trajectories with a clinical, kV-imaging system, we
acquired some of them on Varian's TrueBeam system. The TrueBeam
Developer Mode provides control of the kV imaging system to allow for
motion control that is unavailable in clinical modes. Developer Mode
provides a scriptable control interface that allows control of the
gantry rotation, the kV-imaging arms, as well as the position of the
treatment couch. By combining motions with all of these components, it
is possible to acquire kV projection data from a variety of different
interesting motions. From the acquisition, each projection is returned
with self-reported nominal values that can be used to build the
reconstruction system matrix. Table ([[ref:tab:opt_varian_header]]) shows
a subset of these header variables pertaining to the kV imaging
system.

#+ATTR_LATEX: :environment longtable :align l|l|l|l|l
#+CAPTION: Subset of Varian's TrueBeam projection header variables pertaining to the kV-imaging system.
#+NAME: tab:opt_varian_header
|-------------+----------------+------------------+--------------------+----------------|
|             | Couch          | Detector         | Gantry             | kV Source      |
|-------------+----------------+------------------+--------------------+----------------|
| Acquisition | CouchLat       | ImagerLat        | GantryAcceleration | Current        |
|             | CouchLng       | ImagerLng        | StartAngle         | FrameRate      |
|             | CouchRtn       | ImagerOrigin     | StopAngle          | KVFilter       |
|             | CouchThickness | ImagerResX       |                    | PulseLength    |
|             | CouchVrt       | ImagerResY       |                    | SAD            |
|             | CouchWidth     | ImagerSizeX      |                    | SID            |
|             |                | ImagerSizeY      |                    | Voltage        |
|             |                | ScatterGrid      |                    |                |
|-------------+----------------+------------------+--------------------+----------------|
| Projection  | CouchLat       | ImagerDeltaLat   | GantryRtn          | SourceAngle    |
|             | CouchLng       | ImagerDeltaLng   |                    | SourceDeltaLat |
|             | CouchRtn       | ImagerDeltaPitch |                    | SourceDeltaLng |
|             | CouchVrt       | ImagerDeltaRtn   |                    | SourceDeltaVrt |
|             |                | ImagerDeltaVrt   |                    |                |
|-------------+----------------+------------------+--------------------+----------------|

For the majority of our non-simulation work, we used Varian's TrueBeam
kV imaging system (Varian Medical Systems, Palo Alto, CA) to acquire
the projection data used in this study. This is a linac-mounted CBCT
c-arm system consisting of a Varian kV x-ray source (GS-1542) and a
39.7 cm x 29.8 cm amorphous silicon flat-panel detector (PaxScan
4030CB) with a $2048 \times 1536$ pixel array that performs a $2 \times 2$
binning for a readout of $1024 \times 768$ square pixels of effective size
0.388 mm. The source and detector are mounted on robotic arms with the
kV beam direction orthogonal to the MV treatment beam.
***** note                                   :noexport:
 Also, you might write a paragraph that describes how these parameters
 come to you - that the acquired projections are stored in files using
 Varian’s XIM or HND formats, which have extensive headers containing
 dozens of parameters associated with the geometry, x-ray technique,
 and other information concerning each specific image. maybe have a
 table showing examples of a subset of the data from one projection -
 all the couch, detector, source, gantry and x-ray parameters like kV,
 mA, timing, exposure, etc? people not in this field probably have no
 idea this stuff exists. it’s kind of like DICOM but specific to Varian
 images. this could equally well go in the chapter where you describe
 your geometry framework for building the system matrix, and you could
 refer to it here.
*** Varian coordinates
    :PROPERTIES:
    :ID:       9e81dc1a-091f-4614-9d0f-5a5d4ee4f0d1
    :END:


*table of header parameters hnd/xim*
** ideas                                     :noexport:
Given that part of the robust nature of optimization-based algorithms
is the ability to handle the poorly-conditioned nature of the inverse
problem...
* Geometric calibration                      :geo:
  :PROPERTIES:
  :ID:       652970b8-4916-4190-b83b-2d6ae117c8b3
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       5c9cdd8b-721f-49b3-b136-c3282bf3659c
   :END:
** Introduction
   :PROPERTIES:
   :ID:       26feb0f0-f33e-4972-af9c-f73e0124f074
   :END:
Correctly modeling the geometric parameters of the image acquisition
is a critical component tomographic image reconstruction. This is true
regardless of whether reconstruction is done with analytic-based or
optimization-based methods. Any inconsistency between the real
projection geometry and that used for image reconstruction will cause
artifacts in the reconstructed image. An example of such an artifact
is shown in Figure ([[ref:fig:geo_cal_catphan_example]]).

#+BEGIN_EXPORT latex
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/catphan_virt_1p5x_nocal.png}
    \caption{}
    \label{fig:geo_virt_catphan_nocal}
  \end{subfigure}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad,
  % \hfill etc.
  % (or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/catphan_virt_1p5x_cal.jpg}
    \caption{}
    \label{fig:geo_virt_catphan_cal}
  \end{subfigure}
  \caption{Transverse slice of the Catphan 504 phantom. The image on
    the left is reconstructed without geometric calibration, and the
    image on the right is constructed with geometric calibration. The
    arrow in red indicates one example of the geometric distortion
    incurred by incorrectly modeling the scanning geometry. This
    blurring and subsequent loss of spatial resolution is a typical
    consequence of poor geometric calibration.}
  \label{fig:geo_cal_catphan_example}
\end{figure}
#+END_EXPORT

While investigating different non-standard scanning trajectories, we
found that correct geometric calibration must be performed to avoid
geometric imaging artifacts. As with the optimization-based image
reconstruction, we needed a calibration procedure that would
accommodate the different scanning configurations we wanted to scan.
This is especially true when working with trajectories where the
object is moving in addition to the source and detector during the
scan.

Previous work on geometric calibration for tomographic image
reconstruction has approached the calibration problem via analytic
cite:noo_analytic_2000,smekal_geometric_2004,cho_accurate_2005,yang_geometric_2006,daly_geometric_2008
and estimation
cite:gullberg_estimation_1990,rougee_geometrical_1993,mitschke_optimal_2000,silver_determination_2000,panetta_optimization-based_2008
frameworks. Initial calibration efforts utilized optimization-based
methods to determine the geometric offsets from projections of a known
phantom geometry and nominal system setup. By framing the calibration
as an optimization problem, the acquisition parameters were estimated
in a way that minimized a cost function associated with improper
modeling of the acquisition geometry.

These calibration methods (analytic-based methods included) usually
rely on a known calibration phantom. This is typically a set of highly
attenuating fiducials arranged in a specific pattern. After scanning
the phantom with the system of interest, the detected fiducials are
then compared to predicted positions based on the known geometry of
the phantom and the nominal projection geometry In the analytic-based
approach, the view parameters are determined by solving for parameters
that would transform the projection of the phantom to match the
observed projection. In the optimization-based approach, geometric
parameters are varied to improve the match between the projection of
the modeled fiducials an the detected fiducials in the sinogram.

Both methods of performing geometric calibration have their own
strengths and weaknesses. The biggest advantage of utilizing
analytic-based calibration methods is that the sensitivity to
initialization and the sensitivity to the order of parameter variation
due to nonlinearity and coupling of parameters faced by estimation are
avoided cite:smekal_geometric_2004. However, as with
optimization-based reconstruction, optimization-based calibration
methods are more flexible in providing calibration offsets for the
novel trajectories that we studied.

Using previous work for optimization-based geometric calibration
cite:rougee_geometrical_1993,gullberg_estimation_1990,silver_determination_2000,
we developed a calibration method that utilizes a phantom with known
placement of highly attenuating fiducials. By scanning this phantom
and comparing the the projections to the modeled forward-projection of
a mathematical model of the phantom, we can more accurately determine
the system matrix $(\mathcal{H})$ in Equation ([[ref:eq:ddsys]]) for
reconstructing from a non-circular scanning trajectory with
optimization-based methods resulting in reduced image artifacts.

** Methods
   :PROPERTIES:
   :ID:       0b636fe5-fe45-4f10-a5fc-2de8a82bfbe4
   :END:
Where analytic-based methods, such as FDK, require a certain
acquisition trajectory such a as a fixed scanning radius of the source
and detector and the angular position of each projection, the
optimization-based system matrix makes no assumptions of the geometry
in other views. As such, we created a reconstruction framework that
incorporates the best geometric estimate of the projection geometry of
each view. The flexibility to incorporate geometric corrections in
this way is another useful aspect in using optimization-based methods
for image reconstruction.

Before attempting to determine any geometric errors in our scanning
acquisition, we first modified the calculation of our system matrix to
incorporate the geometry information provided by the TrueBeam system
as discussed in [[id:9e81dc1a-091f-4614-9d0f-5a5d4ee4f0d1][Varian coordinates]]. In doing this, we took advantage
of all the inherent geometry information that is provided with the
current clinical system. This information then provided an initial
estimate of the scanning geometry which we could then refine with the
calibration information we extracted with our calibration protocol.
*** Phantoms
    :PROPERTIES:
    :ID:       F5BECB45-8652-47A3-915C-1E96DA6110E7
    :END:
Our first calibration phantom for determining geometric offsets is
shown in Figure (\ref{fig:geo_geocal}). The phantom is a 15.2 cm outer
diameter acrylic tube with a spiral pattern of CT-spot fiducials
placed 2.5 cm along the axial direction every $45^{\circ}$. When scanned,
the CT spots are clearly visible in the projection images which is
ideal for automating the fiducial detection in the data domain.

However, we realized that using such a spiral calibration phantom
creates a degree of ambiguity in the geometry of the projected
fiducials. With both this phantom and additional calibration phantoms
we created, too much symmetry in the phantom design leads to a rather
challenging objective function. Given that only a small portion of the
phantom is visible in any one projection view, excessive symmetry
produces multiple minima in the objective function where a simple
axial shift and rotation offset allows for multiple matches of the
modeled fiducials and those in the real data. To avoid such
complexity, a calibration phantom with intentional asymmetry is
desirable so that the projected fiducials can be indentified and
matched without ambiguity.

In addition to the necessary complexity created by this phantom,
another concern for a calibration phantom is the uncertainty in the
geometry of the phantom itself. Though the guide lines on the cylinder
were inscribed with the lathe and its rotational stage, we placed the
fiducials by hand. As we were trying to determine millimeter offsets
with our calibration, this fiducial placement was suboptimal.

#+CAPTION: Initial geometric calibration phantom with a spiral fiducial pattern.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:geo_geocal
[[../../research/trajectories/geometry/geocal/20140901_extended_cllc.jpg]]

The phantom we then decided to use for calibration was the Isocal
phantom created by Varian shown in Figure ([[ref:fig:geo_isocal]]). The Isocal
phantom directly addresses the two problems encountered with our first
phantom. First, the phantom is designed with intentional asymmetry.
Additionally, the phantom is manufactured by Varian to help align the
MV-treatment isocenter with the kV-imaging isocenter. As such, the
position of the beads on this phantom have a much tighter tolerance
than that of our original phantom.

#+CAPTION: Varian's Isocal phantom positioned at the isocenter.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:geo_isocal
[[../../research/phantoms/isocal/imgs/161012_isocal.jpg]]

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{figures/geo/old_geocal_full_1701x1024x768}
  \caption{}
  \label{fig:geo_oldcal_proj}
\end{subfigure}
~ %add desired spacing between images, e. g. ~, \quad, \qquad,
% \hfill etc.
% (or a blank line to force the subfigure onto a new line)
\begin{subfigure}[b]{0.45\textwidth}
  \includegraphics[width=\textwidth]{figures/geo/isocal_full_914x1024x768}
  \caption{}
  \label{fig:geo_isocal_proj}
\end{subfigure}
\caption{(a) shows a projection of our first calibration phantom
  consisting of a single spiral of fiducials around the acryllic tube.
  (b) shows a projection of Varian's isocal calibration phantom. The
  additional fiducials seen in each projection, and their unambiguous
  layout in the projection help prevent local minima when searching
  for correct geometry offsets}
\label{fig:geo_cal_sens_cost}
\end{figure}
#+END_EXPORT
*** Calibration method
    :PROPERTIES:
    :ID:       F53F4B5A-83EB-4B16-9B6D-F557D3E441C2
    :END:
We designed a calibration procedure specifically for the non-standard
scanning trajectories we implemented on the TrueBeam system with
Developer Mode. As such, the nominal trajectory we used to initialize
our calibration method was self-reported, view-by-view geometry
provided by the TrueBeam system for each projection. Starting with
this initial estimate with which we calculated our reconstruction
system matrix $\mathcal{H}$, the additional calibration information we
were able to extract with our calibration improved our estimate of
both the system matrix and subsequently the estimated image from the
reconstruction.

Figure ([[ref:fig:geo_cal_schematic]]) provides a schematic illustration
of this with the Isocal phantom for a single view. Ideally, the
nominal geometry used to calculate a single projection would produce
the simulated projected fiducials in blue. However, as both our work
and that of others has found, this is not usually the case.
Discrepancies between the reported geometry and the actual scanning
geometry can arise from multiple sources in a given acquisition.

With a typical CBCT scan, deviations from the nominal geometry can
occur in both the phantom's setup (translation and rotation in all
three dimensions) as well as that of the source and detector positions
(due to translation and rotation deviations in the gantry, source, and
detector). The collective impact of these various discrepancies will
produce projection views for which the projected fiducials in the data
domain do not match the simulated projections from the nominal
geometry as shown by the red projected fiducials in Figure
([[ref:fig:geo_cal_schematic]]).

#+NAME: fig:geo_cal_schematic
#+BEGIN_SRC asymptote :file figures/geo/cal_schematic.pdf :exports results
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import geometry;
import solids;
import three;

// view configuration
size(10cm);
currentprojection=orthographic(-15,5,13,up=Y);
// currentprojection=perspective(-15,5,13,up=Y);
// currentlight=White;

// Draw axis
// draw(Label("$y$",1),(0,0,0)--(0,5,0),red,Arrow3);
// draw(Label("$x$",1),(0,0,0)--(5,0,0),red,Arrow3);
// draw(Label("$z$",1),(0,0,0)--(0,0,5),red,Arrow3);

// kV schematic
real dlat=0, dlng=0, dvrt=50;
triple det_cent=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det_cent-(0,ulen/2,vlen/2));

triple det0 = det_cent-(0,ulen/2,vlen/2);
real s=5;
triple u = (det0+s*(0,1,0));
triple v = (det0+s*(0,0,1));
triple w = (det0+s*(-1,0,0));

// detector coordinate system
draw(det0--u,blue,Arrow3,L=Label("$u$", position=EndPoint, align=W));
draw(det0--v,blue,Arrow3,L=Label("$v$", position=EndPoint, align=N));
draw(det0--w,blue,Arrow3,L=Label("$w$", position=EndPoint, align=S));

draw(detector, blue);

// path3 det180 = rot180*detector;
// path3 det270 = rot270*detector;

// uncal detector coordinate system
transform3 det_pitch=rotate(-5, det_cent, det_cent+(-1,0,0));
transform3 det_roll=rotate(-5, det_cent, det_cent+(0,0,1));
transform3 det_yaw=rotate(5, det_cent, det_cent+(0,-1,0));
transform3 det_shift=shift(5, -8, 2);

path3 detector_uncal = det_pitch*det_roll*det_yaw*det_shift*detector;
path3 det_cent_uncal = det_pitch*det_roll*det_yaw*det_shift*det_cent;
// path3 detector_uncal = det_shift*detector;
// path3 det_cent_uncal = det_shift*det_cent;
real op_uncal=0.35;
draw(detector_uncal, red+opacity(op_uncal));

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

// label("$\theta=0^{\circ}$",red,align=S,position=towardcamera((det_cent-(0, ulen/2, -vlen/2))));

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// uncal source
// triple src_uncal=shift(0,10,5)*(svrt,slat, slng);

// lines from source to detector edges
draw(src..det_cent-(0,-ulen/2,-vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,-ulen/2,vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,ulen/2,vlen/2),blue+opacity(0.15));
draw(src..det_cent-(0,ulen/2,-vlen/2),blue+opacity(0.15));

draw(Label("$X_{\theta_g=0^{\circ}}$", 1),src--det_cent-(110,0,0), blue, arrow=Arrow3);

// transformed frame vectors
triple det0_uncal = point(detector_uncal, 0);
triple u_p = point(detector_uncal, 1) - det0_uncal;
triple v_p = point(detector_uncal, 3) - det0_uncal;

// unit vectors
triple uhat_p = u_p / length(u_p);
triple vhat_p = v_p / length(v_p);
triple what_p = cross(uhat_p, -vhat_p);

// scale
triple u_p = s*uhat_p + det0_uncal;
triple v_p = s*vhat_p + det0_uncal;
triple w_p = s*what_p + det0_uncal;

// uncalibrated detector coordinate system
draw(det0_uncal--u_p,red,Arrow3,L=Label("$u'$", position=EndPoint, align=SE));
draw(det0_uncal--v_p,red,Arrow3,L=Label("$v'$", position=EndPoint, align=S));
draw(det0_uncal--w_p,red,Arrow3,L=Label("$w'$", position=EndPoint, align=S));

// draw(point(detector_uncal, 1)--src, red+opacity(0.15));
// real arrowlength = 5
// vector v_p=new path(real x){
//     return point(detector_uncal, 1)--arrowlength*(-1)*point(detector_uncal, 2));
// };

// draw(v_p)
// draw(point(detector_uncal, 1)--point(detector_uncal, 2),red, arrow=Arrow3);

// // and for real projection
// draw(src_uncal..point(detector_uncal, 0), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 1), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 2), red+opacity(0.15));
// draw(src_uncal..point(detector_uncal, 3), red+opacity(0.15));

// draw(Label("$\mathcal{H}$", 1),src--det_cent_uncal, red, arrow=Arrow3);

// Draw cylinder
// cylinder(startpoint3d, radius, length, along_this_axis)
triple start = (0,0,-8);
real length = 16;
real radius = 11.3;
triple ax = (0,0,1);
revolution r = cylinder(start,radius,length,ax);
draw(r,black);

// isocal spots
triple[] isocal={(0,-11.3,-7.5),
                 (7.9903,-7.9903,-7.5),
                 (7.9903,7.9903,-7.5),
                 (-11.3,0.0,-7.5),
                 (-7.9903,7.9903,-5),
                 (11.3,0.0,-3),
                 (0,11.3,-2),
                 (-10.4398,4.3243,2),
                 (4.3243,10.4398,3),
                 (-10.4398,-4.3243,5),
                 (4.3243,-10.4398,5),
                 (10.4398,-4.3243,7.5),
                 (10.4398,4.3243,7.5),
                 (-4.3243,10.4398,7.5),
                 (-4.3243,-10.4398,7.5)
};

dot(isocal, black);

// project points
transform3 proj=planeproject(detector);
transform3 proj_uncal=planeproject(detector_uncal);
// transform3 proj090=planeproject(det090);
// transform3 proj180=planeproject(det180);
// transform3 proj270=planeproject(det270);

dot(proj*isocal,blue);
dot(proj_uncal*isocal,red+opacity(op_uncal));
// dot(proj090*isocal,red);
// dot(proj180*isocal,red);
// dot(proj270*isocal,red);
#+END_SRC

#+CAPTION: Schematic represenation of a single projection view for the isocal phantom with the TrueBeam kV-CBCT scanning geometry. The blue detector and projected isocal fiducials correspond to the self-reported geometry from the imaging system. The red detector and projected fiducials illustrates how translation and rotation offsets of both the phantom and the source-detector system create variations in the projected fiducials in the sinogram space. The bottom left corner corresponds to the origin of the detector coordinate system. The detector's translation and rotation offsets are exaggerated here for illustrative purposes.
#+LABEL: fig:geo_cal_schematic
#+ATTR_LaTeX: :width 0.9\textwidth
#+RESULTS: fig:geo_cal_schematic
[[file:figures/geo/cal_schematic.pdf]]

Starting with the nominal scanning geometry reported by the projection
metadata, we first build an initial projection matrix $\boldsymbol{X}$
that transforms the simulated phantom fiducials in room coordinates to
projected spots in detector coordinates. The matrix $\boldsymbol{X}$
is calculated using the variables describing each view shown in Figure
([[ref:fig:geo_cal_proj]]). The source and detector (including the
detector's frame vectors $\left\{ \hat{u}, \hat{v}, \hat{w} \right\}$)
are rotated into the global image space by rotating these vectors by
the gantry angle $\left( \theta_{g} \right)$ at each view (the gantry rotation
axis is the logitudinal axis of the cylinder in Figure
([[ref:fig:geo_cal_schematic]]) and the $y$ axis in Figure
([[ref:fig:geo_cal_proj]]).

Projection of the fiducial coordinates onto the detector needs to be
done in a coordinate system aligned with the detector’s frame vectors.
The source-to-detector distance needed for projection is the distance
along a direction normal to the detector plane, i.e. parallel to the
frame vector $w$. The normal distance from source to detector is
calculated by first choosing a ray connecting the source to the
detector, $\vec{r}_{\text{sd}}$. The component of this ray that is
orthogonal to the detector is then found using the dot product
\begin{equation}
  L=\vec{r}_{sd}}\cdot \hat{w},
  \label{eq:geo_along}
\end{equation}
where the frame vector $\hat{w}$ corresponds to the detector's normal
unit vector. This then provides the vector describing the piercing
point $\left( \vec{p} \right)$ at that view which is given by
\begin{equation}
  \vec{p}=\vec{r}_s+L \hat{w},
  \label{eq:geo_pierce}
\end{equation}
where $\vec{r}_s$ is the vector corresponding to the source position in
the image coordinates for that view.

#+NAME: fig:geo_cal_proj
#+BEGIN_SRC asymptote :file figures/geo/cal_proj.pdf :exports results
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import geometry;
import solids;
import three;

// view configuration
size(10cm);
currentprojection=orthographic(-15,10,20,up=Y);
// currentprojection=perspective(-15,5,13,up=Y);
// currentlight=White;

// Draw axis
real ax_scale=15;
draw(Label("$z$",position=EndPoint,align=N),(0,0,0)--(0,ax_scale,0),black,Arrow3);
draw(Label("$x$",position=EndPoint,align=S),(0,0,0)--(ax_scale,0,0),black,Arrow3);
draw(Label("$y$",position=EndPoint,align=SW),(0,0,0)--(0,0,-ax_scale),black,Arrow3);

// show gantry angle
draw(Label("$\theta_{g}$", (2, -0.5, 0)), arc((0, 0, 0), (ax_scale/3, 0, 0), (0, -ax_scale/3, 0)), red, arrow=Arrow3);

// kV schematic
real dlat=-13, dlng=0, dvrt=50;
triple det_cent=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det_cent-(0,ulen/2,vlen/2));

triple det0 = det_cent-(0,ulen/2,vlen/2);
triple u = (det0+ax_scale/2*(0,1,0));
triple v = (det0+ax_scale/2*(0,0,1));
triple w = (det0+ax_scale/2*(-1,0,0));

// detector norm
triple dnorm = (det_cent+ax_scale*(-1,0,0));

// detector coordinate system
draw(det0--u,black,Arrow3,L=Label("$u$", position=EndPoint, align=W));
draw(det0--v,black,Arrow3,L=Label("$v$", position=EndPoint, align=N));
draw(det0--w,black,Arrow3,L=Label("$w$", position=EndPoint, align=N));

draw(detector, black);

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// lines from source to detector edges
draw(src..det_cent-(0,-ulen/2,-vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,-ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,vlen/2),black+opacity(0.15));
draw(src..det_cent-(0,ulen/2,-vlen/2),black+opacity(0.15));

// ray connecting the source to the detector
triple ray_sd = det_cent-src;
draw(L=Label("$\vec{r}_{sd}$", position=EndPoint, align=E), src--det_cent, blue, Arrow3);

// dot product of ray onto normal vecotr
real along = dot(ray_sd, dnorm);

// detector projection operator
transform3 proj=planeproject(detector);

// show pierecing point
triple pierce = proj*src;

draw(L=Label("$\vec{p}$", position=EndPoint, align=NW),src--pierce,blue+dashed,arrow=Arrow3);
draw(L=Label("$\vec{p}_{uv}$", position=EndPoint, align=SE),det_cent--pierce,red+dashed,arrow=Arrow3);
// draw(src--pierce,red+dotted, arrow=Arrow3);
#+END_SRC

#+CAPTION: Schematic of a single projection view and the associated variables used in building the projective transform matrix $\left( \boldsymbol{X} \right)$ for that view. The $\left\{x, y, z \right\}$ coordinate system corresponds to the standard IEC global coordinate system, and the $\left\{u, v, w \right\}$ coordinate system corresponds to the detector frame vectors for that view. The red arrow labeled by $\theta_g$ denotes the gantry rotation angle which is defined from the $x$ axis as shown here for the kV imaging system. The blue vector $\vec{r}_{sd}$ points from the source to the detector center, and the blue vector $\vec{p}$ shows the piercing point of the x-ray source on the detector. The red vector $\vec{p}_{uv}$ corresponds to the piercing point in the detector basis as calculated in Equation ([[ref:eq:geo_pierecuv]]).
#+LABEL: fig:geo_cal_proj
#+ATTR_LaTeX: :width 0.9\textwidth
#+RESULTS: fig:geo_cal_proj
[[file:figures/geo/cal_proj.pdf]]

With this new piercing point, it is possible to now construct a
transform that projects the fiducials as well as transforms them to
the detector basis. The transform to the detector basis is represented
by the homogeneous coordinate transform
\begin{equation}
  \boldsymbol{G} = \begin{bmatrix}
    u_i & u_j & u_k & -r_{s,x} \\
    v_i & v_j & v_k & -r_{s,y} \\
    w_i & w_j & w_k & -r_{s,z} \\
    0 & 0 & 0 & 1
  \end{bmatrix}.
  \label{eq:geo_gmat}
\end{equation}
where $\left[-r_{{s,x}}, -r_{{s,y}}, -r_{{s,z}} \right]$ are the
room-coordinate components of the source position. Then using the
orthogonal ray component found in Equation ([[ref:eq:geo_along]]), the
homogeneous coordinate projection matrix is
\begin{equation}
  \boldsymbol{P} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 &\frac{1}{L}
  \end{bmatrix}.
  \label{eq:geo_pmat}
\end{equation}
Using these transforms so that they are pre-multiplied by the fiducial
position vectors, the combined transform is then
\begin{equation}
  \boldsymbol{M} = \boldsymbol{G}\boldsymbol{P}.
  \label{eq:geo_magicmat}
\end{equation}
which transforms a room coordinate point into the detector basis, and
then projects it onto the detector plane

Finally, this information can be combined to create a single transform
of the fiducials in the global image coordinate system to the
projected spots on the detector in discretized detector bin
coordinates. First, the coordinates of the piercing point must be
calculated in the detector basis as
\begin{equation}
  \vec{p}_{uv} = \left(\vec{p}-\vec{r}_d\right)\boldsymbol{G},
  \label{eq:geo_pierecuv}
\end{equation}
where $\vec{r}_d_{}$ is the center of the detector in room coordinates.
With all this, the projection transform used to calculate the
projected fiducials in discretized detector bin coordinates is
\begin{equation}
  \boldsymbol{X} = \boldsymbol{M}\boldsymbol{T} (\vec{p}_{uv})
  \boldsymbol{S}\left( \left[\frac{1}{s_{\text{pix}}},
    \frac{1}{s_{\text{pix}}}, 1 \right]\right) \boldsymbol{T} \left(
  \left[\frac{u_{\text{len}}}{2}+0.5, \frac{v_{\text{len}}}{2}+0.5, 0
    \right] \right),
  \label{eq:geo_xproj}
\end{equation}
where $boldsymbol{S}$ is a scaling transformation along the $\left\{
u,v \right\}$ basis by the inverse of the pixel size $\left(
s_{{\text{pix}} \right)$, and \boldsymbol{T} is a translation
transformation to place the origin of the discretized detector basis
at the center of the corner pixel.

With the projection transform $\boldsymbol{X}$, each vector
corresponding to the fiducials on the Isocal phantom can be projected
onto the discretized detector basis as illustrated in Figure
([[ref:fig:geo_cal_schematic]]). These projected spots are then matched to
the detector spots in the real sinogram. The $L_2$ norm between the
real and simulated projected spots is then calculated and serves as
the cost function for the optimization-based calibration.

As with other optimization-based calibration procedures, we
iteratively vary the parameters corresponding to the geometric degrees
of freedom (DOF) of the scanning trajectory. The phantom pose
(position and orientation) is first allowed to vary in the room
coordinate system to account for potential setup errors between the
room coordinates and the modeled position of the phantom. Once the
pose of the Isocal phantom is identified, then the source, detector,
and patient couch translations and rotations are allowed to vary, and
the cost of the simulated fiducial projections are calculated at each
step. We use the Nelder-Mead simplex algorithm
cite:lagarias_convergence_1998 to minimize the $L_{2}\text{-norm}$ cost
function.

Given that there are there are different combinations of couch, source
and detector motions that can cause the same change of the object
relative to the source and detector within the image coordinate
system, there are some degrees of freedom that can couple with others.
For instance, shifting the patient in the positive longitudinal
direction is effectively the same as allowing the source and detector
to move the same distance in the negative longitudinal direction. This
requires that only a few parameters are allowed to vary at once as
allowing too many parameters on this non-convex surface will often
produce nonphysical geometric corrections. Once the cost has been
minimized, the geometric offsets are used as the calibration
information for calculating the system matrix $\mathcal{H}$ for the
image reconstruction.

For a new trajectory, this phantom is first scanned to identify any
potential corrections to the parameters reported in the TrueBeam data
headers. Though we find the self-reported position accuracy from the
acquisition metadata to be very good, there are still some scanning
configurations for which the additional refinement from our geometric
calibration is critical for obtaining the best quality reconstruction.
This is particularly true for scanning trajectories where the object
and the kV imaging system move simultaneously.
*** Geometric-offset artifact catalog        :noexport:
    :PROPERTIES:
    :ID:       DED4A0A6-3775-41ED-AF64-BD6604B2B3AD
    :END:
Though the type of artifacts that are introduced by geometric offsets
for circular scanning trajectories are relatively well known, this
same sort of understanding is lacking for these new trajectories. To
study how geometric offsets affect images reconstructed from these new
trajectories, we will create a simulation catalog of artifacts
produced by different geometric errors. By introducing intentional
geometric inconsistencies in the reconstruction system matrix, we can
characterize the artifacts that appear in the reconstruction compared
to a numerically-exact inverse crime reconstruction.

As one of our primary objectives in using these novel trajectories is
to create an extended axial FOV image, we need to study how these
geometric errors degrade the image quality along the axial
direction. To ensure our simulation can adequately identify these
artifacts, we will create a simulated phantom such as an axially
extended version of the Catphan high resolution module. This will
provide resolution metrics not only in the axial dimension, but also
in the transverse planes as a function of axial position.

The simulation catalog of different artifacts that arise from
geometric offsets will provide a guide to visually identify potential
geometric errors based on the reconstructed image. This provide one
way in which we can verify the effectiveness of our geometric
calibration procedure. By incorporating the calibration information we
obtain with the calibration, known geometric error artifacts should be
reduced.
**** notebooks                               :noexport:
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"geometry/overview.ipynb")][geometry/overview.ipynb]]
- Overview of the simulated work/analysis
*** Image entropy                            :noexport:
    :PROPERTIES:
    :ID:       2410E321-8750-473F-B6B6-13DC1719B6AE
    :END:
To further verify the effectiveness of the calibration procedure, we
will also need to use additional metrics to quantitatively
characterize the impact of using the calibration on image quality. The
work of Wicklein et al. has suggested that the best metric for
measuring the impact of geometric error on image quality is entropy
$(E)$ of the image's gray-level histogram $(H)$. This is defined as
\begin{equation}
  \label{eq:entropy}
  E=-\sum_{q=0}^Q h(q)\cdot\text{log}(h(q))
\end{equation}
where $Q$ is the maximum intensity value and
\begin{equation}
  \label{eq:norm_hist}
  h(q)=\frac{H(q)}{N}
\end{equation}
is the normalized histogram cite:wicklein_image_2012. For this metric,
minimum entropy is obtained for an image with a single intensity value
while an image with uniform distribution over all intensity values
would have maximum entropy.

Geometric errors introduce blurring at sharp boundaries in the image
which increases the entropy. By reducing geometric errors with
calibration, this blurring effect and subsequently entropy should
reduced. For our non-circular trajectories, Wicklein's conclusion can
be verified readily with the images in our catalog of geometric
errors. The image entropy of the correct-geometry reconstruction will
be against the reconstructions with intentional geometric errors to
determine if improved geometric modeling reduces the image entropy in
Equation (\ref{eq:entropy}).

If the entropy calculations based on simulation agree with Wicklein's
findings, entropy would be reasonable metric to characterize the
benefits and limitations of using the geometric offsets from the
calibration phantom on different non-circular trajectory
reconstructions. We would then use entropy as the metric to compare
reconstructions with and without calibration. From this, we can not
only verify the effectiveness of our calibration method with different
non-circular trajectories, but also then characterize the impact
additional geometric corrections have on image quality.
*** Experimental validation
    :PROPERTIES:
    :ID:       150f19dd-e68d-4226-bdd4-01e31ea1176f
    :END:
To evaluate the efficacy of our calibration procedure, we investigated
its performance on calibrating both a standard, half-fan, circular
trajectory where the couch is stationary as well as a virtual
isocenter trajectory with the same object illumination where the couch
moves during the acquisition. For each of these trajectories, we used
the same Developer Mode script to scan both the Catphan phantom and
the Isocal phantom. We subsequently used the sinogram from the Isocal
scan to extract calibration offsets for that particular trajectory
using the calibration method described in Section ([[id:F53F4B5A-83EB-4B16-9B6D-F557D3E441C2][Calibration
method]]).

We reconstructed the Catphan scans from these two trajectories with
and without the calibrations offsets. We reconstructed onto an
isotropic image grid of 0.473 mm for each reconstruction, and applied
the half-fan weighting described in Section ([[id:cc6bcac6-a445-4dfb-8815-a95e31f517ed][Detector weighting]]). For
all of these reconstructions we used 200 iterations of MLEM as
described in Section ([[id:e0a24b69-d136-4f9a-9e85-dc42e1d114a9][MLEM]]).

*metric analysis on these two scans w/ and w/o calibration*
*** figures                                  :noexport:
**** four detector schematic
# +LABEL: fig:geo_schematic
#+BEGIN_SRC asymptote :file figures/geo/schematic.pdf :exports results :tangle no
settings.multisample=0;
settings.outformat="pdf";
settings.prc = false;
settings.render = 0;

import graph3;
import solids;
import three;

// view configuration
size(10cm);
// currentprojection=orthographic(-5,1,5,up=Y);
currentprojection=perspective(-5,1,5,up=Y);
// currentlight=White;

// Draw axis
// draw(Label("$y$",1),(0,0,0)--(0,5,0),red,Arrow3);
// draw(Label("$x$",1),(0,0,0)--(5,0,0),red,Arrow3);
// draw(Label("$z$",1),(0,0,0)--(0,0,5),red,Arrow3);

// kV schematic
real dlat=0, dlng=0, dvrt=50;
triple det=(dvrt,dlat,dlng);
real ulen=40.0, vlen=30.0;

path3 detector=plane((0,ulen,0), (0,0,vlen), det-(0,ulen/2,vlen/2));

transform3 rot090=rotate(90, Z);
transform3 rot180=rotate(180, Z);
transform3 rot270=rotate(270, Z);

path3 det090 = rot090*detector;
path3 det180 = rot180*detector;
path3 det270 = rot270*detector;

draw(detector, black);
draw(det090, black);
draw(det180, black);
draw(det270, black);

// labels
//From Charles Staats's tutorial
//Direction of a point toward the camera.
triple cameradirection(triple pt, projection P=currentprojection) {
  if (P.infinity) {
    return unit(P.camera);
  } else {
    return unit(P.camera - pt);
  }
}

//Move a point closer to the camera.
triple towardcamera(triple pt, real distance=1, projection P=currentprojection) {
  return pt + distance * cameradirection(pt, P);
}

label("$\theta=0^{\circ}$",red,align=S,position=towardcamera((det-(0, ulen/2, -vlen/2))));
// label("$B$",align=S,position=towardcamera((B)));
// label("$C$",align=SE,position=towardcamera((C)));
// label("$D$",align=SE,position=towardcamera((D)));
// label("$E$",align=NE,position=towardcamera((E)));
// label("$F$",align=S,position=towardcamera((F)));

// source
real slat=0, slng=0, svrt=-100;
triple src=(svrt,slat, slng);

// lines from source to detector edges
// draw(src..det-(0,-ulen/2,-vlen/2),black);
// draw(src..det-(0,-ulen/2,vlen/2),black);
// draw(src..det-(0,ulen/2,-vlen/2),black);
// draw(src..det-(0,ulen/2,vlen/2),black);

// Draw cylinder
// cylinder(startpoint3d, radius, length, along_this_axis)
triple start = (0,0,-8);
real length = 16;
real radius = 11.3;
triple ax = (0,0,1);
revolution r = cylinder(start,radius,length,ax);
draw(r,black);

// isocal spots
triple[] isocal={(0,-11.3,-7.5),
                 (7.9903,-7.9903,-7.5),
                 (7.9903,7.9903,-7.5),
                 (-11.3,0.0,-7.5),
                 (-7.9903,7.9903,-5),
                 (11.3,0.0,-3),
                 (0,11.3,-2),
                 (-10.4398,4.3243,2),
                 (4.3243,10.4398,3),
                 (-10.4398,-4.3243,5),
                 (4.3243,-10.4398,5),
                 (10.4398,-4.3243,7.5),
                 (10.4398,4.3243,7.5),
                 (-4.3243,10.4398,7.5),
                 (-4.3243,-10.4398,7.5)
};

dot(isocal, black);

// project points
transform3 proj=planeproject(detector);
transform3 proj090=planeproject(det090);
transform3 proj180=planeproject(det180);
transform3 proj270=planeproject(det270);

dot(proj*isocal,red);
dot(proj090*isocal,red);
dot(proj180*isocal,red);
dot(proj270*isocal,red);
#+END_SRC

#+CAPTION: Schematic representation of the scanning geometry
#+ATTR_LaTeX: :width 0.75\textwidth
# +RESULTS: fig:geo_schematic
** Results
   :PROPERTIES:
   :ID:       bc50c80a-fbb7-41d3-a9d0-ebc552f59896
   :END:
*** Experimental validation
    :PROPERTIES:
    :ID:       2c3c25d5-477a-4013-bf2a-5a74716b9c20
    :END:
Figure ([[ref:fig:geo_cal_catphan_sens]]) shows the CTP 528 spatial
resolution module slice from the reconstructions of both the circular
scan in the left column and the virtual isocenter scan in the right
column. The top row shows the slice from the uncalibrated
reconstruction using the nominal projection geometry from image
metadata. It can be seen by comparing the circle and virtual isocenter
scans without calibration that moving the treatment couch during the
scan introduces additional geometric error over the standard circle
scan which visually degrades spatial resolution.

The bottom row of Figure ([[ref:fig:geo_cal_catphan_sens]]) shows the same
slice from the corresponding trajectory with the geometric offsets
from the calibration procedure incorporated into the system matrix
$\mathcal(H)$. For the circular scan, using the calibration
information does provide a bit of an improvement in spatial
resolution. However, the efficacy of the calibration method is
particularly striking for the virtual isocenter scan. By using the
calibration offsets in the reconstruction model, the spatial
resolution of the virtual isocenter reconstruction becomes comparable
to that of the circular scan.

Figure ([[ref:fig:geo_cal_cost]]) shows the $L_{2}_{}-\text{norm}$ of the
distance between the simulated fiducial projections and the real
fiducial projections acquired from the circle and virtual isocenter
scans of the isocal phantom. As this served as the cost function which
we used as the minimization objective for the optimized offset search,
we can see that the calibration did effectively reduce this cost from
the nominal geometry (blue) to the calibrated geometry (green). This
cost also reflects the same trend we see in the spatial resolution of
the images shown in Figure ([[ref:fig:geo_cal_catphan_sens]]).

Comparing the the $L_{2}_{}-\text{norm}$ of the uncalibrated scans in Figure
([[ref:fig:geo_cal_cost]]), we see that there is far more disagreement
between modeled and observed Isocal fiducial positions for the virtual
isocenter scan than that of the circular scan, leading to more
artifacts and loss of spatial resolution in the virtual isocenter
reconstruction than in that of the circular scan. With the geometric
calibrations applied, the cost for the virtual isocenter and circular
trajectories is quite comparable, as is the spatial resolution.

*describe TB1 virtual isocenter couch backlash*
#+BEGIN_EXPORT latex
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/catphanCalComp}
    \caption{}
    \label{fig:geo_cal_catphan_sens}
  \end{subfigure}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad,
  % \hfill etc.
  % (or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/geo/costComp1p5}
    \caption{}
    \label{fig:geo_cal_cost}
  \end{subfigure}
  \caption{(a) shows the 200$^{\text{th}}$ iteration of MLEM
    reconstructions of the CTP 528 spatial resolution module from the
    Catphan phantom for two different trajectories. The left column is
    from a 1.5X circular scan, and the right column is from a 1.5X
    virtual isocenter scan reconstructed onto a 0.473 mm isotropic
    image grid([-100, 2000] HU). The top row shows the reconstruction
    using the nominal geometry from self-reported metadata, and the
    bottom row corresponds to the calibrated reconstructions. (b)
    shows the $L_{2}$-norm used for the calibration cost function
    before (blue) and after (green) calibration for both the circle
    (left) and the virtual isocenter (right).}
  \label{fig:geo_cal_sens_cost}
\end{figure}
#+END_EXPORT
**** figures                                 :noexport:
- [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/calibration_images.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/calibration_images.ipynb]]
- [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb]]
**** notes                                   :noexport:
- [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb")][170603_virtiso_circ_smth_catphan/dynmag/calibs/calib_analysis.ipynb]]
** Conclusion
   :PROPERTIES:
   :ID:       fd41d566-a4b3-4dcd-9f8c-7417276ad25c
   :END:
In developing our optimization-based geometry calibration procedure,
we found that proper geometric calibration is a critical component of
improving tomographic image quality. This is particularly true for
more complicated trajectories where additional motion components such
as that of the treatment couch introduce additional degrees of freedom
in which geometric errors can arise. As shown in Figure
([[ref:fig:geo_cal_sens_cost]]), the additional motion of the couch with
the simultaneous motion of the source and detector introduces a larger
deviation from the nominal scanning geometry.

The optimization-based calibration we used in this study provides a
robust framework for calibrating arbitrary scanning trajectories. The
ability to acquire view-by-view calibration information with this
approach dovetails nicely with the optimization-based framework that
enables the reconstruction from the different trajectories we studied
in this research. Though many of the different analytic-based methods
described in the literature could be adopted to many of these
trajectories (and have been for some), the benefit of the
optimization-based framework for both reconstruction and geometric
calibration comes from freedom to easily model and reconstruct from
trajectories as well as geometric offsets that deviate from the
analytically prescribed model.

Though this does imply that calibration scans must be acquired for
each scan of interest, there are optimization-based calibration
methods similar to ours that attempt to extract calibration
information with no /a priori/ knowledge of the phantom
cite:panetta_optimization-based_2008. Such calibration methods or
built-in calibration markers in the table are potential ways in which
it would be possible to avoid acquiring calibration information for
every scan of interest. As we used the TrueBeam kV-CBCT system for our
data acquisition, Varian's Isocal phantom provided a convenient means
of calibrating the imaging system as the linac use case already
demands accurate calibration for treatment accuracy in addition to
image quality alone.

In the following chapters we where we investigate particular
applications of these different trajectories, we will use our
calibration method with the Isocal phantom to more accurately model
the system matrix $\mathcal(H)$. Though the more exotic scanning
trajectories introduce more degrees of freedom that create greater
geometric uncertainty, our calibration procedure performs rather well
in determining what these deviations are from the self-reported
geometry metadata. For these trajectories, we found that
incorporating geometric calibration consistently improves image
quality.
* Axial field-of-view extension              :fov:
  :PROPERTIES:
  :ID:       eaae199f-f899-4862-af50-720895a31c36
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       7c250434-fff6-41a3-aea3-e7bc9ff88dc6
   :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
*** publications
    :PROPERTIES:
    :ID:       48459222-20e7-43e5-9863-5022a5803a1b
    :END:
**** cite:davis_extended_2013
     :PROPERTIES:
     :ID:       5b4c7bca-d59b-4f33-8151-a6b359071249
     :END:
- simulation study of axial FOV extension
**** cite:davis_verifying_2013
     :PROPERTIES:
     :ID:       d4c20a7d-4982-4318-b591-9ff84ee809f5
     :END:
- Trilogy scans of RANDO and Defrise phantom for axial FOV extension
**** cite:pearson_investigation_2013
     :PROPERTIES:
     :ID:       6ae09b4c-d1d3-4705-b110-8a4a0e1f33dd
     :END:
- Similar results to [[id:d4c20a7d-4982-4318-b591-9ff84ee809f5][cite:davis_verifying_2013]] using RANDO and Defrise
  Trilogy scans
**** cite:davis_we-g-brf-07:_2014
     :PROPERTIES:
     :ID:       3f9687ce-f913-43a0-8e96-0ace96d7f67c
     :END:
- AAPM talk using CLLC scan from TrueBeam
**** cite:davis_su-e-i-02:_2015
     :PROPERTIES:
     :ID:       15f62bff-3fae-4083-b4b1-ad0594d25121
     :END:
- AAPM poster for disk phantom metrics
**** cite:davis_non-circular_2015
     :PROPERTIES:
     :ID:       cee07d24-100a-4c78-a42d-59cd707cda3b
     :END:
- Varian meeting showing non-circular scans
** Introduction
   :PROPERTIES:
   :ID:       b815fcd4-92c6-4f72-9905-10acc22b580e
   :END:
# What question (problem) was studied?
A major limitation of linac-mounted CBCT kV-imaging systems is their
limited axial coverage. This is partially due to the detector size
which is restricted due to engineering concerns. Another reason for
this limited coverage is the prevalence of analytic-based
reconstruction algorithms in the clinic cite:pan_why_2009. These
algorithms, such as FDK cite:feldkamp_practical_1984, are known to
suffer from cone-angle artifacts at the axial extremes of the
reconstruction volume. The increasing severity of these artifacts at
the axial extremes is partially why developing larger kV detectors
have not been pursued.

With the robust trajectory framework allowed by optimization-based
algorithms, we are no longer bound to the strict trajectory
limitations imposed by analytic-based reconstruction methods. Provided
that the correct geometry of the acquisition trajectory is well
understood, the system matrix of the image formation process can be
calculated for arbitrary CBCT scanning configurations. These new
trajectories are no longer limited to the few cases of non-circular
trajectories for which analytic inverse formulations exist such as the
line cite:sidky_volume_2005, circle and line
cite:zeng_cone-beam_1992,katsevich_image_2004, circle and arcs
cite:zou_image_2005-1,katsevich_image_2005, and non-planar orbits
cite:kudo_derivation_1994

With some of the trajectories enabled by these optimization-based
algorithms, we can address the problem of the limited axial coverage
for these kV-imaging systems. For trajectories where the source and
detector move in the axial direction relative to the patient,
projection data can be obtained for axial positions beyond what is
acquired with a traditional circular trajectory. Optimization-based
reconstruction algorithms make it possible to formulate an imaging
trajectory that can provide extended axial coverage beyond what is
currently possible with circle trajectories and analytic-based FDK.

In this chapter, we study a few different trajectories that can
address the limited axial coverage provided by linac-mounted
kV-imaging systems with a limited axial coverage that, as in our
TrueBeam system, can be half the size of the MV-treatment beam FOV.
This limitation can be problematic for patients with treatment volumes
that have axial coverage beyond what is visible in a single circular
acquisition cite:voong_dosimetric_2014. Using optimization-based
reconstruction, we studied some potential trajectories and then
compared the subsequent extended-axial images to the clinical FDK
standard. The hypothesis is that there can be extensive gains in
clinical utility for these extended volume images provided that the
image quality is comparable to the current clinical standard.

** Methods
   :PROPERTIES:
   :ID:       b42e5e65-dfda-4692-8ea6-f6d96bc1dd5b
   :END:
*** Trajectories
    :PROPERTIES:
    :ID:       b16942be-e3d8-4fb2-a872-aa68fe6bd4fd
    :END:
 We define a scanning trajectory as the sequence of source and detector
 positions used to acquire each projection view. The coordinates of the
 trajectory are then defined relative to a fixed origin in the patient.
 Moving either the imaging arms or the patient, it is possible to
 create a component of axial translation that acquires projection views
 with information of the patient volume at extended axial positions.
 Moving either the imaging arms or the patient is equivalent provided
 the relative translation is correctly accounted for when developing
 the system matrix for the acquisition.

 The current clinical method of obtaining an extended axial image
 involves stacking the FDK reconstructions of two circular scans at
 different axial positions. For this reason, the first trajectory we
 wanted to study was the dual-circle trajectory show in Figure ([[ref:fig:traj_dcirc][double
 circle]]). This provided a direct comparison between the clinical
 practice of using analytic-based to optimization-based reconstruction
 of the entire image volume using all the dual-circle projection data
 at once.

 In addition to the dual circle trajectory, we studied two additional
 trajectories. One trajectory consists of two circles with a line
 connecting them in the axial direction shown in Figure ([[ref:fig:traj_clc][clc]]). We will
 refer to this as the circle-line-circle (CLC) trajectory. The other
 trajectory is a smooth trajectory that translates the source in the
 axial direction while the gantry rotation is slowing and reversing
 direction before acquiring the second circle show in Figure ([[ref:fig:traj_smth][smooth]]).

#+BEGIN_LaTeX
\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{img/traject/dual_circle_20cm.png}
    \caption{Dual circle}
    \label{fig:traj_dcirc}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{img/traject/dual_circle_line_20cm.png}
    \caption{Circle line circle}
    \label{fig:traj_clc}}
\end{subfigure}
~
\begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[width=\textwidth]{img/traject/dual_smth_20cm.png}
  \caption{Smooth}
  \label{fig:traj_smth}}
\end{subfigure}
~
\caption{The left image shows the dual circle trajectory. The center
  image shows the circle-line-circle trajectory. The right image shows
  the smooth trajectory. These diagrams of the source trajectory do
  not show the opposing trajectory of the detector around the digital
  patient.}
\label{fig:poly}
\end{figure*}
#+END_LaTeX

**** Trajectories
     :PROPERTIES:
     :ID:       4787aeca-bdf3-4b8b-bfda-6c7248cb01d4
     :END:
 After simulation, we then wanted to see how well combining these
 trajectories with optimization-based reconstruction could extend the
 axial FOV with real projection data. Based on the simulation results,
 we acquired a scans from spacings of 17, 18,19, and 20 cm between the
 two circular planes of these trajectories. These larger spacings
 provided data for potential axial FOV's of 37, 38, 39, and 40 cm
 respectively.

 For each of these spacings, we acquired the same three trajectories
 mentioned in the simulation study. First the dual-circle trajectory
 was acquired by moving the c-arms and acquiring circular scans at two
 different axial positions. We then acquired the CLC trajectory with
 the c-arms by also acquiring projection data as they translated along
 the axial direction from the plane of the first circle to the plane of
 the second circle. Finally, the smooth trajectory was acquired by
 having the treatment couch move relative to the source and detector as
 the gantry rotated. *maybe cut this last sentence?* While we would
 like to demonstrate this motion while moving only the c-arms during
 the gantry rotation, this is unfortunately not possible with our
 TrueBeam model due to hardware limitations.

 #+NAME: tab:disk_trajects
 | Trajectory | Axial gap [cm] | Detector configuration | Air scan  | lognorm | Notes                      |
 |------------+----------------+------------------------+-----------+---------+----------------------------|
 | Circle     |                | Full                   |           | x       |                            |
 | Circle     |                | Half                   |           | x       |                            |
 |------------+----------------+------------------------+-----------+---------+----------------------------|
 | CLLC       |             20 | Full                   | 20 cm air | x       |                            |
 | CLLC       |             19 | Full                   | ""        | x       |                            |
 | CLLC       |             18 | Full                   | ""        | x       |                            |
 | CLLC       |             17 | Full                   | ""        | x       |                            |
 |------------+----------------+------------------------+-----------+---------+----------------------------|
 | CLLC       |             18 | Half                   |           | x       | 2 of these for some reason |
 |------------+----------------+------------------------+-----------+---------+----------------------------|
 | SMTH       |             20 | Full                   |           | x       |                            |
 | SMTH       |             19 | Full                   |           | x       |                            |
 | SMTH       |             18 | Full                   |           | x       |                            |
 | SMTH       |             17 | Full                   |           | x       |                            |
 |------------+----------------+------------------------+-----------+---------+----------------------------|
 | SMTH       |             18 | Half                   |           | x       |                            |
 |------------+----------------+------------------------+-----------+---------+----------------------------|

**** Phantoms
     :PROPERTIES:
     :ID:       cd67b907-804c-4212-bf45-6ca6daad766c
     :END:

*** Simulation
    :PROPERTIES:
    :ID:       C07F2BE3-413C-490F-B171-0ADB82B939EA
    :END:
Clinical extended-axial-FOV images are obtained by stitching together
two circular scans at different axial locations. We first wanted to
find the maximum axial coverage that can be achieved with such a
trajectory. That is, what would be the maximum axial spacing between
the two planes of the source's trajectory for which a useful extended
volume image could be reconstructed?

When an extended axial FOV is needed in the clinic, in current
practice two circular scans are acquired at different axial positions.
Once each independent volume has been reconstructed with the
analytic-based FDK, the two volumes are stacked to create the extended
image. While this stacked image does have extended axial coverage,
there are problems with this approach that can be alleviated with
iterative reconstruction techniques. To evaluate this, we compared the
simulated results of stacking FDK images from two circular scans to
the iterative reconstruction from the combined projection data of the
two circular scans treated as a single trajectory composed of two
circles cite:davis_extended_2013.

Simulating forward projections from this trajectory, we compared the
images obtained from stitching together the independent FDK images to
those obtained by reconstructing the two circles as a single
trajectory with MLEM. We also compared stacked FDK images to
reconstructions from the simulated *CLC* and smooth trajectories.

We simulated a Defrise-style phantom modeled with the 3D X-ray
projection software TAKE cite:seger_matlab/c_2005. The phantom was
composed of a 15.2 cm outer diameter acrylic cylinder with alternating
density disks of Delrin and cork 0.5 cm thick. This particular phantom
with alternating density disks is acknowledged by the authors of FDK
as being particularly susceptible to cone-angle artifacts
cite:feldkamp_practical_1984.

We used the TAKE software to forward project the phantom as well as
generate a digitized ''truth'' phantom for calculating comparison
metrics. The projector generates a forward projection from a specified
trajectory given a mathematical definition of the phantom as well as
its material properties and the spectrum generated by the x-ray
source.

We created projection data for a set of dual-circle trajectories that
had an increasing amount of axial separation between the two circles.
With a 1.5x magnification factor and a 30 cm detector size along the
axial direction, a single circular scan has a maximum axial coverage
of 20 cm in the image space. Furthermore, the maximum spacing between
the two circles is 20 cm as any separation larger than this means the
independent image volumes from the two circles are no longer
contiguous. We therefore created trajectories with 10, 12, 14, 16, 18,
and 20 cm separations *only show the larger gaps that are of
interest?* between the planes of the source's dual-circle trajectory.

We uniformly distributed 600 views over the entire trajectory which is
comparable to the total number of views used in a single clinical CBCT
scan with the kV imaging system. For the other two trajectories with a
component of projection views taken during the axial translation (CLC
and smooth), 600 views were used with 20% of the views being
distributed along the axial translation stage.

*FIX*

The reconstruction image space consisted of a $256\times256$ transverse
grid of 1 mm isotropic voxels. As the spacing between the circles
increased, the number of voxels in the axial direction also increased
to accommodate the increasingly large FOV.

For the extended-volume reconstruction using the stacked FDK, we
independently reconstructed each circular scan with FDK using a
standard Hann filter. To combine the two reconstructed volumes for an
extended axial-coverage image at a given spacing, we used the midplane
between the two planes of the source's circular trajectory to select
how much of each reconstruction to put in the combined image.

For the MLEM reconstructions, we used all of the projection data
simultaneously to reconstruct the extended volume. After defining the
extended image volume, we computed the system matrix for each of the
different spacings and trajectories based on the trajectory of the
source and detector. We used 100 iterations *justify choice* of the
MLEM algorithm to find an estimate for the image.

*FIX*

Our initial evaluation of the images obtained from non-circular
trajectories is simply a qualitative visual inspection which does
provide an informative assessment of the variety of artifacts that
occur for a given reconstruction. For a more rigorous evaluation of
the images obtained from different trajectories, we will use mutual
information (MI) cite:pluim_mutual-information-based_2003 and the
universal quality index (UQI) cite:wang_universal_2002 to provide a
quantitative assessment of the image similarity between the reference
image and the images from different trajectories.

*** Data
After identifying potential benefits from using these non-circular
trajectories with iterative reconstruction methods from simulations,
we acquired experimental scan data with Varian's Trilogy OnBoard
Imager. Though the Trilogy lacks the mechanical control features of
the TrueBeam Developer Mode, we were able to acquire circular scans at
different axial positions by translating the treatment couch.

The image quality that results from using these trajectories with
optimization-based algorithms must be quantitatively evaluated for the
different trajectories and spacings chosen. Given that contrast
resolution is on of the tasks that enables physicians to utilize the
imaging system's clinical potential cite:dawson_advances_2007, we
wanted to characterize the low-contrast resolution as a function of
axial position for the different trajectories and spacings.

As the extended axial coverage we obtain with the kV imaging system
using these methods is novel, the authors do not know of any standard
phantom for characterizing low-contrast resolution as a function of
axial position within a single scan. For this reason, we built a
custom low-contrast disk phantoms that fit in an acrylic tube with
extended axial coverage as shown in Figure ([[ref:fig:disk_tube][tube setup]]). The disks
themselves, such as the one shown in Figure ([[ref:fig:lc_disk][low constrast disk]]) *get
updated disk*, are designed to provide similar metrics such as those
obtained with the Catphan (The Phantom Laboratory, Salem, NY)
phantom's low-contrast module CTP515. Additionally, the largest holes
are designed to hold the different electron density plugs from the
Gammex (Middleton, WI) RMI tissue characterization phantom. By placing
four of these disks in the tube, we can obtain these metrics as a
function of axial position within a given reconstruction.

#+BEGIN_LaTeX
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=1.75in]{img/poly/20150408_183320_setup.jpg}
    \caption{Disk phantom tube}
    \label{fig:disk_tube}
  \end{subfigure}
  \qquad
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=1.75in]{img/poly/20150609_disk_phantom.jpg}
    \caption{Low-contrast disk phantom}
    \label{fig:lc_disk}}
\end{subfigure}
\caption{The left image shows the experimental setup of the acrylic
  tube with four low-contrast disks. Given they symmetry of the
  scanning geometry, one disk is placed at the plane between the two
  circles. The remaining 3 are place at different axial positions in
  one half of the image volume. The right image shows one of the
  low-contrast disks with the larger holes for the solid water RMI
  inserts.}
\label{fig:poly}
\end{figure}
#+END_LaTeX

To study these techniques with an anthropomorphic phantom, we used the
CIRS (Norfolk, VA) Model 600 torso phantom shown in Figure ([[ref:fig:ed][ed]]). Using
a 14 cm offset detector, we acquired a smooth trajectory scan at *18
cm spacing*? This scan was chosen as it had the best performance with
the metrics obtained with the disk phantom.

#+BEGIN_LaTeX
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[height=1.75in]{../../research/trajectories/phantoms/cirs/20140901_ed_cllc.j

      \caption{CIRS torso phantom}
      \label{fig:ed_setup}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includegraphics[height=1.75in]{../../research/trajectories/phantoms/cirs/600_PHO_01.jpg}
      \caption{Phantom cross sections}
      \label{fig:ed_secs}}
  \end{subfigure}
  \caption{The left image shows the experimental setup of the CIRS
    torso
    phantom. The right image shows the cross sections of this phantom
    with some of its low-contrast features.}
  \label{fig:ed}
\end{figure}
#+END_LaTeX

While a 20 cm spacing between the two circles is the theoretical
maximum spacing where contiguous volumes can be combined in an
extended axial image, there are some practical limitations that
further limit the spacing. Specifically, the edge of the kV detector
has a few rows of unusable pixels. When the maximum spacing is used,
these rows result in a small volume in the image where there is no
redundancy to fill in the corrupted volume. The lack of redundancy in
this overlap region also makes the reconstruction more susceptible to
geometric artifacts like the vertical displacement in the couch
between the two longitudinal positions. The geometric calibration
phantom is being developed to detect such displacements which will
reduce the subsequent artifacts in these regions. By using a slightly
smaller spacing of 18 cm which still produces an axial FOV of 38 cm,
sufficient redundancy is available to reconstruct this volume.

 Subsequent experimental scans were performed with Varian's TrueBeam
 kV imaging system. This allowed the use of the TrueBeam Developer Mode to acquire
 scan data from non-circular trajectories other than two circles. In
 addition to programming couch motion, Developer Mode allows the motion
 of the source and detector arms to be programmed as a series of
 control points. The advantage of using just the imaging arms to cover
 an extended axial volume is that the patient can be left at the
 isocenter. This is very advantageous as moving the patient will always
 introduce some uncertainty after moving the patient back, and it is
 more comfortable for the patient when the couch does not move.

 For the dual-circle-extended-line trajectory, it was possible to
 acquire this in Developer Mode by controlling just the c-arms. This
 trajectory shown in Figure (\ref{fig:cllc_traj}) acquires data from
 two circles 19 cm apart and during the translation of the source and
 detector from the superior to inferior circle. It also acquires an
 additional 4.5 cm linear scan above and below the two circles, which
 helps reduce the cone-angle artifacts at the edge of the
 FOV. Downsampling from this data set can provide scan data for the
 dual circle, the dual circle and line, and the dual circle extended
 line. We scanned the CIRS Torso Phantom (Computerized Imaging
 Reference Systems, Norfolk, VA) which contains low-contrast structures
 unlike the RANDO phantom.

 #+BEGIN_LaTeX
   \begin{figure}
     \centering
     \subfloat[Source
     trajectory]{\includegraphics[height=4.5cm]{img/tb/cllc_detector_trajectory_18cm.png}%
       \label{fig:cllc_traj}}
     \qquad
     \subfloat[Mid-sagittal]{\includegraphics[height=4.5cm]{img/tb/ed_cllc_19cm_sag}%
       \label{fig:ed_sag}}
     \qquad
     \subfloat[Mid-coronal]{\includegraphics[height=4.5cm]{img/tb/ed_cllc_19cm_cor}%
       \label{fig:ed_cor}}
     \caption{Dual-circle-extended-line TrueBeam acquisition and
       reconstruction. The left image (a) shows the kV source trajectory
       from the data headers of the scan implemented with Developer
       Mode. Figures (b) and (c) are the sagittal and coronal views of
       200$^{\text{th}}$ MLEM iteration reconstruction of the CIRS torso phantom
       in this scan.}
     \label{fig:cllc}
   \end{figure}
 #+END_LaTeX

 One limitation of our version of Developer Mode is that it is not
 possible to move the source and detector arms during the gantry
 rotation. For this work, any desired arm translation during gantry
 rotation is mimicked with the couch translation whenever it will
 produce an equivalent relative translation. While this is not exactly
 equivalent, it allows a close approximation of such a trajectory. For
 this reason, the smooth trajectory discussed in the previous section
 was implemented with gantry rotation and couch translation.

 To create the stacked FDK reconstructions, we first reconstructed the
 FDK volume from each circular scan. We then used the axial plane at
 the midpoint between the two circular planes as a discriminator for
 selecting the FDK volume from which each slice was taken. For
 iterative reconstructions, we used 100 iterations of the maximum
 likelihood expectation maximization (MLEM) algorithm
 cite:dempster_maximum_1977,shepp_maximum_1982 to reconstruct the
 entire extended image from all of the projection data at once. Both
 the FDK and iterative reconstructions used the same 256 x 256
 transverse grid size with 1 mm isotropic voxels. Figure
 (\ref{fig:sim_take}) shows mid-sagittal slices from some of these
 simulations.

 By increasing the separation between the two circular scans up to the
 maximum spacing of 20 cm, it is apparent that FDK has fundamental
 support limitations that restricts the acceptable distance between
 these two circles to less than the maximum 20 cm spacing. The gap in
 the axial coverage is a region with insufficient scan information
 which is known as the shadow zone cite:forthmann_adaptive_2009. The
 MLEM algorithm can use information from both circular scans
 simultaneously, leading to improved reconstruction of the image in the
 shadow zone as seen in Figure (\ref{fig:sim_take}). It is likely that
 if such an extended image volume were to be used in the clinic, it
 would be centered on an object of interest. If this were so, the most
 important content in the image would be in the region most afflicted
 by cone-angle artifacts.

 To acquire more scan information from the central region between the
 two circles, we also simulated two other trajectories. Both of these
 trajectories acquire additional view while translating the source and
 detector from the axial position of one circle to the other. One of
 these trajectories is a circle-line-circle trajectory where the
 translation phase of the source and detector between the two circles
 is a line as Figure (\ref{fig:traj_clc}) shows. The other trajectory
 shown in Figure (\ref{fig:traj_smth}) is a smooth trajectory that is
 similar to the circle-line-circle, but the gantry rotation and
 longitudinal translation are designed to avoid sharp transition points
 in the imaging trajectory.

 #+BEGIN_LaTeX
   \begin{figure}
     \centering
     \subfloat[Circle line circle]{\includegraphics[width=0.45\textwidth]{img/traject/clc.pdf}%
       \label{fig:traj_clc}}
     \qquad
     \subfloat[Smooth]{\includegraphics[width=0.45\textwidth]{img/traject/smth.pdf}%
       \label{fig:traj_smth}}
     \caption{The left image (a) shows the circle-line-circle trajectory
       with a 20 cm spacing between the circles. The right image (b)
       shows the smooth trajectory with a 20 cm spacing between the
       circles. The units for the axes are in centimeters, and the color
       gradient shows the temporal evolution of the source position.}
     \label{fig:poly}
   \end{figure}
 #+END_LaTeX

 #+CAPTION: Plot of the RMSE for extended volumes of different trajectories with different spacing between planes of the circles, compared to the central CBCT volume of a single circular scan.
 #+ATTR_LaTeX: :width \textwidth
 #+LABEL: fig:take_rmse
 file:./img/take/central_cbct_rmse.pdf


** Results
   :PROPERTIES:
   :ID:       b2a353e8-8531-4a0e-8337-9f702ecf02f8
   :END:
*** Simulation
:PROPERTIES:
:ID:       ff434d74-757c-47b8-bd98-9250d2751ff2
:END:
The initial simulation results demonstrated promising advantages to
using the optimization-based reconstruction to produce extended-axial
coverage images.

- [ ] insert comparison table of FDK
- [ ] show plot of RMSE in the overlap region

From the results shown in Figure (\ref{fig:})
*** Data
 We repeated the same reconstruction process of stacking FDK
 reconstructions and using MLEM to iteratively reconstruct the entire
 image volume from a single trajectory composed of two circular
 scans. One phantom was a Defrise disk phantom of the same dimensions
 of the simulated disk phantom, with alternating disks of Delrin and
 cork. The other phantom was the upper torso of the RANDO Man phantom
 (The Phantom Laboratory, Salem NY). The experimental results shown in
 Figure (\ref{fig:trilogy_data}) demonstrate the same finding that
 iterative reconstruction could recover more of the shadow zone where
 the FDK fails.

 #+BEGIN_LaTeX
   \begin{figure*}[t!]
     \centering
     \begin{tabular}{lccc}
       \toprule
       &Central Single Circle&12~cm Double Circle&20~cm Double Circle\\
       \midrule

       FDK&
       \includegraphics[height=3cm]{img/trilogy/stacked_defrise_00cm_256x256x200_sag.png}&
       \includegraphics[height=3cm]{img/trilogy/stacked_defrise_12cm_256x256x320_sag.png}&
       \includegraphics[height=3cm]{img/trilogy/stacked_defrise_20cm_256x256x400_sag.png}\\
       MLEM&
       \includegraphics[height=3cm]{img/trilogy/disk_dual_00cm_em_list_256x256x234_sag.png}&
       \includegraphics[height=3cm]{img/trilogy/disk_dual_12cm_em_list_256x256x354_sag.png}&
       \includegraphics[height=3cm]{img/trilogy/disk_dual_20cm_em_list_256x256x434_sag.png}\\

       \midrule

       FDK&
       \includegraphics[height=3.5cm]{img/trilogy/stacked_rando_00cm_200x512x512_sag.png}&
       \includegraphics[height=3.5cm]{img/trilogy/stacked_rando_12cm_320x512x512_sag.png}&
       \includegraphics[height=3.5cm]{img/trilogy/stacked_rando_20cm_400x512x512_sag.png}\\
       MLEM&
       \includegraphics[height=3.5cm]{img/trilogy/rando_00cm_hf_300x512x512_sag.png}&
       \includegraphics[height=3.5cm]{img/trilogy/rando_12cm_hf_380x512x512_sag.png}&
       \includegraphics[height=3.5cm]{img/trilogy/rando_20cm_hf_440x512x512_sag.png}\\

       \bottomrule
     \end{tabular}
     \caption{Mid-sagittal views of the Defrise disk and RANDO phantom
       reconstructions for a single scan and double circular scans at
       different separation distances. The top two rows show the Defrise
       disk phantom, and the bottom two rows show the RANDO phantom. The
       display window is [0.1, 0.3]~cm$^{-1}$. The red arrows indicate
       misalignment artifacts in the MLEM reconstruction introduced by
       the couch translation.}
     \label{fig:trilogy_data}
     %\vspace{-1em}
   \end{figure*}

 #+END_LaTeX

 #+BEGIN_LaTeX
   \begin{figure*}[t!]
     \centering
     \begin{tabular}{lccc}
       \toprule
       &FDK stacked&MLEM double circle&MLEM smooth\\
       \midrule

       18~cm&
       \includegraphics[height=3cm]{img/take/fdk_18cm.png}&
       \includegraphics[height=3cm]{img/take/mlem_dual_18cm.png}&
       \includegraphics[height=3cm]{img/take/mlem_smth_18cm.png}\\
       20~cm&
       \includegraphics[height=3cm]{img/take/fdk_20cm.png}&
       \includegraphics[height=3cm]{img/take/mlem_dual_20cm.png}&
       \includegraphics[height=3cm]{img/take/mlem_smth_20cm.png}\\

       \bottomrule
     \end{tabular}
     \caption{Mid-sagittal views of the Defrise disk phantom
       reconstructions at different separation distances. The display
       window is [0.1, 0.3]~cm$^{-1}$. The left column shows the stacked
       FDK extended volumes, and the remaining columns show the
       100$^{\text{th}}$ iteration of the MLEM extended volumes for
       different trajectories.}
     \label{fig:sim_take}
     %\vspace{-1em}
   \end{figure*}

 #+END_LaTeX

 Figure (\ref{fig:take_rmse}) shows a root-mean-square error (RMSE)
 comparison of a variety of different trajectories with different
 spacings between the two planes of the circles. The volume for which
 the RMSE is calculated is the central volume between the two circles
 with a 20 cm axial length, which would be the region seen with a
 single CBCT scan. The figure shows that for any extended volume
 spacing, the stacked-FDK reconstruction from two separate circles
 deviates the most from the truth, and it degrades with increasing
 separation. The figure also shows that the iterative reconstruction of
 the same dual-circle trajectory is much closer to the truth, but
 demonstrates the same degradation with increasing spacing between the
 two circles. Finally, the double circle and line trajectory and the
 new smooth trajectory iterative reconstructions remain relatively
 constant for increasing spacing, with the smooth trajectory being
 closer to the truth. The slices shown in Figure (\ref{fig:sim_take})
 visually agree with these results. The double circle and line
 trajectory was left out of Figure (\ref{fig:sim_take}) since the
 results were not visually distinguishable from the smooth trajectory
 reconstructions.

**** notebooks                               :noexport:
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/fov/em_vs_fdk.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/fov/em_vs_fdk.ipynb]]

** Conclusion
   :PROPERTIES:
   :ID:       99a861bc-c072-4082-806f-9279fa7c3a3c
   :END:
* Collision-avoiding trajectories            :col:
  :PROPERTIES:
  :ID:       99055e18-4b61-404e-9408-ebd5fd0a5d8d
  :END:
** notes                                     :noexport:
   :PROPERTIES:
   :ID:       53a46fd0-a854-4b6a-a253-dde04d4f7a87
   :END:
- General approach seems to be to make the chapters presentations of
  different studies (papers/proceedings) and the subsequent results
  and conclusions that can be made.
*** publications
    :PROPERTIES:
    :ID:       32703eae-6f65-4a6a-9f23-813e60747126
    :END:
- 2015 MIC virtual isocenter
- 2016 CT meeting dyanmic magnification
- 2016 MIC mixed magnification
- 2017 Varian dynamic magnification
** Introduction
   :PROPERTIES:
   :ID:       b0e53ca9-9c57-46e5-a558-c878b2ee1bdd
   :END:
Given the clinical benefits provided by the linac-mounted CBCT sytem,
it is detrimental when adequate tomographic information cannot be
obtained from the kV-imaging CBCT system. One situation in which this
can occur is when a collision between the patient and the machine
arises. While there is has been substantial work done on the detection
and avoidance of collisions in treatment delivery
cite:humm_collision_1995,chao_image_2001,tsiakalos_graphical_2001,nioutsikou_patient-specific_2003,hua_practical_2004,becker_collision_2013,padilla_patient-specific_2014,padilla_collision_2015,
the methods are often insufficient for standard CBCT imaging because
they generally seek to resolve collisions preventing ideal treatment
positions without attempting obtain tomographic image information. A
generalized imaging framework such as the one we use hered may allow
for some of these techniques to also acquire tomographic information
at the same time.

These may be of particular concern in breast and lung cancer patients
where the arm position leads to a possible collision as shown in
Figure ([[ref:fig:col_barbie_collision]]). Collisions also present a
problem in treatment of posterior and lateral lesions in stereotactic
body radiosurgery (SBRT). Similarly in prone breast treatments, where
the target is near the couch top and a lateral couch translation is
needed to bring the target to isocenter, collision with the
contralateral side of the patient may occur. When collisions do occur,
the angular range available for scanning is restricted and it is not
possible to acquire a complete circular scan in the treatment
position.

#+BEGIN_EXPORT latex
\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/col/barbie_mv.jpg}
    \caption{}
    \label{fig:col_barbie_mv}
  \end{subfigure}
  ~%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
  %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/col/barbie_kv.jpg}
    \caption{}
    \label{fig:col_barbie_kv}
  \end{subfigure}
  \caption{Two examples of potential collision for a
    typical patient setup using a mannequin in a supine treatment
    position. As can be seen, collisions can occur both with the face
    of the MV treatment head (distance 41.7 cm from isocenter for this
    linac) and with the kV detector (distance 45-70 cm from isocenter,
    depending on magnification).}
  \label{fig:col_barbie_collision}
\end{figure*}
#+END_EXPORT

To avoid collisions with the treatment head, it could be desirable to
move the patient away from the gantry as it approaches a collision,
simply by translating the couch. To avoid collisions with the imaging
panel, the patient might also be moved away from the panel as a
collision approaches. As some linac-mounted, kV-imaging panels have
motion capabilities, another solution would be to move the imager away
from the patient in the collision zone, which changes the imaging
magnification for that portion of the scan. Given that the clearance
distance of the kV-imaging panel is not much larger than that of the
MV-treatment head, a collision avoidance solution needs to take into
account both of these components.

Here, we investigate examples of potential scanning trajectories that
would allow the acquisition of sufficient projection information for a
clinically useful image while avoiding a potential patient collision
with the gantry. As the gantry rotates, there are two components of
the linac that are potential sources of patient collisions. These are
the MV-treatment head, shown in Figure ([[ref:fig:col_barbie_collision]]),
and the kV-CBCT detector.

One trajectory that would avoid a patient collision with the
MV-treatment head is a virtual isocenter trajectory. This trajectory
avoids such a collision by increasing the effective source-to-axis
distance (SAD). By using this increased SAD for an imaging trajectory,
the clearance between the patient and the MV-treatment head as the
gantry rotates is increased and the collision is avoided.

The virtual isocenter trajectory utilizes synchronized gantry rotation
and couch translation to maintain a fixed distance (``virtual SAD'')
between the MV source and a chosen center of rotation (``virtual
isocenter'') in the patient. At the beginning of the scan, the patient
is moved away from the linac head along the MV beam direction. As the
gantry rotates, the couch moves continuously to maintain the specified
separation as shown in Figure ([[ref:fig:col_virtual_iso]]). The virtual
SAD can be chosen large enough so that collisions as shown in Figure
([[ref:fig:col_barbie_mv]]) are avoided; at this point in the trajectory,
the couch would have moved far enough to the left to avoid the
collision. Note that it is only the distance to the linac head that is
increased in the plain virtual SAD technique; the distance from the kV
source and detector to the patient and to each other are unchanged.

#+BEGIN_EXPORT latex
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/col/gantry_3angles.eps}
\caption{Patient, kV and MV beams and kV detector at several angles
  during a virtual isocenter rotation. Room coordinate system (dotted
  axes) has its origin at mechanical isocenter, also the intersection
  of the MV (red) and kV (green) beam axes. As the gantry rotates, the
  patient (filled contour) is continually shifted to maintain a
  specified distance along the MV beam direction between the
  mechanical isocenter and the chosen virtual isocenter (circle symbol
  within the patient). The path of the virtual isocenter is a circle
  about the mechanical isocenter, with radius equal to the chosen
  shift (12 cm from the isocenter in this example). Detector may or
  may not be shifted as shown, depending on virtual isocenter position
  and patient geometry.
\label{fig:col_virtual_iso}}
\end{figure*}
#+END_EXPORT

Another trajectory that could avoid a patient collision with the kV
detector would be one during which either the patient or the detector
is moved during the scan in the angular range of a collision. Either
solution leads to changing kV-CBCT imaging magnification during the
acquisition. Again, optimization-based reconstruction methods can
readily handle such a change in magnification provided the projection
information is correctly incorporated into the system matrix.

Finally, we study a trajectory that combines both virtual isocenter
and dynamic magnification trajectories to create a hybrid scanning
acquisition that would alleviate collisions with both the MV-treatment
head and the kV-CBCT detector. We use such a trajectory as an example
of a patient-specific scanning trajectory designed to avoid a
collision that would arise with a particular treatment position.

** Methods and Materials
   :PROPERTIES:
   :ID:       973b1793-b733-43ce-a7aa-dd31af58c680
   :END:
*** Phantoms
    :PROPERTIES:
    :ID:       501e0d09-ed7c-4c78-a997-477e6414df18
    :END:
The abdominal region of the CIRS (Computerized Imaging Reference
Systems, Norfolk, VA) model 600 torso phantom was scanned. This
phantom contains both high-contrast (bone) and low-contrast (soft
tissue) structures in an anthropomorphic form. These features provide
an indication of imaging performance in more clinically relevant
conditions.

To quantify image quality metrics for the different scanning
trajectories, we used a Catphan 504 (The Phantom Laboratory, Salem,
NY). This is a standard quality assessment (QA) phantom for clinical
CT devices that provides a series of sections with different objects
for calculating image quality metrics. We used the CTP 404 module for
evaluating low-contrast resolution, and the CTP 404 and 528 modules
for evaluating spatial resolution.

*** Scans
    :PROPERTIES:
    :ID:       dd0b68a5-c0eb-4c57-a28f-c27e2c57c8d8
    :END:
For the circular scans, the gantry made a full rotation about the
patient with the treatment volume at a fixed mechanical SAD of 100 cm.
For the virtual isocenter scans, the patient couch was translated
continuously in the gantry rotation plane during gantry rotation to
maintain a distance of 112 cm between the MV source and the chosen
target point within the treatment volume (the "virtual isocenter"),
rather than the mechanical SAD of 100 cm as shown in Figure
([[ref:fig:col_virtual_iso]]). We generated all of the scanning
trajectories in this study using the Developer Mode 2.0 XML schema to
create control points for the gantry, the kV imaging arms, and the
patient treatment table. Imaging control points were placed along the
trajectory to acquire kV-projection images during the scan. We used a
half-fan detector configuration with a 13 cm offset for the circular
acquisition, and an equivalent offset for the virtual isocenter to
obtain the same illumination.

To increase the clearance between the kV detector and the patient, we
increased the radius of the kV detector with accompanying increase in
magnification of the kV imaging system. To create projection datasets
where the detector distance changes during a scan, we acquired
multiple scans using different detector positions and subsequently
spliced these together to create the sinograms of interest with the
corresponding system matrix $\mathcal{H}$. This allowed us to create
different dynamic magnification scan datasets. We acquired both
circular and virtual isocenter trajectories with detector positions of
50 cm, 60 cm and 70 cm away from the mechanical isocenter for
magnifications of 1.5X, 1.6X and 1.7X respectively. In each case, the
detector cover is 5 cm closer to the patient than the CsI layer,
potentially leading to collisions with the limits shown in the first
plot in Figure ([[ref:fig:col_collision_zones]]).

#+CAPTION: Collision zones in the patient image space for the kV detector cover and the MV treatment head accessory mount. The left figure shows the increasing radius of the kV-detector collision zone with an increase in magnification. The middle figure shows the increased radius of the kV-detector collision zone for the two dynamic magnification trajectories utilizing a $45^{\circ}$ bump at a higher magnification. The right figure shows the increased radius of the MV-treatment-head collision zone when using the virtual isocenter scanning trajectory.
#+ATTR_LaTeX: :width \textwidth :float multicolumn
#+LABEL: fig:col_collision_zones
[[file:figures/col/collision_zones.pdf]]

To create the combined sinogram of a hypothetical collision-avoiding
dynamic magnification scan, we replaced a $45^{\circ}$ region of the 1.5X
circular scan with the corresponding angular range from scans at
different magnifications. We chose this region to be centered on the
angular position where the mannequin's elbow is in Figure
([[ref:fig:col_barbie_collision]]). Increasing the magnification in this
region corresponds to increasing the clearance between the kV-detector
and the patient. Increasing the magnification to 1.6X provides an
additional 10 cm of clearance, and to 1.7X an additional 20 cm of
clearance. We also created an additional $35^{\circ}$ 1.7X bump
magnification with $5^{\circ}$ transitions at a 1.6X magnification. The
kV-detector collision zones of these dynamic magnification
trajectories are shown in the middle plot shown in Figure
([[ref:fig:col_collision_zones]]).

For the collision with the MV treatment head accessory mount shown in
Figure ([[ref:fig:col_barbie_collision]]), using the virtual isocenter
imaging trajectory would alleviate this problem. The radius of the
accessory mount from the mechanical isocenter is 41.7 cm. For the
virtual isocenter used in this study, there is a 12 cm increase in the
radius of this collision zone as shown in the last plot in Figure
([[ref:fig:col_collision_zones]]). Utilizing a different virtual SAD would
allow for additional clearance if necessary.

The last set of trajectories we studied combines the dynamic
magnification with the virtual isocenter trajectory. As the collision
radius with the MV treatment head and the kV detector are similar for
the current clinical scan, collisions with both could arise. By
combining the change in magnification with the virtual isocenter
trajectory, both collision zones could be avoided. Table
([[ref:tab:col_trajectories]]) shows the different scans investigated in this
study.

# +ATTR_LATEX: :environment longtable
#+CAPTION: Scanning trajectories
#+NAME: tab:col_trajectories
|-------------------+------------------------------------------|
| Trajectory        | Magnification                            |
|-------------------+------------------------------------------|
| Circle            | 1.5X                                     |
|                   | 1.5X & $45^{\circ}$ 1.6X bump                   |
|                   | 1.5X & $45^{\circ}$ 1.7X bump                   |
|                   | 1.5X & $35^{\circ}$ 1.7X bump, 1.6X transitions |
|-------------------+------------------------------------------|
| Virtual isocenter | 1.5X                                     |
|                   | 1.5X & $45^{\circ}$ 1.6X bump                   |
|                   | 1.5X & $45^{\circ}$ 1.7X bump                   |
|                   | 1.5X & $35^{\circ}$ 1.7X bump, 1.6X transitions |
|-------------------+------------------------------------------|
**** figures                                 :noexport:
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/collision_avoidance.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/collision_avoidance.ipynb]]
*** Reconstruction
    :PROPERTIES:
    :ID:       d0eac288-37d6-4cef-97c8-7896dcc67733
    :END:
The addition of couch motion to the imaging trajectory does increase
the degrees of freedom for which proper geometric calibration must be
acquired. Using Varian's Isocal calibration phantom, we developed an
optimization-based calibration protocol that allows us to extract
calibration offsets for improving our estimate of the system matrix
$\mathcal{H'}$. Using same scanning trajectory to image the Isocal
phantom, the nominal scanning metadata from the TrueBeam projections
provided an initial estimate of the imaging geometry. We then used the
Nelder-Mead simplex program to search for the geometric offsets for
the source, detector, and treatment couch positions at each view, as
well as the phantom pose to account for possible setup errors. The
cost of this optimization used the $L_{2}-\text{norm}$ between the
projected fiducials in the real data sinogram and the simulated
projection of the fiducials using the current estimate of the system
geometry. We used the estimated offsets that minimized the cost
function to refine our system matrix $\mathcal{H'}$ that we
subsequently used in the MLEM reconstruction program.

We reconstructed all of these fixed magnification and dynamic
magnification scans from circular and virtual isocenter trajectories
into the patient image space described by the imaging model in
Equation ([[ref:eq:opt_linmodel_patient]]). The Catphan scans were
reconstructed onto an isotropic voxel size of 0.473 mm. The CIRS torso
scans were reconstructed onto an isotropic voxel size of 0.836 mm. As
the circular acquisition with 1.5X magnification is the typical
clinical acquisition trajectory, this provides a clinical reference
volume for the reconstructions from the other scanning configurations.

#+CAPTION: Plot of the Catphan's edge-spread function MTF at 50% and 25% as well as the MTF AUC for the clinical circular 1.5X half-fan scan.
#+ATTR_LaTeX: :width 0.8\textwidth
#+LABEL: fig:col_mtf_vs_iteration
[[file:figures/col/edge_spread_mtf_iteration.pdf]]

Figure ([[ref:fig:mtf_vs_iteration]]) shows the 25% and 50% MTF crossing
spatial frequencies and the MTF AUC of the edge spread function from
the MLEM reconstruction of the Catphan phantom acquired with a
circular scan at 1.5X magnification, vs. iteration number. Based on
this result from a typical scanning configuration, we chose to use 200
iterations of the MLEM algorithm for all reconstructions, using
approximately 900 projection views for each scan.
**** figs                                    :noexport:
***** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/161213_virtiso_circ_half_dynmag_catphan_isocal/iteration_plots.ipynb")][truebeam/161213_virtiso_circ_half_dynmag_catphan_isocal/iteration_plots.ipynb]]
*** Metric Evaluation
    :PROPERTIES:
    :ID:       bbb809be-d901-4945-b045-bb886cf349bd
    :END:
To evaluate the different magnifications of both the circular and
virtual isocenter trajectory, we used the sensitometry and spatial
resolution modules (CTP 404 and CTP 528 respectively) of the Catphan
phantom. Using metrics that quantify these image qualities, we
compared the different magnifications of both the circular and virtual
isocenter scans to the MLEM reconstruction of a standard circular
scan.

There are a number of ways to evaluate the spatial resolution from
images of the Catphan phantom in a CT image. The CTP528 module
contains a circular array of bar patterns which we used to,
subjectively, determine the highest frequency set which is resolvable.
The same module also has two 0.28mm tungsten carbide beads simulating
an impulse source from which a point spread and then modulation
transfer function (MTF) can be determined. Furthermore, the MTF can be
calculated from the bar patterns themselves
cite:droege_practical_1982, as well as any suitably high contrast edge
in the image cite:rossmann_point_1969.

While the use of MTF in CT has its challenges, notably the assumption
of shift-invariance is not satisfied, it still can be useful when
treated with some care. Each of the methods above has some advantages
and disadvantages. The point source method can provide 3D directional
estimates of the point-spread function (PSF), however it can also be
sensitive to the location of the bead relative to the image grid with
significant difference between a bead located totally within a single
voxel or on the interface of many.

The bar pattern based evaluation is a clear complement to the visual
analysis, however the orientation of the bars relative to the grid
will affect some frequencies differently than others which can result
in atypical appearing MTF curves. Using an edge spread analysis on the
circular phantom boundary provides many samples, at varying directions
to the image grid which can be averaged out. It can be impacted by
scatter or saturation in the air region near the phantom boundary,
however this has not proven to be a significant factor in the images
we have analyzed.

The image slice for analysis, the central slice here, is first
thresholded based on the image intensity, the connected component with
area of the appropriate size is isolated and any holes in the
thresholded region are filled. The center of this region is taken as
the phantom center and used as the origin of the coordinates for
analysis. The data are then resampled at high density, along radial
spokes at 8 angles chosen to avoid surface alignment marks, using a
linear interpolant from 5 mm inside to 5 mm outside the surface
boundary. The edge-spread function is then the mean $(\mu(X))$
subtracted profile over the standard deviation $(\sigma(X))$, or
\begin{equation}
\text{ESF} = \frac{X-\mu(X)}{\sigma(X)}.
\end{equation}
In standard form, the line-spread function (LSF) can be computed from
the derivative of the edge-spread function (ESF),
\begin{equation}
\text{LSF} = \frac{d}{dX}\text{ESF},
\end{equation}
and the MTF as the discrete fourier transform $(\mathcal{D})$ of the
LSF,
\begin{equation}
\text{MTF} = \mathcal{D}(\text{LSF})
\end{equation}

To characterize low-contrast resolution, we calculated the
contrast-to-noise ratio (CNR) using the polystyrene insert in the CTP
404 sensitometry module. These inserts have CT numbers which are the
closest to the water-like polymer that surrounds them. The metric is
defined as
\begin{equation}
\label{eq:lcv}
\text{CNR} = \frac{2\left|\mu_{\text{roi}}-\mu_{\text{bkg}}\right|}{\sigma_\text{roi}+\sigma_\text{bkg}}
\end{equation}
where $\sigma$ represents the standard deviation and $\mu$ the mean of the of
the pixel values in the respective regions.

The last metric we evaluated was the reproducibility of the CT numbers
between the circular and virtual isocenter scan, and the different
combinations of magnification bumps. For this we used the mean and
standard deviations in ROIs for all the material insert the Catphan
CTP 404 sensitometry module in addition to the polystyrene and
background ROIs used for the low-contrast CNR calculations. We also
evaluated three additional ROIs of the water-like background for a
total of four background ROIs.

In addition to the catphan modules, we also analyzed ROIs
corresponding to aorta, liver and spleen in the abdomen of the CIRS
torso phantom. For each of the magnifications from both the circular
and virtual isocenter trajectories, the mean HU values and standard
deviations were calculated.
*** analysis                                 :noexport:
 - [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"161213_virtiso_circ_half_dynmag_catphan_isocal/collision_avoidance.ipynb")][collisions figures]]
 - [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"170126_virtiso_circ_half_dynmag_ed/ed_images.ipynb")][ed analysis]]
** Results
   :PROPERTIES:
   :ID:       28832b04-7d26-4ef8-83f2-08db94493ab9
   :END:
Figure ([[ref:fig:col_recons_catphan]]) shows slices of the CTP 528
spatial-resolution module from the $200^{\text{th}}$ iteration of the
MLEM reconstructions of the Catphan phantom for the different scanning
trajectories. The top row shows reconstructions from the circular
scanning trajectories, and the bottom row shows reconstructions from
the virtual isocenter trajectories. The left column is from a single
1.5X magnification, and the remaining columns are different
synthesized trajectories with different magnifications as illustrated
in Figure ([[ref:fig:col_collision_zones]]). In all of these images, the
$8^{\text{th}}$ largest gauge is visually resolvable.

#+CAPTION: Images of the Catphan 528 spatial resolution module in a display window of [-100, 2000] HU. The top row shows all the circular scan permutations while the bottom row shows those of the virtual isocenter. The columns show different magnification combinations from left to right of 1.5X only, 1.5X with a 1.6X bump, 1.5X with a 1.7X bump, and a 1.5X with a 1.7X bump and a 1.6X transition on either side. For all of the reconstructions, the 8$^{\text{th}}$ largest gauge is resolvable (indicated by the red arrow).
#+ATTR_LaTeX: :width \textwidth :float multicolumn
#+LABEL: fig:col_recons_catphan
[[file:figures/col/catphanDynmagQuarter.pdf]]

The visual similarity in the spatial resolution shown in Figure
([[ref:fig:col_recons_catphan]]) is reflected in the MTF metrics for all of
the different scanning configurations. We compared MTF metrics derived
from the PSF using the Catphan beads, the bar pattern shown in Figure
([[ref:fig:col_recons_catphan]]), and the edge-spread function (ESF) measure
along an ensemble of radial lines. When comparing these MTF-based
metrics between the circle and the virtual isocenter scans with
different magnifications, we found no clear trend distinguishing the
different trajectories.

#+CAPTION: Plot of low-contrast polystyrene CNR and error bars corresponding to $\pm$ one standard deviation of the CNR from the Catphan scanned with both the circular and virtual isocenter trajectories.
#+ATTR_LaTeX: :width 0.8\textwidth
#+LABEL: fig:col_catphan_cnr
[[file:figures/col/catphanCNR_ps.pdf]]

Figure ([[ref:fig:catphan_cnr]]) shows the low-contrast CNR from the
Catphan with these different imaging configurations which are also in
good agreement with each other. In addition to using the material rods
in the CTP 404 sensitometry module to calculate the CNR, we also
compared mean values from each of the materials for the different
scanning trajectories and magnifications. Figure
([[ref:fig:catphan_rois]]) shows the ROI means for these different
scanning configurations. The height of the bar represents the standard
deviation of the ROI. The first four bars for each material are from
the circular scans, and the remaining four from the virtual isocenter
scans.

#+BEGIN_EXPORT latex
\begin{figure*}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/col/sensROIplot.pdf}
    \caption{}
    \label{fig:col_catphan_rois}
  \end{subfigure}
  ~%add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
  %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/col/edROIplot.pdf}
    \caption{}
    \label{fig:col_ed_rois}
  \end{subfigure}
  \caption{ROI material comaprison results for the Catphan CTP 404
    module and the CIRS torso phantom. The height of the bars are $\pm$
    one standard deviation of the ROI mean. The first four bars for
    each material ROI are from the circular scans, and the remaining four
    bars for each material ROI are from the virtual isocenter scans. The
    order of the scans for each material ROI are displayed in the order listed in the
    legend in (a) from left to right.}
  \label{fig:col_hu_results}
\end{figure*}
#+END_EXPORT

#+CAPTION: Images of the CIRS torso phantom's abdomen in a display window of display window of [-200, 500] HU. The top row shows all the circular scan permutations while the bottom row shows those of the virtual isocenter. The columns show different magnification combinations from left to right of 1.5X only, 1.5X with a 1.6X bump, 1.5X with a 1.7X bump, and a 1.5X with a 1.7X bump and a 1.6X transition on either side.
#+ATTR_LaTeX: :width \textwidth :float multicolumn :placement
#+LABEL: fig:col_recons_ed
[[file:figures/col/edDynmag.pdf]]

Figure ([[ref:fig:col_recons_ed]]) shows an abdominal slice from the
$200^{\text{th}}$ iteration MLEM reconstruction of the CIRS torso phantom
scanned with a 13 cm offset half-fan configuration. The layout of
these images is the same as that in Figure
([[ref:fig:col_recons_catphan]]) with the top row showing magnification
variations from the circular trajectory. The bottom row shows the
corresponding magnifications from the virtual isocenter trajectory.
The slice is the same as that in Figure ([[ref:fig:col_ed_rois]]) which we
used to calculate values for three organ ROIs.

For the different organ ROIs, we recorded the means and standard
deviations for the different scanning trajectories and magnifications.
Figure ([[ref:fig:col_ed_rois]]) shows these mean values and the
associated standard deviations. Though the circular scan variations
fluctuate more than the virtual isocenter scans, the values are in
agreement for the different organs of interest. As with Figure
([[ref:fig:col_catphan_rois]]), the first four points for each organ are
from the circular scan, and the remaining four are from the virtual
isocenter trajectory.
*** fixed illumination analysis              :noexport:
**** [[file:/ssh:remus:/data/amdavis/truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/figs/][figs]]
**** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/catphan_images.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/catphan_images.ipynb]]
**** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/metric_analysis.ipynb")][truebeam/170603_virtiso_circ_smth_catphan/dynmag/em/dlatlonslon/metric_analysis.ipynb]]
*** new analysis                             :noexport:
**** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/161213_virtiso_circ_half_dynmag_catphan_isocal/catphan_images.ipynb")][161213_virtiso_circ_half_dynmag_catphan_isocal/catphan_images.ipynb]]
**** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/161213_virtiso_circ_half_dynmag_catphan_isocal/metrics.ipynb")][161213_virtiso_circ_half_dynmag_catphan_isocal/metrics.ipynb]]
**** [[ipynb:(:url-or-port%20"https://remus.uchicago.edu:9999"%20:name%20"truebeam/170126_virtiso_circ_half_dynmag_ed/ed_images.ipynb")][170126_virtiso_circ_half_dynmag_ed/ed_images.ipynb]]
*** old analysis                             :noexport:
    :PROPERTIES:
    :var:      iter=75
    :ID:       c2336149-1ac8-4474-ae0a-ed1b97efcdf8
    :END:
**** catphan
     :PROPERTIES:
     :ID:       066a0563-0e56-4ae2-ba5d-66c120f05c92
     :END:
***** contrast
      :PROPERTIES:
      :ID:       12232f76-bd36-4c93-abc4-3d14828dbbc5
      :END:
****** images
       :PROPERTIES:
       :ID:       9694ff06-d9cc-40f0-aaa1-7e5b3f937bf5
       :END:
 #+BEGIN_SRC python :results file
import dicom
import glob
import numpy as np
import matplotlib.pyplot as plt

from IPython.core.debugger import Tracer
debug_here = Tracer()

def crop(center, width, res):
    """Crop image

    Keyword Arguments:
    center -- [x, y] of center
    width  -- deisred width (mm)
    res    -- res (mm/px)
    """
    px_width = np.int(width*0.5/res)

    xbox = [center[0]-px_width, center[0]+px_width]
    ybox = [center[1]-px_width, center[1]+px_width]

    return(xbox, ybox)

# window
window_hu = [-160, 240]         # HU
window_em = [0.015, 0.026]      # mm^1

# itools
catphan_dcm = dicom.read_file(('itools/161213_catphan_circ_1p5x_ringrem/IMG_0043.dcm'))
catphan_hu = catphan_dcm.pixel_array*catphan_dcm.RescaleSlope+catphan_dcm.RescaleIntercept

# crop
width = 230.                 # mm each way

res_hu = 0.473                 # mm/px
center_hu = [np.int(catphan_hu.shape[0]*0.5), np.int(catphan_hu.shape[1]*0.5)]

winx, winy = crop(center_hu, width, res_hu)

catphan_hu = catphan_hu[winx[0]:winx[1], winy[0]:winy[1]]

plt.imsave('figures/catphan_itools_lcr.png', catphan_hu, vmin=window_hu[0], vmax=window_hu[1])

# recon
dir = "/home/amdavis/research/truebeam/150902_virtiso_cbct_catphan_ed/itools/img/"

files = sorted(glob.glob(dir+"*catphan*itools*{0:04d}*.raw".format(iter)))

catphan_circular = np.fromfile(files[0], 'f').reshape(125, 528, 528).swapaxes(1, 2).T
catphan_virtiso = np.fromfile(files[1], 'f').reshape(125, 528, 528).swapaxes(1, 2).T

catphan_circular = catphan_circular[...,
                                    int(catphan_circular.shape[2]/2)]
catphan_virtiso = catphan_virtiso[...,
                                  int(catphan_virtiso.shape[2]/2)]

# crop
res_hu = 0.473                 # mm/px
center_circular = [np.int(catphan_circular.shape[0]*0.5), np.int(catphan_circular.shape[1]*0.5)]

winx, winy = crop(center_hu, width, res_hu)

catphan_circular = catphan_circular[winx[0]:winx[1], winy[0]:winy[1]]
catphan_virtiso = catphan_virtiso[winx[0]:winx[1], winy[0]:winy[1]]

plt.imsave('figures/catphan_circular_lcr_mlem_iter_{0:03d}.png'.format(iter), catphan_circular, vmin=window_em[0], vmax=window_em[1])
plt.imsave('figures/catphan_virtiso_lcr_mlem_iter_{0:03d}.png'.format(iter), catphan_virtiso, vmin=window_em[0], vmax=window_em[1])

# return('figures/catphan_itools_lcr.png')
# return('figures/catphan_circular_lcr_mlem.png')
# return('figures/catphan_virtiso_lcr_mlem.png')

fig = plt.figure()
ax1 = fig.add_subplot(131)
ax2 = fig.add_subplot(132)
ax3 = fig.add_subplot(133)

ax1.imshow(catphan_hu, vmin=window_hu[0], vmax=window_hu[1])
ax1.set_title('Circular iTools FDK')
ax1.axis('off')

ax2.imshow(catphan_circular, vmin=window_em[0], vmax=window_em[1])
ax2.set_title('Circular MLEM')
ax2.axis('off')

ax3.imshow(catphan_virtiso, vmin=window_em[0], vmax=window_em[1])
ax3.set_title('Virtual Isocenter MLEM')
ax3.axis('off')

fig.savefig('figures/catphan_comparison_iter_{0:03d}.png'.format(iter), bbox_inches="tight")
return('figures/catphan_comparison_iter_{0:03d}.png'.format(iter))
 #+END_SRC

 #+RESULTS:
 [[file:figures/catphan_comparison_iter_075.png]]
****** itools analysis
       :PROPERTIES:
       :ID:       bc6df6dd-13e6-4bc5-826c-2002d54ab552
       :END:
  Contrast analysis for catphan

 #+BEGIN_SRC python :results output
import dicom
import numpy as np
import matplotlib.pyplot as plt

from phantoms import lcr

from IPython.core.debugger import Tracer
debug_here = Tracer()

catphan_dcm = dicom.read_file(('itools/161213_catphan_circ_1p5x_ringrem/IMG_0042.dcm'))
catphan_hu = catphan_dcm.pixel_array*catphan_dcm.RescaleSlope+catphan_dcm.RescaleIntercept

itools_lcr = lcr.CatphanLowContrast(catphan_hu, xres=0.473, x0=121.8/0.473,
                                    y0=121.6/0.473, z=0, rot=-3.48, hu=True)

itools_lcr.save_contrast_calcs('data/', 'catphan_circular_itools')

# itools_lcr.show_rod_locs(vmin=-60, vmax=60)
# plt.savefig('figures/catphan_itools_lcr_locs.png')
# return('figures/catphan_itools_lcr_locs.png')
print(itools_lcr.contrast_1_0)
print(itools_lcr.contrast_0_5)
print(itools_lcr.contrast_0_3)
 #+END_SRC

 #+RESULTS:
 #+begin_example
              % contrast  % contrast sigma       cnr  cnr sigma
rod diameter
15.0            1.273217          1.022240  1.236393   1.414493
9.0             1.157174          1.021059  1.100177   1.415368
8.0             0.767365          1.017061  0.898154   1.437275
7.0             1.198443          1.021447  1.296850   1.420209
6.0             0.486577          1.014224  0.484239   1.414228
5.0             0.860374          1.018018  0.881201   1.415018
4.0             0.763490          1.017019  0.933650   1.452495
3.0             0.356576          1.012901  0.589711   1.701662
2.0             1.844718          1.027938  2.757142   1.586562
              % contrast  % contrast sigma       cnr  cnr sigma
rod diameter
15.0            0.501931          1.033669  0.492042   1.414261
9.0             0.094065          1.029462  0.093504   1.414567
8.0             0.392379          1.032535  0.429770   1.425485
7.0            -0.057559          1.027902  0.061673   1.421552
6.0             0.466486          1.033301  0.477422   1.416169
5.0             0.405244          1.032665  0.482771   1.449648
4.0            -0.082372          1.027647  0.091132   1.427591
3.0            -0.630458          1.022024  0.675272   1.421494
2.0            -0.591860          1.022407  0.984642   1.735271
              % contrast  % contrast sigma       cnr  cnr sigma
rod diameter
15.0            0.162203          0.979250  0.181559   1.420491
9.0            -0.001550          0.977648  0.001615   1.414454
8.0             0.488048          0.982444  0.519364   1.415367
7.0             0.614937          0.983694  0.624087   1.414256
6.0             0.189741          0.979519  0.207107   1.417398
5.0             1.030511          0.987745  1.524829   1.548857
4.0             0.985444          0.987344  1.008152   1.414214
3.0             0.322520          0.980822  0.318188   1.415103
2.0            -0.096169          0.976723  0.107345   1.420092
 #+end_example
****** recon em analysis
       :PROPERTIES:
       :ID:       73598a41-2659-4be4-8473-9086ba73c896
       :END:
 Results from my reconstructions

 #+BEGIN_SRC python :results output
import glob
import numpy as np
import matplotlib.pyplot as plt

from phantoms import lcr

from IPython.core.debugger import Tracer
debug_here = Tracer()

# get files we have
dir = "/home/amdavis/research/truebeam/150902_virtiso_cbct_catphan_ed/itools/img/"
files = sorted(glob.glob(dir+"*catphan*itools*{0:04d}*.raw".format(iter)))

catphan_circular = np.fromfile(files[0], 'f').reshape(125, 528, 528).swapaxes(1, 2).T
catphan_virtiso = np.fromfile(files[1], 'f').reshape(125, 528, 528).swapaxes(1, 2).T

catphan_circular = catphan_circular[...,
                                    int(catphan_circular.shape[2]/2)]
catphan_virtiso = catphan_virtiso[...,
                                  int(catphan_virtiso.shape[2]/2)]

circular_lcr = lcr.CatphanLowContrast(catphan_circular, xres=0.473,
                                      x0=263.5, y0=261.5, z=0, rot=-3.,
                                      hu=False)

virtiso_lcr = lcr.CatphanLowContrast(catphan_virtiso, xres=0.473,
                                     x0=272, y0=259.5, z=0, rot=-3.,
                                     hu=False)

circular_lcr.save_contrast_calcs('data/', 'catphan_circular_mlem_iter_{0:03d}'.format(iter))
virtiso_lcr.save_contrast_calcs('data/', 'catphan_virtiso_mlem_iter_{0:03d}'.format(iter))

# circ_fig = circular_lcr.show_rod_locs(vmin=0.02, vmax=0.021)
# virt_fig = virtiso_lcr.show_rod_locs(vmin=0.019, vmax=0.021)

# circ_fig.savefig('figures/catphan_circular_mlem_lcr_locs.png')
# virt_fig.savefig('figures/catphan_virtiso_mlem_lcr_locs.png')
# return('figures/catphan_circular_mlem_lcr_locs.png')
# return('figures/virtiso_circular_mlem_lcr_locs.png')

print("\n0.3% circular")
print(circular_lcr.contrast_0_3)
print("\n0.3% virtiso")
print(virtiso_lcr.contrast_0_3)

print("\n0.5% circular")
print(circular_lcr.contrast_0_5)
print("\n0.5% virtiso")
print(virtiso_lcr.contrast_0_5)

print("\n1.0% circular")
print(circular_lcr.contrast_1_0)
print("\n1.0% virtiso")
print(virtiso_lcr.contrast_1_0)
 #+END_SRC

 #+RESULTS:
 #+begin_example

0.3% circular
      % contrast  % contrast sigma       cnr  cnr sigma
15.0    0.336178          0.632722  0.757847   1.423412
9.0     0.292402          0.669551  0.619270   1.415762
8.0     0.131968          0.623152  0.302362   1.426568
7.0     0.431134          0.613007  1.009752   1.431675
6.0     0.290603          0.656327  0.628716   1.417611
5.0     0.422215          0.665411  0.900781   1.416314
4.0     0.155166          0.543975  0.431161   1.509609
3.0    -0.297085          0.688549  0.609311   1.414347
2.0     0.472236          0.741477  0.903903   1.416268

0.3% virtiso
      % contrast  % contrast sigma       cnr  cnr sigma
15.0    0.382718          0.830310  0.658319   1.424841
9.0     0.092026          0.799518  0.165280   1.435069
8.0    -0.107992          0.868174  0.176177   1.417189
7.0    -0.103785          0.786340  0.190046   1.440932
6.0    -0.309173          0.774152  0.576760   1.447345
5.0    -0.309976          0.759113  0.594024   1.458054
4.0    -0.223083          1.130837  0.282885   1.435041
3.0    -0.471983          0.698019  1.039611   1.543806
2.0    -1.038081          0.691397  2.308892   1.551891

0.5% circular
      % contrast  % contrast sigma       cnr  cnr sigma
15.0    0.335700          0.647368  0.735412   1.415590
9.0     0.015566          0.591873  0.037623   1.430468
8.0     0.560906          0.624661  1.278832   1.419516
7.0     0.200324          0.689673  0.411264   1.414540
6.0    -0.258247          0.630936  0.579385   1.417608
5.0     0.550274          0.624253  1.255414   1.419604
4.0     0.465531          0.577228  1.165342   1.440343
3.0     0.078896          0.601583  0.187137   1.426210
2.0    -0.371316          0.476344  1.444140   1.859480

0.5% virtiso
      % contrast  % contrast sigma       cnr  cnr sigma
15.0   -0.323800          0.992989  0.467154   1.435720
9.0    -0.265211          0.949907  0.405867   1.456524
8.0     0.103588          0.939622  0.161663   1.465258
7.0     0.500131          0.977952  0.741149   1.444188
6.0     0.876336          0.922225  1.421072   1.485184
5.0     0.751536          0.897809  1.275159   1.513860
4.0     0.118879          0.984507  0.174040   1.440150
3.0     1.058663          1.008137  1.516077   1.433655
2.0     0.703962          0.820208  1.728628   2.000000

1.0% circular
      % contrast  % contrast sigma       cnr  cnr sigma
15.0    0.891881          0.658649  1.933036   1.420049
9.0     0.723097          0.732432  1.401420   1.414637
8.0     0.697721          0.628473  1.594919   1.430160
7.0     0.958346          0.679275  2.008902   1.416360
6.0     0.700465          0.639470  1.568403   1.425579
5.0     0.878039          0.685795  1.821070   1.415579
4.0     0.613320          0.672501  1.296906   1.417139
3.0     0.315199          0.567085  0.827869   1.485735
2.0     0.664848          0.506692  2.641720   2.000000

1.0% virtiso
      % contrast  % contrast sigma       cnr  cnr sigma
15.0    1.216002          0.926873  1.869765   1.415754
9.0     0.418370          0.884876  0.673363   1.420661
8.0     1.236294          0.892132  1.982379   1.420123
7.0     0.963466          0.803941  1.755233   1.454411
6.0     1.147464          0.792635  2.137994   1.464240
5.0     0.164518          0.983530  0.236795   1.414503
4.0    -0.124803          0.796152  0.227947   1.455467
3.0    -0.180899          0.874025  0.294034   1.422201
2.0    -1.164900          0.681244  2.957545   1.749484
 #+end_example
****** plots
       :PROPERTIES:
       :ID:       229ce930-658c-447c-9cbc-d1958f97022b
       :END:
******* CNR
        :PROPERTIES:
        :ID:       184b9d1e-9b95-4aa3-a7f9-127053b3246e
        :END:
 #+BEGIN_SRC python :results file
import glob
import numpy as np
import matplotlib.pyplot as plt
import pandas
import seaborn as sns

sns.set(font='serif', font_scale=0.8)

from itertools import cycle
lines = ["-","--","-."]
linecycler = cycle(lines)

from matplotlib.ticker import FormatStrFormatter

# setup plot params
sns.set_context("paper")

from phantoms import lcr

from IPython.core.debugger import Tracer
debug_here = Tracer()

# load previous calculations
circ_fdk = sorted(glob.glob("data/*itools*rods.pkl"))
circ_em = sorted(glob.glob("data/*circular_mlem*_iter_{0:03d}*rods.pkl".format(iter)))
virt_em = sorted(glob.glob("data/*virtiso*_iter_{0:03d}*rods.pkl".format(iter)))

# load 0.3%, 0.5%, 1.0% rod contrast
circ_fdk = [pandas.read_pickle(f) for f in circ_fdk]
circ_em = [pandas.read_pickle(f) for f in circ_em]
virt_em = [pandas.read_pickle(f) for f in virt_em]

# data sets
titles = ['0.3% contrast rods', '0.5% contrast rods', '1.0% contrast rods']

fig, axs = plt.subplots(3, 1, True)

for j, ax in enumerate(axs):
    ax.errorbar(circ_fdk[j].index, 'cnr', yerr='cnr sigma', data=circ_fdk[j], label='FDK Circular', ls=next(linecycler))
    ax.errorbar(circ_em[j].index, 'cnr', yerr='cnr sigma', data=circ_em[j], label='MLEM Circular', ls=next(linecycler))
    ax.errorbar(virt_em[j].index, 'cnr', yerr='cnr sigma', data=virt_em[j], label='MLEM Virtual Isocenter', ls=next(linecycler))

    ax.set_xlim([1, 16])
    # ax.set_ylim([-3, 3])
    ax.set_ylabel('CNR')
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))
    ax.set_title(titles[j])

ax.legend(loc='best', fancybox=True, framealpha=0.5)
ax.set_xlabel('Rod diameter [mm]')

fig.savefig('figures/cnr_iter_{0:03d}.pdf'.format(iter))

fig.savefig('figures/cnr_iter_{0:03d}.png'.format(iter))
return('figures/cnr_iter_{0:03d}.png'.format(iter))
 #+END_SRC

 #+RESULTS:
 [[file:figures/cnr_iter_075.png]]

******* % contrast
        :PROPERTIES:
        :ID:       03ee4467-a1d4-4d7e-8c12-b8e767bf43a3
        :END:
 #+BEGIN_SRC python :results file
import glob
import numpy as np
import matplotlib.pyplot as plt
import pandas
import seaborn as sns

sns.set(font='serif', font_scale=0.8)

from itertools import cycle
lines = ["-","--","-."]
linecycler = cycle(lines)

from matplotlib.ticker import FormatStrFormatter

# setup plot params
sns.set_context("paper")

from phantoms import lcr

from IPython.core.debugger import Tracer
debug_here = Tracer()

# load previous calculations
circ_fdk = sorted(glob.glob("data/*itools*rods.pkl"))
circ_em = sorted(glob.glob("data/*circular_mlem*_iter_{0:03d}*rods.pkl".format(iter)))
virt_em = sorted(glob.glob("data/*virtiso*_iter_{0:03d}*rods.pkl".format(iter)))

# load 0.3%, 0.5%, 1.0% rod contrast
circ_fdk = [pandas.read_pickle(f) for f in circ_fdk]
circ_em = [pandas.read_pickle(f) for f in circ_em]
virt_em = [pandas.read_pickle(f) for f in virt_em]

# data sets
titles = ['0.3% contrast rods', '0.5% contrast rods', '1.0% contrast rods']

fig, axs = plt.subplots(3, 1, True)

for j, ax in enumerate(axs):
    ax.errorbar(circ_fdk[j].index, '% contrast', yerr='% contrast sigma', data=circ_fdk[j], label='FDK Circular', ls=next(linecycler))
    ax.errorbar(circ_em[j].index, '% contrast', yerr='% contrast sigma', data=circ_em[j], label='MLEM Circular', ls=next(linecycler))
    ax.errorbar(virt_em[j].index, '% contrast', yerr='% contrast sigma', data=virt_em[j], label='MLEM Virtual Isocenter', ls=next(linecycler))

    ax.set_xlim([1, 16])
    ax.set_ylim([-3, 3])
    ax.set_ylabel('% contrast')
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))
    ax.set_title(titles[j])

ax.legend(loc='best', fancybox=True, framealpha=0.5)
ax.set_xlabel('Rod diameter [mm]')

fig.savefig('figures/percent_contrast_iter_{0:03d}.pdf'.format(iter))

fig.savefig('figures/percent_contrast_iter_{0:03d}.png'.format(iter))
return('figures/percent_contrast_iter_{0:03d}.png'.format(iter))
 #+END_SRC

 #+RESULTS:
 [[file:figures/percent_contrast_iter_075.png]]

***** spatial res
      :PROPERTIES:
      :ID:       b755dc97-c01c-461c-99d3-7f116bfc4f5f
      :END:
****** images
       :PROPERTIES:
       :ID:       1a7be01c-fc76-42bf-a09d-7e7556456fd8
       :END:
  #+BEGIN_SRC python :results file
import dicom
import glob
import numpy as np
import matplotlib.pyplot as plt

from IPython.core.debugger import Tracer
debug_here = Tracer()


def crop(center, width, res):
    """Crop image

    Keyword Arguments:
    center -- [x, y] of center
    width  -- deisred width (mm)
    res    -- res (mm/px)
    """
    px_width = np.int(width*0.5/res)

    xbox = [center[0]-px_width, center[0]+px_width]
    ybox = [center[1]-px_width, center[1]+px_width]

    return(xbox, ybox)

# window
window_hu = [150, 1000]         # HU
window_em = [0.022, 0.032]      # mm^1

# itools
catphan_dcm = dicom.read_file(('itools/150902_catphan_full_low_contrast/hu/IMG_0075.dcm'))
catphan_hu = catphan_dcm.pixel_array*catphan_dcm.RescaleSlope+catphan_dcm.RescaleIntercept

# crop
width = 130.                 # mm each way

res_hu = 0.473                 # mm/px
center_hu = [np.int(catphan_hu.shape[0]*0.5), np.int(catphan_hu.shape[1]*0.5)]

winx, winy = crop(center_hu, width, res_hu)

catphan_hu = catphan_hu[winx[0]:center_hu[0], winy[0]:winy[1]]

plt.imsave('figures/catphan_itools_sr.png', catphan_hu, vmin=window_hu[0], vmax=window_hu[1])

# recon
dir = "/home/amdavis/research/truebeam/150902_virtiso_cbct_catphan_ed/itools/img/"
files = sorted(glob.glob(dir+"*catphan*itools*{0:04d}*.raw".format(iter)))

catphan_circular = np.fromfile(files[0], 'f').reshape(125, 528, 528).swapaxes(1, 2).T
catphan_virtiso = np.fromfile(files[1], 'f').reshape(125, 528, 528).swapaxes(1, 2).T

sr_slice = 93
catphan_circular = catphan_circular[..., sr_slice]
catphan_virtiso = catphan_virtiso[..., sr_slice]

# crop
res_em = 0.5                 # mm/px
center_circular = [np.int(catphan_circular.shape[0]*0.5), np.int(catphan_circular.shape[1]*0.5)]

winx, winy = crop(center_circular, width, res_em)

catphan_circular = catphan_circular[winx[0]:center_circular[0], winy[0]:winy[1]]
catphan_virtiso = catphan_virtiso[winx[0]:center_circular[0], winy[0]:winy[1]]

plt.imsave('figures/catphan_circular_sr_mlem_iter_{0:03d}.png'.format(iter), catphan_circular, vmin=window_em[0], vmax=window_em[1])
plt.imsave('figures/catphan_virtiso_sr_mlem_iter_{0:03d}.png'.format(iter), catphan_virtiso, vmin=window_em[0], vmax=window_em[1])

# return('figures/catphan_itools_sr.png')
# return('figures/catphan_circular_sr_mlem.png')
# return('figures/catphan_virtiso_sr_mlem.png')

fig = plt.figure()
ax1 = fig.add_subplot(131)
ax2 = fig.add_subplot(132)
ax3 = fig.add_subplot(133)

ax1.imshow(catphan_hu, vmin=window_hu[0], vmax=window_hu[1])
ax1.set_title('Circular iTools FDK')
ax1.axis('off')

ax2.imshow(catphan_circular, vmin=window_em[0], vmax=window_em[1])
ax2.set_title('Circular MLEM')
ax2.axis('off')

ax3.imshow(catphan_virtiso, vmin=window_em[0], vmax=window_em[1])
ax3.set_title('Virtual Isocenter MLEM')
ax3.axis('off')

fig.savefig('figures/catphan_comparison_sr_iter_{0:03d}.png'.format(iter), bbox_inches="tight")
return('figures/catphan_comparison_sr_iter_{0:03d}.png'.format(iter))
 #+END_SRC

 #+RESULTS:
 [[file:figures/catphan_comparison_sr_iter_075.png]]
**** ed
     :PROPERTIES:
     :ID:       fc37283b-966f-4f98-8693-4c885b4c516d
     :END:
  Generate slices for Ed
  #+BEGIN_SRC python :results file
import glob
import numpy as np
import matplotlib.pyplot as plt
import dicom
# import pickle

# window
window_hu = [-200, 240]         # HU
window_em = [0.015, 0.025]      # mm^1

from IPython.core.debugger import Tracer
debug_here = Tracer()


def crop(center, width, res):
    """Crop image

    Keyword Arguments:
    center -- [x, y] of center
    width  -- deisred width (mm)
    res    -- res (mm/px)
    """
    px_width = np.int(width*0.5/res)

    xbox = [center[0]-px_width, center[0]+px_width]
    ybox = [center[1]-px_width, center[1]+px_width]

    return(xbox, ybox)

# itools
ed_dcm = dicom.read_file(('itools/150902_ed_circular_half_13cm_offset/hu/IMG_0042.dcm'))
ed_hu = ed_dcm.pixel_array*ed_dcm.RescaleSlope+ed_dcm.RescaleIntercept

# crop
width = 300.                 # mm each way

res_hu = 0.836                 # mm/px
center_hu = [np.int(ed_hu.shape[0]*0.5), np.int(ed_hu.shape[1]*0.5)]

winx, winy = crop(center_hu, width, res_hu)

ed_hu = ed_hu[winx[0]:winx[1], winy[0]:winy[1]]

# recon
dir = "/home/amdavis/research/truebeam/150902_virtiso_cbct_catphan_ed/itools/img/"
files = sorted(glob.glob(dir+"*ed*itools*{0:04d}*.raw".format(iter)))

ed_circular = np.fromfile(files[0], 'f').reshape(126, 418, 418).swapaxes(1, 2).T
ed_virtiso = np.fromfile(files[1], 'f').reshape(126, 418, 418).swapaxes(1, 2).T

ed_circular = ed_circular[..., int(ed_circular.shape[2]/2+1)]
ed_virtiso = ed_virtiso[..., int(ed_virtiso.shape[2]/2+1)]

# crop
res_em = 0.836                    # mm/px
center_em = [int(ed_circular.shape[0]*0.5), int(ed_circular.shape[1]*0.5)]

winx, winy = crop(center_em, width, res_em)

ed_circular = ed_circular[winx[0]:winx[1], winy[0]:winy[1]]
ed_virtiso = ed_virtiso[winx[0]:winx[1], winy[0]:winy[1]]

# save img slice
plt.imsave('figures/ed_itools_fdk.png', ed_hu, vmin=window_hu[0], vmax=window_hu[1])
plt.imsave('figures/ed_circular_mlem_iter_{0:03d}.png'.format(iter), ed_circular, vmin=window_em[0], vmax=window_em[1])
plt.imsave('figures/ed_virtiso_mlem_iter_{0:03d}.png'.format(iter), ed_virtiso, vmin=window_em[0], vmax=window_em[1])

# return('figures/ed_itools_fdk.png')
# return('figures/catphan_circular_lcr_mlem.png')
# return('figures/catphan_virtiso_lcr_mlem.png')

# display all three
fig = plt.figure()
ax1 = fig.add_subplot(131)
ax2 = fig.add_subplot(132)
ax3 = fig.add_subplot(133)

ax1.imshow(ed_hu, vmin=window_hu[0], vmax=window_hu[1])
ax1.set_title('Circular iTools FDK')
ax1.axis('off')

ax2.imshow(ed_circular, vmin=window_em[0], vmax=window_em[1])
ax2.set_title('Circular MLEM')
ax2.axis('off')

ax3.imshow(ed_virtiso, vmin=window_em[0], vmax=window_em[1])
ax3.set_title('Virtual Isocenter MLEM')
ax3.axis('off')

fig.savefig('figures/ed_comparison_iter_{0:03d}.png'.format(iter), bbox_inches="tight")
return('figures/ed_comparison_iter_{0:03d}.png'.format(iter))
 #+END_SRC

 #+RESULTS:
 [[file:figures/ed_comparison_iter_075.png]]

** Discussion
   :PROPERTIES:
   :ID:       5d03aa38-f620-4ad1-a60d-f02d5000ea97
   :END:
As mentioned earlier, the virtual isocenter trajectory is designed to
increase the distance between the gantry head and the patient by
synchronized table translation and gantry motion. The geometry of the
imaging arms remains unchanged, though the center of the image space
is moved from the mechanical isocenter to the virtual isocenter. As
shown in the previous section, this results in CBCT image quality that
is comparable to a normal circular scan.

It is certainly possible to envision situations in which collisions
with the kV imaging source or (more likely) the detector also occur.
The system matrix in our optimization-based reconstruction method can
incorporate changes in the position of the imaging arms as well as the
patient position. Thus a trajectory with variable source-detector
distance, caused by the detector moving to avoid a patient collision,
can also be reconstructed. Such a variable magnification scan may be
performed with the patient couch moving in a virtual isocenter
trajectory, or with the patient couch fixed and rotation about the
physical machine isocenter.

All virtual isocenter scanning in the present work was done in
TrueBeam Developer Mode, which is a strictly nonclinical mode of
operation. Simultaneous motion of couch, gantry and imaging arms is
not fully supported by the TrueBeam; however, the motions required for
the virtual isocenter scan, which involve only coupled gantry rotation
and couch translation, are feasible in Developer Mode. Although the
linac can clearly execute the required motions and acquire the images,
virtual isocenter scanning is as yet not available as a clinical
capability. In addition, these scans must be reconstructed using
iterative optimization-based methods, rather than the current
clinically available filtered back-projection method. A historic
concern about optimization-based methods has been reconstruction
speed. These are very computationally intensive programs, but with the
recent availability of GPU-based processing, reconstruction times are
more manageable.

** Conclusion
   :PROPERTIES:
   :ID:       016dd868-817a-44fc-8717-e64fdc5bc0d3
   :END:
Virtual isocenter trajectories and dynamic magnification are
potentially useful as collision-avoiding alternatives to standard
isocentric rotation, both in cases where arc treatments are being
delivered and in cases where CBCT scanning is desirable, but a normal
isocentric scan would cause gantry-patient collisions. Using
optimization-based reconstruction methods, patient-specific, collision
avoiding imaging trajectories that utilize virtual isocenter CBCT
scans and different kV-detector magnifications can be reconstructed by
incorporating the view-by-view imaging and patient geometry into the
system matrix. Image quality, as characterized by spatial resolution
and low contrast object detectability, is comparable for virtual
isocenter scans and for standard isocentric scans using different
magnification bumps to avoid kV-detector collisions. Thus, virtual
isocenter CBCT scans and kV-detector magnification changes could be
combined with optimization-based reconstruction as a useful clinical
approach in collision-plagued situations.

* Summary and conclusions                    :conc:
  :PROPERTIES:
  :ID:       1bade25b-80d6-4650-b8a3-baf370fa657c
  :END:
\makebibliography
